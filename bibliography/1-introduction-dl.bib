@article{ruder_character-level_2016,
	title = {Character-level and {Multi}-channel {Convolutional} {Neural} {Networks} for {Large}-scale {Authorship} {Attribution}},
	url = {http://arxiv.org/abs/1609.06686},
	abstract = {Convolutional neural networks (CNNs) have demonstrated superior capability for extracting information from raw signals in computer vision. Recently, character-level and multi-channel CNNs have exhibited excellent performance for sentence classification tasks. We apply CNNs to large-scale authorship attribution, which aims to determine an unknown text's author among many candidate authors, motivated by their ability to process character-level signals and to differentiate between a large number of classes, while making fast predictions in comparison to state-of-the-art approaches. We extensively evaluate CNN-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches. We show that character-level CNNs outperform the state-of-the-art on four out of five datasets in different domains. Additionally, we present the first application of authorship attribution to reddit.},
	urldate = {2020-04-08},
	journal = {arXiv:1609.06686 [cs]},
	author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G.},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.06686},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 5 figures, 3 tables},
	file = {arXiv Fulltext PDF:/home/thibault/Zotero/storage/8ZTWVC7P/Ruder et al. - 2016 - Character-level and Multi-channel Convolutional Ne.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/Zotero/storage/D9BK7F5D/1609.html:text/html}
}

@article{nagy_metre_nodate,
	title = {Metre as a stylometric feature for {Latin} hexameter poetry},
	author = {Nagy, Ben}
}

@misc{bamman_11k_2012,
	title = {{11K} {Latin} {Texts}},
	url = {https://www.cs.cmu.edu/~dbamman/latin.html},
	author = {Bamman, David},
	year = {2012}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-04-10},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
	file = {Snapshot:/home/thibault/Zotero/storage/MTWGA9LD/neco.1997.9.8.html:text/html}
}


@misc{nguyen_illustrated_2019,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2020-04-10},
	journal = {Medium},
	author = {Nguyen, Michael},
	month = jul,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/thibault/Zotero/storage/BKB4RRWB/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html}
}


@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2020-04-13},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Fondateur GRU},
	file = {arXiv Fulltext PDF:/home/thibault/Zotero/storage/LM9DCE9D/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/Zotero/storage/ZT9A58QP/1409.html:text/html}
}


@misc{lynn_get_2018,
	title = {Get {Busy} with {Word} {Embeddings} – {An} {Introduction}},
	url = {https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/},
	abstract = {This post provides an introduction to “word embeddings” or “word vectors”. Word embeddings are real-number vectors that represent words from a vocabulary, and have broad app…},
	language = {en-US},
	urldate = {2020-04-15},
	journal = {Shane Lynn},
	author = {Lynn, Shane},
	month = feb,
	year = {2018},
	note = {Library Catalog: www.shanelynn.ie},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/2DE7A3SH/get-busy-with-word-embeddings-introduction.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2020-04-17},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/FHHUVD3C/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/3XZ6QQU5/1706.html:text/html}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-04-17},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/RHX3SMPJ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/6XGQZ8NV/1810.html:text/html}
}


@article{raben_humanities_1991,
	title = {Humanities computing 25 years later},
	volume = {25},
	issn = {1572-8412},
	url = {https://doi.org/10.1007/BF00141184},
	doi = {10.1007/BF00141184},
	abstract = {This paper attempts to provide an overview of the development of humanities computing during the past twenty-five years. Mention is made of the major applications of the computer to humanities disciplines, and of the most important and representative projects across the world.},
	language = {en},
	number = {6},
	urldate = {2021-08-19},
	journal = {Computers and the Humanities},
	author = {Raben, Joseph},
	month = dec,
	year = {1991},
	pages = {341--350},
	file = {Springer Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/CG97IRMN/Raben - 1991 - Humanities computing 25 years later.pdf:application/pdf},
}

@article{gelderblom_musisque_2008,
	title = {Musisque {Deoque}: {Un} {Archivio} {Digitale} di {Poesia} {Latina}/{A} {Digital} {Archive} of {Latin} {Poetry}},
	shorttitle = {Musisque {Deoque}},
	number = {7},
	journal = {Variants: the Journal of the European Society for Textual Scholarship},
	author = {Gelderblom, Werner},
	year = {2008},
	note = {Publisher: Brill Academic Publishers, Inc.},
	pages = {223},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/38TSHSYA/1.html:text/html},
}
