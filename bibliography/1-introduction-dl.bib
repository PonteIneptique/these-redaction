@article{ruder_character-level_2016,
	title = {Character-level and {Multi}-channel {Convolutional} {Neural} {Networks} for {Large}-scale {Authorship} {Attribution}},
	url = {http://arxiv.org/abs/1609.06686},
	abstract = {Convolutional neural networks (CNNs) have demonstrated superior capability for extracting information from raw signals in computer vision. Recently, character-level and multi-channel CNNs have exhibited excellent performance for sentence classification tasks. We apply CNNs to large-scale authorship attribution, which aims to determine an unknown text's author among many candidate authors, motivated by their ability to process character-level signals and to differentiate between a large number of classes, while making fast predictions in comparison to state-of-the-art approaches. We extensively evaluate CNN-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches. We show that character-level CNNs outperform the state-of-the-art on four out of five datasets in different domains. Additionally, we present the first application of authorship attribution to reddit.},
	urldate = {2020-04-08},
	journal = {arXiv:1609.06686 [cs]},
	author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G.},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.06686},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 5 figures, 3 tables},
	file = {arXiv Fulltext PDF:/home/thibault/Zotero/storage/8ZTWVC7P/Ruder et al. - 2016 - Character-level and Multi-channel Convolutional Ne.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/Zotero/storage/D9BK7F5D/1609.html:text/html}
}

@article{nagy_metre_nodate,
	title = {Metre as a stylometric feature for {Latin} hexameter poetry},
	author = {Nagy, Ben}
}

@misc{bamman_11k_2012,
	title = {{11K} {Latin} {Texts}},
	url = {https://www.cs.cmu.edu/~dbamman/latin.html},
	author = {Bamman, David},
	year = {2012}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-04-10},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
	file = {Snapshot:/home/thibault/Zotero/storage/MTWGA9LD/neco.1997.9.8.html:text/html}
}


@misc{nguyen_illustrated_2019,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2020-04-10},
	journal = {Medium},
	author = {Nguyen, Michael},
	month = jul,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/thibault/Zotero/storage/BKB4RRWB/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html}
}