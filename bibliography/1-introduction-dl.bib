@article{ruder_character-level_2016,
	title = {Character-level and {Multi}-channel {Convolutional} {Neural} {Networks} for {Large}-scale {Authorship} {Attribution}},
	url = {http://arxiv.org/abs/1609.06686},
	abstract = {Convolutional neural networks (CNNs) have demonstrated superior capability for extracting information from raw signals in computer vision. Recently, character-level and multi-channel CNNs have exhibited excellent performance for sentence classification tasks. We apply CNNs to large-scale authorship attribution, which aims to determine an unknown text's author among many candidate authors, motivated by their ability to process character-level signals and to differentiate between a large number of classes, while making fast predictions in comparison to state-of-the-art approaches. We extensively evaluate CNN-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches. We show that character-level CNNs outperform the state-of-the-art on four out of five datasets in different domains. Additionally, we present the first application of authorship attribution to reddit.},
	urldate = {2020-04-08},
	journal = {arXiv:1609.06686 [cs]},
	author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G.},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.06686},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 5 figures, 3 tables},
	file = {arXiv Fulltext PDF:/home/thibault/Zotero/storage/8ZTWVC7P/Ruder et al. - 2016 - Character-level and Multi-channel Convolutional Ne.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/Zotero/storage/D9BK7F5D/1609.html:text/html}
}

@article{nagy_metre_nodate,
	title = {Metre as a stylometric feature for {Latin} hexameter poetry},
	author = {Nagy, Ben}
}

@misc{bamman_11k_2012,
	title = {{11K} {Latin} {Texts}},
	url = {https://www.cs.cmu.edu/~dbamman/latin.html},
	author = {Bamman, David},
	year = {2012}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-04-10},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Publisher: MIT Press},
	pages = {1735--1780},
	file = {Snapshot:/home/thibault/Zotero/storage/MTWGA9LD/neco.1997.9.8.html:text/html}
}


@misc{nguyen_illustrated_2019,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2020-04-10},
	journal = {Medium},
	author = {Nguyen, Michael},
	month = jul,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/thibault/Zotero/storage/BKB4RRWB/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html}
}


@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2020-04-13},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Fondateur GRU},
	file = {arXiv Fulltext PDF:/home/thibault/Zotero/storage/LM9DCE9D/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/Zotero/storage/ZT9A58QP/1409.html:text/html}
}


@misc{lynn_get_2018,
	title = {Get {Busy} with {Word} {Embeddings} – {An} {Introduction}},
	url = {https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/},
	abstract = {This post provides an introduction to “word embeddings” or “word vectors”. Word embeddings are real-number vectors that represent words from a vocabulary, and have broad app…},
	language = {en-US},
	urldate = {2020-04-15},
	journal = {Shane Lynn},
	author = {Lynn, Shane},
	month = feb,
	year = {2018},
	note = {Library Catalog: www.shanelynn.ie},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/2DE7A3SH/get-busy-with-word-embeddings-introduction.html:text/html}
}