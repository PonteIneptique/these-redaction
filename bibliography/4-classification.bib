@inproceedings{shen_large-scale_2021,
	title = {Large-{Scale} {Historical} {Watermark} {Recognition}: dataset and a new consistency-based approach},
	shorttitle = {Large-{Scale} {Historical} {Watermark} {Recognition}},
	doi = {10.1109/ICPR48806.2021.9412762},
	abstract = {Historical watermark recognition is a highly practical, yet unsolved challenge for archivists and historians. With a large number of well-defined classes, cluttered and noisy samples, different types of representations, both subtle differences between classes and high intra-class variation, historical watermarks are also challenging for pattern recognition. In this paper, overcoming the difficulty of data collection, we present a large public dataset with more than 6k new photographs, allowing for the first time to tackle at scale the scenarios of practical interest for scholars: one-shot instance recognition and cross-domain one-shot instance recognition amongst more than 16k fine-grained classes. We demonstrate that this new dataset is large enough to train modern deep learning approaches, and show that standard methods can be improved considerably by using mid-level deep features. More precisely, we design both a matching score and a feature fine-tuning strategy based on filtering local matches using spatial consistency. This consistency-based approach provides important performance boost compared to strong baselines. Our model achieves 55\% top-1 accuracy on our very challenging 16,753-class one-shot cross-domain recognition task, each class described by a single drawing from the classic Briquet catalog. In addition to watermark classification, we show our approach provides promising results on fine-grained sketch-based image retrieval.},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Shen, Xi and Pastrolin, Ilaria and Bounou, Oumayma and Gidaris, Spyros and Smith, Marc and Poncet, Olivier and Aubry, Mathieu},
	month = jan,
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Data collection, Deep learning, Filtering, Image recognition, Image retrieval, Pattern recognition, Watermarking},
	pages = {6810--6817},
	file = {Submitted Version:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9QL9GDBN/Shen et al. - 2021 - Large-Scale Historical Watermark Recognition data.pdf:application/pdf},
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}


@inproceedings{ni_justifying_2019,
	address = {Hong Kong, China},
	title = {Justifying {Recommendations} using {Distantly}-{Labeled} {Reviews} and {Fine}-{Grained} {Aspects}},
	url = {https://www.aclweb.org/anthology/D19-1018},
	doi = {10.18653/v1/D19-1018},
	abstract = {Several recent works have considered the problem of generating reviews (or ‘tips’) as a form of explanation as to why a recommendation might match a user’s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justiﬁcations that are relevant to users’ decision-making process. We seek to introduce new datasets and methods to address this recommendation justiﬁcation task. In terms of data, we ﬁrst propose an ‘extractive’ approach to identify review segments which justify users’ intentions; this approach is then used to distantly label massive review corpora and construct largescale personalized recommendation justiﬁcation datasets. In terms of generation, we design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justiﬁcations covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justiﬁcations based on templates extracted from justiﬁcation histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justiﬁcations.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
	year = {2019},
	pages = {188--197},
	file = {Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9PGP2HUM/Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:application/pdf},
}


@book{lakoff_metaphors_2003,
	address = {Chicago},
	edition = {New édition},
	title = {Metaphors {We} {Live} {By}},
	isbn = {978-0-226-46801-3},
	abstract = {The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are "metaphors we live by"—metaphors that can shape our perceptions and actions without our ever noticing them.  In this updated edition of Lakoff and Johnson's influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.},
	language = {Anglais},
	publisher = {University of Chicago Press},
	author = {Lakoff, George and Johnson, Mark},
	year = {2003},
}


@inproceedings{leong_report_2018,
	address = {New Orleans, Louisiana},
	title = {A {Report} on the 2018 {VUA} {Metaphor} {Detection} {Shared} {Task}},
	url = {https://www.aclweb.org/anthology/W18-0907},
	doi = {10.18653/v1/W18-0907},
	abstract = {As the community working on computational approaches to figurative language is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area. One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset. We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing.},
	urldate = {2019-12-08},
	booktitle = {Proceedings of the {Workshop} on {Figurative} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Leong, Chee Wee (Ben) and Beigman Klebanov, Beata and Shutova, Ekaterina},
	month = jun,
	year = {2018},
	pages = {56--66},
	annote = {Résumé
Dataset
Set de données de 117 fragments sur 4 genres (Academic, News, Conversation, Fiction) dont 23\% est réservé pour le test.
Inter-annotator reliability : K {\textgreater} 0.8
Notes générales

Le seul outil n'ayant pas utilisé de DNN fait dernier sur chaque tâche.
Mesure par FScore
Beaucoup de participants ont utilisé la cross-validation.
2 des 3 meilleurs ont utilisé des features linguistiques en plus de features type embeddings

Modèles à creuser ainsi que features

bot.zen (Stemle and Onysko, 2018) (3 en all pos, 2 en verb uniquement): usage de différents embeddings de différents corpora.

Dans le cadre du latin, intéressant d'utiliser différentes périodes ?
Entraîner Elmo sur un corpus all periods ?


nsu\_ai n'utilise pas de DNN mais par contre a proposé une utilisation de LDA... Vecteur de LDA pour du DNN ?
},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/IC8KI9GI/Leong et al. - 2018 - A Report on the 2018 VUA Metaphor Detection Shared.pdf:application/pdf;Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/KP2GS77C/W18-0907.html:text/html},
}


@book{steen_method_2010,
	title = {A {Method} for {Linguistic} {Metaphor} {Identification}. {From} {MIP} to {MIPVU}},
	isbn = {978-90-272-8815-8},
	url = {https://benjamins.com/catalog/celcr.14},
	abstract = {This book presents a complete method for the identification of metaphor in language at the level of word use. It is based on extensive methodological and empirical corpus-linguistic research in two languages, English and Dutch.},
	language = {English},
	urldate = {2021-08-25},
	publisher = {John Benjamins Publishing Company},
	author = {Steen, Gerard J. {\textbar}Dorst},
	year = {2010},
	note = {Publication Title: celcr.14},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/Y4I9IH3I/celcr.html:text/html},
}


@inproceedings{stemle_using_2018,
	address = {New Orleans, Louisiana},
	title = {Using {Language} {Learner} {Data} for {Metaphor} {Detection}},
	url = {http://aclweb.org/anthology/W18-0918},
	doi = {10.18653/v1/W18-0918},
	abstract = {This article describes the system that participated in the shared task (ST) on metaphor detection (Leong et al., 2018) on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing ﬁgurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018).},
	language = {en},
	urldate = {2021-08-25},
	booktitle = {Proceedings of the {Workshop} on {Figurative} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Stemle, Egon and Onysko, Alexander},
	year = {2018},
	pages = {133--138},
	file = {Stemle and Onysko - 2018 - Using Language Learner Data for Metaphor Detection.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/YS5NRPBF/Stemle and Onysko - 2018 - Using Language Learner Data for Metaphor Detection.pdf:application/pdf},
}


@inproceedings{klebanov_argumentation-relevant_2013,
	title = {Argumentation-relevant metaphors in test-taker essays},
	booktitle = {Proceedings of the {First} {Workshop} on {Metaphor} in {NLP}},
	author = {Klebanov, Beata Beigman and Flor, Michael},
	year = {2013},
	pages = {11--20},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/YTE86SZ4/Klebanov and Flor - 2013 - Argumentation-relevant metaphors in test-taker ess.pdf:application/pdf},
}


@inproceedings{leong_report_2020,
	address = {Online},
	title = {A {Report} on the 2020 {VUA} and {TOEFL} {Metaphor} {Detection} {Shared} {Task}},
	url = {https://aclanthology.org/2020.figlang-1.3},
	doi = {10.18653/v1/2020.figlang-1.3},
	abstract = {In this paper, we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus. The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language.},
	urldate = {2021-08-25},
	booktitle = {Proceedings of the {Second} {Workshop} on {Figurative} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Leong, Chee Wee (Ben) and Beigman Klebanov, Beata and Hamill, Chris and Stemle, Egon and Ubale, Rutuja and Chen, Xianyang},
	month = jul,
	year = {2020},
	pages = {18--29},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/TZMGNVV5/Leong et al. - 2020 - A Report on the 2020 VUA and TOEFL Metaphor Detect.pdf:application/pdf},
}


@article{herrmann_high_2015,
	title = {High on metaphor, low on simile? {An} examination of metaphor type in sub-registers of academic prose},
	shorttitle = {High on metaphor, low on simile?},
	url = {https://www.jbe-platform.com/content/books/9789027267849-milcc.4.07her},
	abstract = {Recent corpus studies found that academic prose is particularly rich in metaphor,but exhibits an unexpectedly low proportion of forms of \&amp;\#8216;direct metaphor\&amp;\#8217;,such as simile (cf. Steen, Dorst, Herrmann, Kaal, \&amp;\#38; Krennmayr, 2010a; Steenet al., 2010b). One explanation is deliberate metaphor use: in opposition toindirect forms (he attacked my argument), direct forms of metaphor (the leaf isshaped like a minaret) are normally explicitly signaled and often appear morevividly \&amp;\#8216;metaphorical\&amp;\#8217;. To control precision of linguistic reference, and to abideby an overarching stylistic maxim of academic prose that regulates markedfigurativeness, writers of academic texts may thus try to delimitate deliberatemetaphor use in the form of direct metaphor.However, recent advances in the study of English for Academic Purposeshave stressed that the analysis of academic discourse cannot ignore \&amp;\#8216;disciplinaryspecificity\&amp;\#8217; (cf. Hyland, 2009). Using an exploratory approach, the present chapterhence transgresses the rather broad unit of \&amp;\#8216;register\&amp;\#8217; to zoom in on academicprose as specialist discourse of distinct \&amp;\#8216;sub-registers\&amp;\#8217;. Using the academic textsample (some 49,000 words) of the VUAMC (Steen et al. 2010c), it analyzesthree metaphor types (indirect, implicit, and direct) across four different academicsub-registers (humanities arts, natural sciences, politics law education,social sciences). I report variation of metaphor type across the sub-registers,with the highest proportion of direct metaphors in natural sciences, followed byhumanities arts. My findings on variation of metaphor type advances a finergrainedview of metaphor use in academic prose, taking into account distinctcommunicative functions of metaphor types.},
	language = {en},
	urldate = {2021-08-25},
	journal = {Metaphor in Specialist Discourse},
	author = {Herrmann, J. Berenike},
	month = dec,
	year = {2015},
	note = {Publisher: John Benjamins},
	pages = {163--190},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/CI7CAB4U/9789027267849-milcc.4.html:text/html},
}


@inproceedings{kesarwani_metaphor_2017,
	address = {Vancouver, Canada},
	title = {Metaphor {Detection} in a {Poetry} {Corpus}},
	url = {http://aclweb.org/anthology/W17-2201},
	doi = {10.18653/v1/W17-2201},
	abstract = {Metaphor is indispensable in poetry. It showcases the poet’s creativity, and contributes to the overall emotional pertinence of the poem while honing its speciﬁc rhetorical impact. Previous work on metaphor detection relies on either rulebased or statistical models, none of them applied to poetry. Our method focuses on metaphor detection in a poetry corpus. It combines rule-based and statistical models (word embeddings) to develop a new classiﬁcation system. Our system has achieved a precision of 0.759 and a recall of 0.804 in identifying one type of metaphor in poetry.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics}           for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Kesarwani, Vaibhav and Inkpen, Diana and Szpakowicz, Stan and Tanasescu, Chris},
	year = {2017},
	pages = {1--9},
	file = {Kesarwani et al. - 2017 - Metaphor Detection in a Poetry Corpus.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/4PHBEUN4/Kesarwani et al. - 2017 - Metaphor Detection in a Poetry Corpus.pdf:application/pdf},
}


@book{darbo-peschanski_citation_2004,
	title = {La citation dans l'antiquité: actes du colloque du {PARSA}, {Lyon}, {ENS} {LSH}, 6-8 novembre 2002},
	shorttitle = {La citation dans l'antiquité},
	publisher = {Editions Jérôme Millon},
	author = {Darbo-Peschanski, Catherine},
	year = {2004},
}


@incollection{aron_intertextualite_2010,
	address = {Paris, France},
	title = {Intertextualité},
	isbn = {978-2-13-056628-1},
	language = {français},
	booktitle = {Le dictionnaire du littéraire},
	publisher = {Presses universitaires de France},
	author = {Chassay, Jean-François},
	editor = {Aron, Paul and Saint-Jacques, Denis and Viala, Alain},
	year = {2010},
	note = {ISSN: 1762-7370},
	keywords = {Littérature -- Histoire et critique, Littérature -- Terminologie},
	pages = {317},
	annote = {Autres tirages : 2014, 2016, 2018},
	file = {Library Catalog Entry Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/8ERJMRVD/SRCH.html:text/html},
}

@incollection{aron_plagiat_2010,
	address = {Paris, France},
	title = {Plagiat},
	isbn = {978-2-13-056628-1},
	language = {français},
	booktitle = {Le dictionnaire du littéraire},
	publisher = {Presses universitaires de France},
	author = {Chassay, Jean-François},
	editor = {Aron, Paul and Saint-Jacques, Denis and Viala, Alain},
	year = {2010},
	note = {ISSN: 1762-7370},
	keywords = {Littérature -- Histoire et critique, Littérature -- Terminologie},
	pages = {458},
	annote = {Autres tirages : 2014, 2016, 2018},
	file = {Library Catalog Entry Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/8ERJMRVD/SRCH.html:text/html},
}


@book{forstall_quantitative_2019,
	address = {Cham},
	title = {Quantitative {Intertextuality}: {Analyzing} the {Markers} of {Information} {Reuse}},
	shorttitle = {Quantitative {Intertextuality}},
	url = {http://link.springer.com/10.1007/978-3-030-23415-7},
	language = {en},
	urldate = {2021-08-25},
	publisher = {Springer International Publishing},
	author = {Forstall, Christopher W. and Scheirer, Walter J.},
	year = {2019},
	doi = {10.1007/978-3-030-23415-7},
}


@inproceedings{smith_infectious_2013,
	title = {Infectious texts: {Modeling} text reuse in nineteenth-century newspapers},
	shorttitle = {Infectious texts},
	doi = {10.1109/BigData.2013.6691675},
	abstract = {Texts propagate through many social networks and provide evidence for their structure. We present efficient algorithms for detecting clusters of reused passages embedded within longer documents in large collections. We apply these techniques to analyzing the culture of reprinting in the United States before the Civil War. Without substantial copyright enforcement, stories, poems, news, and anecdotes circulated freely among newspapers, magazines, and books. From a collection of OCR'd newspapers, we extract a new corpus of reprinted texts, explore the geographic spread and network connections of different publications, and analyze the time dynamics of different genres.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Smith, David A. and Cordell, Ryan and Dillon, Elizabeth Maddock},
	month = oct,
	year = {2013},
	keywords = {Aggregates, Clustering algorithms, Heuristic algorithms, Indexing, Optical character recognition software, Social network services},
	pages = {86--94},
	file = {IEEE Xplore Abstract Record:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/YCXRHCVT/6691675.html:text/html;Submitted Version:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9GYSRAU9/Smith et al. - 2013 - Infectious texts Modeling text reuse in nineteent.pdf:application/pdf},
}

@article{smith_computational_2015,
	title = {Computational {Methods} for {Uncovering} {Reprinted} {Texts} in {Antebellum} {Newspapers}},
	volume = {27},
	issn = {0896-7148},
	url = {https://doi.org/10.1093/alh/ajv029},
	doi = {10.1093/alh/ajv029},
	number = {3},
	urldate = {2021-08-25},
	journal = {American Literary History},
	author = {Smith, David A. and Cordell, Ryan and Mullen, Abby},
	month = sep,
	year = {2015},
	pages = {E1--E15},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/BVVU9RAS/86032.html:text/html},
}


@article{coffee_tesserae_2013,
	title = {The {Tesserae} {Project}: intertextual analysis of {Latin} poetry},
	volume = {28},
	issn = {0268-1145},
	shorttitle = {The {Tesserae} {Project}},
	url = {https://doi.org/10.1093/llc/fqs033},
	doi = {10.1093/llc/fqs033},
	abstract = {Tesserae is a web-based tool for automatically detecting allusions in Latin poetry. Although still in the start-up phase, it already is capable of identifying significant numbers of known allusions, as well as similar numbers of allusions previously unnoticed by scholars. In this article, we use the tool to examine allusions to Vergil’s Aeneid in the first book of Lucan’s Civil War. Approximately 3,000 linguistic parallels returned by the program were compared with a list of known allusions drawn from commentaries. Each was examined individually and graded for its literary significance, in order to benchmark the program’s performance. All allusions from the program and commentaries were then pooled in order to examine broad patterns in Lucan’s allusive techniques which were largely unapproachable without digital methods. Although Lucan draws relatively constantly from Vergil’s generic language in order to maintain the epic idiom, this baseline is punctuated by clusters of pointed allusions, in which Lucan frequently subverts Vergil’s original meaning. These clusters not only attend the most significant characters and events but also play a role in structuring scene transitions. Work is under way to incorporate the ability to match on word meaning, phrase context, as well as metrical and phonological features into future versions of the program.},
	number = {2},
	urldate = {2021-08-25},
	journal = {Literary and Linguistic Computing},
	author = {Coffee, Neil and Koenig, Jean-Pierre and Poornima, Shakthi and Forstall, Christopher W. and Ossewaarde, Roelant and Jacobson, Sarah L.},
	month = jun,
	year = {2013},
	pages = {221--228},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/93M2SSDL/1033788.html:text/html},
}


@inproceedings{manjavacas_feasibility_2019,
	address = {Minneapolis, USA},
	title = {On the {Feasibility} of {Automated} {Detection} of {Allusive} {Text} {Reuse}},
	url = {https://aclanthology.org/W19-2514},
	doi = {10.18653/v1/W19-2514},
	abstract = {The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely — commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.},
	urldate = {2021-08-25},
	booktitle = {Proceedings of the 3rd {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Manjavacas, Enrique and Long, Brian and Kestemont, Mike},
	month = jun,
	year = {2019},
	pages = {104--114},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/L4S6Z3GW/Manjavacas et al. - 2019 - On the Feasibility of Automated Detection of Allus.pdf:application/pdf},
}


@inproceedings{blackburn_stfu_2014,
	address = {New York, NY, USA},
	series = {{WWW} '14},
	title = {{STFU} {NOOB}! predicting crowdsourced decisions on toxic behavior in online games},
	isbn = {978-1-4503-2744-2},
	url = {https://doi.org/10.1145/2566486.2567987},
	doi = {10.1145/2566486.2567987},
	abstract = {One problem facing players of competitive games is negative, or toxic, behavior. League of Legends, the largest eSport game, uses a crowdsourcing platform called the Tribunal to judge whether a reported toxic player should be punished or not. The Tribunal is a two stage system requiring reports from those players that directly observe toxic behavior, and human experts that review aggregated reports. While this system has successfully dealt with the vague nature of toxic behavior by majority rules based on many votes, it naturally requires tremendous cost, time, and human efforts. In this paper, we propose a supervised learning approach for predicting crowdsourced decisions on toxic behavior with large-scale labeled data collections; over 10 million user reports involved in 1.46 million toxic players and corresponding crowdsourced decisions. Our result shows good performance in detecting overwhelmingly majority cases and predicting crowdsourced decisions on them. We demonstrate good portability of our classifier across regions. Finally, we estimate the practical implications of our approach, potential cost savings and victim protection.},
	urldate = {2021-08-25},
	booktitle = {Proceedings of the 23rd international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {Blackburn, Jeremy and Kwak, Haewoon},
	month = apr,
	year = {2014},
	keywords = {crowdsourcing, league of legends, machine learning, online video games, toxic behavior},
	pages = {877--888},
}


@techreport{bradley_affective_1999,
	title = {Affective norms for {English} words ({ANEW}): {Instruction} manual and affective ratings},
	shorttitle = {Affective norms for {English} words ({ANEW})},
	institution = {Technical report C-1, the center for research in psychophysiology …},
	author = {Bradley, Margaret M. and Lang, Peter J.},
	year = {1999},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/8MSCPWRS/Bradley and Lang - 1999 - Affective norms for English words (ANEW) Instruct.pdf:application/pdf},
}


@article{vo_automatically_2021,
	title = {Automatically {Detecting} {Cyberbullying} {Comments} on {Online} {Game} {Forums}},
	url = {http://arxiv.org/abs/2106.01598},
	abstract = {Online game forums are popular to most of game players. They use it to communicate and discuss the strategy of the game, or even to make friends. However, game forums also contain abusive and harassment speech, disturbing and threatening players. Therefore, it is necessary to automatically detect and remove cyberbullying comments to keep the game forum clean and friendly. We use the Cyberbullying dataset collected from World of Warcraft (WoW) and League of Legends (LoL) forums and train classification models to automatically detect whether a comment of a player is abusive or not. The result obtains 82.69\% of macro F1-score for LoL forum and 83.86\% of macro F1-score for WoW forum by the Toxic-BERT model on the Cyberbullying dataset.},
	urldate = {2021-08-26},
	journal = {arXiv:2106.01598 [cs]},
	author = {Vo, Hanh Hong-Phuc and Tran, Hieu Trung and Luu, Son T.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.01598},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at RIVF 2021 Conference},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/266TVCFP/Vo et al. - 2021 - Automatically Detecting Cyberbullying Comments on .pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/6NVRTN22/2106.html:text/html},
}


@inproceedings{inches_overview_2012,
	title = {Overview of the {International} {Sexual} {Predator} {Identification} {Competition} at {PAN}-2012.},
	volume = {30},
	booktitle = {{CLEF} ({Online} working notes/labs/workshop)},
	author = {Inches, Giacomo and Crestani, Fabio},
	year = {2012},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9P2SHZTM/Inches et Crestani - 2012 - Overview of the International Sexual Predator Iden.pdf:application/pdf},
}

@inproceedings{munoz_smartsec4cop_2020,
	title = {{SMARTSEC4COP}: {Smart} {Cyber}-{Grooming} {Detection} {Using} {Natural} {Language} {Processing} and {Convolutional} {Neural} {Networks}},
	shorttitle = {{SMARTSEC4COP}},
	booktitle = {International {Symposium} on {Distributed} {Computing} and {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Muñoz, Fabián and Isaza, Gustavo and Castillo, Luis},
	year = {2020},
	pages = {11--20},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/CZ9FZUIU/Muñoz et al. - 2020 - SMARTSEC4COP Smart Cyber-Grooming Detection Using.pdf:application/pdf;Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/YILAH2GX/978-3-030-53036-5_2.html:text/html},
}


@inproceedings{vogt_early_2021,
	address = {Online},
	title = {Early {Detection} of {Sexual} {Predators} in {Chats}},
	url = {https://aclanthology.org/2021.acl-long.386},
	doi = {10.18653/v1/2021.acl-long.386},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vogt, Matthias and Leser, Ulf and Akbik, Alan},
	year = {2021},
	pages = {4985--4999},
	file = {Vogt et al. - 2021 - Early Detection of Sexual Predators in Chats.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/PRYUCGX6/Vogt et al. - 2021 - Early Detection of Sexual Predators in Chats.pdf:application/pdf},
}

@inproceedings{das_google_2007,
	address = {New York, NY, USA},
	series = {{WWW} '07},
	title = {Google news personalization: scalable online collaborative filtering},
	isbn = {978-1-59593-654-7},
	shorttitle = {Google news personalization},
	url = {https://doi.org/10.1145/1242572.1242610},
	doi = {10.1145/1242572.1242610},
	abstract = {Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News.},
	urldate = {2021-08-27},
	booktitle = {Proceedings of the 16th international conference on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Das, Abhinandan S. and Datar, Mayur and Garg, Ashutosh and Rajaram, Shyam},
	month = may,
	year = {2007},
	keywords = {Google news, mapreduce, minhash, oneline recommendation system, personalization, PLSI, scalable collaborative filtering},
	pages = {271--280},
}



@inproceedings{nugent_comparison_2017,
	title = {A comparison of classification models for natural disaster and critical event detection from news},
	doi = {10.1109/BigData.2017.8258374},
	abstract = {We present a contrastive study of document-level event classification of a range of seven different event types, namely floods, storms, fires, armed conflict, terrorism, infrastructure breakdown and labour unavailability from English-language news. Our study compares different supervised classification approaches, namely Support Vector Machine (SVM), Random Forest (RF), Convolutional Neural Network (CNN) and Hierarchical Attention Network (HAN). While past systems for Topic Detection and Tracking (TDT) and event extraction have proposed different machine learning models, to date SVMs, RFs, CNNs and HANs have not been compared on this task. Our classifiers are also informed by word embeddings trained on large amounts of high-quality agency news, which leads to improvements compared to the use of pre-trained embedding vectors. We report a detailed quantitative error analysis.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Nugent, Tim and Petroni, Fabio and Raman, Natraj and Carstens, Lucas and Leidner, Jochen L.},
	month = dec,
	year = {2017},
	keywords = {Electric breakdown, Event detection, Event Extraction, Radio frequency, Storms, Support vector machines, Terrorism, Text Classification, Topic Detection and Tracking (TDT)},
	pages = {3750--3759},
	file = {IEEE Xplore Abstract Record:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/I2S47JJC/authors.html:text/html},
}


@inproceedings{shah_neural_2018,
	title = {Neural network based extreme classification and similarity models for product matching},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 3 ({Industry} {Papers})},
	author = {Shah, Kashif and Kopru, Selcuk and Ruvini, Jean David},
	year = {2018},
	pages = {8--15},
	annote = {Résumé
Comparaison de deux méthodes (fasttext et bilstm siamois) pour des classifications de textes courts (100 mots) avec autres données.
Pas clair si les mots ont été projetés sur un espace d'embeddings. Le code source dont ils se sont inspirés l'utilise.
Les données de bases ne sont pas parfaites: génération automatique des classes, qui se sont avérées fausses pour certaines.
Fasttext fonctionne bien et rapide, les réseaux siamois plus long et moins légèrement meilleurs (de {\textasciitilde}{\textless}1\% à {\textasciitilde}{\textless}4\%)
Bon article d'introduction, citation d'un code},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/N5BFUE7I/Shah et al. - 2018 - Neural network based extreme classification and si.pdf:application/pdf},
}


@incollection{aggarwal_survey_2012,
	address = {Boston, MA},
	title = {A {Survey} of {Text} {Classification} {Algorithms}},
	isbn = {978-1-4614-3223-4},
	url = {https://doi.org/10.1007/978-1-4614-3223-4_6},
	abstract = {The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms.},
	language = {en},
	urldate = {2021-08-27},
	booktitle = {Mining {Text} {Data}},
	publisher = {Springer US},
	author = {Aggarwal, Charu C. and Zhai, ChengXiang},
	editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
	year = {2012},
	doi = {10.1007/978-1-4614-3223-4_6},
	keywords = {Text Classification},
	pages = {163--222},
	file = {Springer Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/5T6D3W9H/Aggarwal and Zhai - 2012 - A Survey of Text Classification Algorithms.pdf:application/pdf},
}


@article{kestemont_authenticating_2016,
	title = {Authenticating the writings of {Julius} {Caesar}},
	volume = {63},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417416303116},
	doi = {10.1016/j.eswa.2016.06.029},
	abstract = {In this paper, we shed new light on the authenticity of the Corpus Caesarianum, a group of five commentaries describing the campaigns of Julius Caesar (100–44 BC), the founder of the Roman empire. While Caesar himself has authored at least part of these commentaries, the authorship of the rest of the texts remains a puzzle that has persisted for nineteen centuries. In particular, the role of Caesar’s general Aulus Hirtius, who has claimed a role in shaping the corpus, has remained in contention. Determining the authorship of documents is an increasingly important authentication problem in information and computer science, with valuable applications, ranging from the domain of art history to counter-terrorism research. We describe two state-of-the-art authorship verification systems and benchmark them on 6 present-day evaluation corpora, as well as a Latin benchmark dataset. Regarding Caesar’s writings, our analyses allow us to establish that Hirtius’s claims to part of the corpus must be considered legitimate. We thus demonstrate how computational methods constitute a valuable methodological complement to traditional, expert-based approaches to document authentication.},
	language = {en},
	urldate = {2021-08-27},
	journal = {Expert Systems with Applications},
	author = {Kestemont, Mike and Stover, Justin and Koppel, Moshe and Karsdorp, Folgert and Daelemans, Walter},
	month = nov,
	year = {2016},
	keywords = {Authentication, Authorship verification, Julius Caesar, Stylometry},
	pages = {86--96},
	file = {Accepted Version:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/6RMWJMQN/Kestemont et al. - 2016 - Authenticating the writings of Julius Caesar.pdf:application/pdf;ScienceDirect Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/5UUKSSNS/S0957417416303116.html:text/html},
}


@article{gorman_author_2020,
	title = {Author identification of short texts using dependency treebanks without vocabulary},
	volume = {35},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqz070},
	doi = {10.1093/llc/fqz070},
	abstract = {How to classify short texts effectively remains an important question in computational stylometry. This study presents the results of an experiment involving authorship attribution of ancient Greek texts. These texts were chosen to explore the effectiveness of digital methods as a supplement to the author’s work on text classification based on traditional stylometry. Here it is crucial to avoid confounding effects of shared topic, etc. Therefore, this study attempts to identify authorship using only morpho-syntactic data without regard to specific vocabulary items. The data are taken from the dependency annotations published in the Ancient Greek and Latin Dependency Treebank. The independent variables for classification are combinations generated from the dependency label and the morphology of each word in the corpus and its dependency parent. To avoid the effects of the combinatorial explosion, only the most frequent combinations are retained as input features. The authorship classification (with thirteen classes) is done with standard algorithms—logistic regression and support vector classification. During classification, the corpus is partitioned into increasingly smaller ‘texts’. To explore and control for the possible confounding effects of, e.g. different genre and annotator, three corpora were tested: a mixed corpus of several genres of both prose and verse, a corpus of prose including oratory, history, and essay, and a corpus restricted to narrative history. Results are surprisingly good as compared to those previously published. Accuracy for fifty-word inputs is 84.2–89.6\%. Thus, this approach may prove an important addition to the prevailing methods for small text classification.},
	number = {4},
	urldate = {2021-08-27},
	journal = {Digital Scholarship in the Humanities},
	author = {Gorman, Robert},
	month = dec,
	year = {2020},
	pages = {812--825},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/RYGL9FRQ/Gorman - 2020 - Author identification of short texts using depende.pdf:application/pdf},
}


@article{nagy_metre_2021,
	title = {Metre as a stylometric feature in {Latin} hexameter poetry},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqaa043},
	doi = {10.1093/llc/fqaa043},
	abstract = {This article demonstrates that metre is a privileged indicator of authorial style in classical Latin hexameter poetry. Using only metrical features, classification experiments are performed between the works of six authors using four different machine-learning models. The results showed a pairwise classification accuracy of at least 90\% with samples as small as ten lines and no greater than seventy-five lines (up to around 500 words). In a multiclass setting, classification accuracy exceeded 95\% for all four algorithms when using eighty-one-line chunks. These sample sizes are an order of magnitude smaller than those typically recommended for BOW (‘bag of words’) or n-gram approaches, and the reported accuracy is outstanding. Additionally, this article explores the potential for outlier (forgery) detection, or ‘one-class classification’. As an example, analysis of the disputed Aldine Additamentum (Sil. Ital. Pun. 8:144–223) concludes (P \&lt; 0.0001) that the metrical style differs significantly from that of the rest of the poem.},
	number = {fqaa043},
	urldate = {2021-08-27},
	journal = {Digital Scholarship in the Humanities},
	author = {Nagy, Benjamin},
	month = feb,
	year = {2021},
}


@inproceedings{vanni_textual_2018,
	address = {Melbourne, Australia},
	title = {Textual {Deconvolution} {Saliency} ({TDS}) : a deep tool box for linguistic analysis},
	shorttitle = {Textual {Deconvolution} {Saliency} ({TDS})},
	url = {http://aclweb.org/anthology/P18-1051},
	doi = {10.18653/v1/P18-1051},
	abstract = {In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classiﬁcation. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efﬁciency of our Text Deconvolution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.},
	language = {en},
	urldate = {2021-08-27},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vanni, Laurent and Ducoffe, Melanie and Aguilar, Carlos and Precioso, Frederic and Mayaffre, Damon},
	year = {2018},
	pages = {548--557},
	file = {Vanni et al. - 2018 - Textual Deconvolution Saliency (TDS)  a deep tool.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/TW6WK76X/Vanni et al. - 2018 - Textual Deconvolution Saliency (TDS)  a deep tool.pdf:application/pdf},
}


@inproceedings{fell_comparing_2019,
	address = {Varna, Bulgaria},
	title = {Comparing {Automated} {Methods} to {Detect} {Explicit} {Content} in {Song} {Lyrics}},
	url = {https://aclanthology.org/R19-1039},
	doi = {10.26615/978-954-452-056-4_039},
	abstract = {The Parental Advisory Label (PAL) is a warning label that is placed on audio recordings in recognition of profanity or inappropriate references, with the intention of alerting parents of material potentially unsuitable for children. Since 2015, digital providers – such as iTunes, Spotify, Amazon Music and Deezer – also follow PAL guidelines and tag such tracks as “explicit”. Nowadays, such labelling is carried out mainly manually on voluntary basis, with the drawbacks of being time consuming and therefore costly, error prone and partly a subjective task. In this paper, we compare automated methods ranging from dictionary-based lookup to state-of-the-art deep neural networks to automatically detect explicit contents in English lyrics. We show that more complex models perform only slightly better on this task, and relying on a qualitative analysis of the data, we discuss the inherent hardness and subjectivity of the task.},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2019)},
	publisher = {INCOMA Ltd.},
	author = {Fell, Michael and Cabrio, Elena and Corazza, Michele and Gandon, Fabien},
	month = sep,
	year = {2019},
	pages = {338--344},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/NNGS29RU/Fell et al. - 2019 - Comparing Automated Methods to Detect Explicit Con.pdf:application/pdf},
}



@misc{juola_how_2013,
	type = {News},
	title = {How a {Computer} {Program} {Helped} {Show} {J}.{K}. {Rowling} write {A} {Cuckoo}’s {Calling}},
	url = {https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/},
	abstract = {Author of the Harry Potter books has a distinct linguistic signature},
	language = {en},
	urldate = {2021-08-30},
	journal = {Scientific American},
	author = {Juola, Patrick},
	month = aug,
	year = {2013},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/6B328TUQ/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling.html:text/html},
}

@article{stylo_r,
    title        = "Stylometry with R: a package for computational text analysis",
    author       = {Eder, Maciej and Rybicki, Jan and Kestemont, Mike}, 
    journal      = "R Journal",
    year         = "2016",
    volume       = "8",
    number       = "1",
    pages        = "107--121",
    url          = "https://journal.r-project.org/archive/2016/RJ-2016-007/index.html",
}


@article{jarvis_capturing_2019,
	title = {Capturing the {Diversity} in {Lexical} {Diversity}},
	volume = {19},
	copyright = {Copyright (c) 2019 Scott Jarvis},
	issn = {1524-2110},
	url = {https://scholarworks.iu.edu/journals/index.php/iulcwp/article/view/26883},
	abstract = {The range, variety, or diversity of words found in learners’ language use is believed to reflect the complexity of their vocabulary knowledge as well as the level of their language proficiency. Many indices of lexical diversity have been proposed, most of which involve statistical relationships between types and tokens, and which ultimately reflect the rate of word repetition. These indices have generally been validated in accordance with how well they overcome sample-size effects and/or how well they predict language knowledge or behavior, rather than in accordance with how well they actually measure the construct of lexical diversity. In this article, I review developments that have taken place in lexical diversity research, and also describe obstacles that have prevented it from advancing further. I compare these developments with parallel research on biodiversity in the field of ecology, and show what language researchers can learn from ecology regarding the modeling and measurement of diversity as a multidimensional construct of compositional complexity.},
	language = {en},
	number = {2},
	urldate = {2021-09-02},
	journal = {IULC Working Papers},
	author = {Jarvis, Scott},
	month = feb,
	year = {2019},
	note = {Number: 2},
	keywords = {Indiana University, linguistics, working papers},
	annote = {p.44  These terms are usually operationalized into measures designed to capture the proportionof words in a language sample that are not repetitions of words already encountered
MTLD -{\textgreater} McCarthy's 2005
 
p. 52 As a starting point, language researchers should consider at least the following properties of diversity: 1. size (number of tokens)2. richness (number of types)3. effective number of types (e.g., the exponential function applied to Shannon’s index;MacArthur, 1965)4. evenness (e.g., the degree to which tokens are distributed equally across types)5. disparity (e.g., the proportion of words in a text that are semantically related)6. importance (e.g., the relative frequency with which the words in a text occur in thelanguage as a whole; cf. lexical sophistication)7. dispersion (e.g., the average interval between tokens of the same type)
 
Carthy, P. M. (2005). An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity (MTLD) [Microfiche]. Doctoral dissertation, University of Memphis.},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/L2744GYT/Jarvis - 2019 - Capturing the Diversity in Lexical Diversity.pdf:application/pdf},
}


@phdthesis{mccarthy_assessment_2005,
	type = {{PhD} {Thesis}},
	title = {An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity ({MTLD})},
	school = {The University of Memphis},
	author = {McCarthy, Philip M.},
	year = {2005},
}


@inproceedings{zipitria_observing_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Observing {Lemmatization} {Effect} in {LSA} {Coherence} and {Comprehension} {Grading} of {Learner} {Summaries}},
	isbn = {978-3-540-35160-3},
	doi = {10.1007/11774303_59},
	abstract = {Current work in learner evaluation of Intelligent Tutoring Systems (ITSs), is moving towards open-ended educational content diagnosis. One of the main difficulties of this approach is to be able to automatically understand natural language. Our work is directed to produce automatic evaluation of learner summaries in Basque. Therefore, in addition to language comprehension, difficulties emerge from Basque morphology itself. In this work, Latent Semantic Analysis (LSA) is used to model comprehension in a language in which lemmatization has shown to be highly significant. This paper tests the influence of corpus lemmatization while performing automatic comprehension and coherence grading. Summaries graded by human judges in coherence and comprehension, have been tested against LSA based measures from source lemmatized and non-lemmatized corpora. After lemmatization, the amount of LSA known single terms was reduced in a 56\% of its original number. As a result, LSA grades almost match human measures, producing no significant differences between the lemmatized and non-lemmatized approaches.},
	language = {en},
	booktitle = {Intelligent {Tutoring} {Systems}},
	publisher = {Springer},
	author = {Zipitria, Iraide and Arruarte, Ana and Elorriaga, Jon Ander},
	editor = {Ikeda, Mitsuru and Ashley, Kevin D. and Chan, Tak-Wai},
	year = {2006},
	keywords = {Automatic Evaluation, Basque Country, Intelligent Tutor System, Latent Semantic Analysis, Text Comprehension},
	pages = {595--603},
	file = {Springer Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/MCMBQEEQ/Zipitria et al. - 2006 - Observing Lemmatization Effect in LSA Coherence an.pdf:application/pdf},
}

@article{lemaire_limites_2008,
	title = {Limites de la lemmatisation pour l'extraction de significations},
	abstract = {Corpus lemmatization is an usual procedure which is sometimes realized just by following a tradition. This paper highlights the limits of this process in the case of automatic extraction of semantic information, that is when the context in which words occur is used. First, we uncovered significant differences between contexts of singular and plural forms of 58 nouns in a huge French corpus. Systematically replacing plural forms by singular forms might therefore affect the performances of semantic extraction systems. Then, we relied on Latent Semantic Analysis to show in another way that the two contexts are different and that LSA performances on a vocabulary test decrease when the corpus is lemmatized. Lemmatizing corpora for such an usage might therefore work against the general intention.},
	language = {fr},
	author = {Lemaire, Benoît},
	year = {2008},
	pages = {9},
	file = {Lemaire - 2008 - Limites de la lemmatisation pour l'extraction de s.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9ZAUGE43/Lemaire - 2008 - Limites de la lemmatisation pour l'extraction de s.pdf:application/pdf},
}


@article{camps_stylometry_2020,
	title = {Stylometry for {Noisy} {Medieval} {Data}: {Evaluating} {Paul} {Meyer}'s {Hagiographic} {Hypothesis}},
	shorttitle = {Stylometry for {Noisy} {Medieval} {Data}},
	journal = {arXiv preprint arXiv:2012.03845},
	author = {Camps, Jean-Baptiste and Clérice, Thibault and Pinche, Ariane},
	year = {2020},
}


@article{zhang_text_2016,
	title = {Text {Understanding} from {Scratch}},
	url = {http://arxiv.org/abs/1502.01710},
	abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classiﬁcation. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
	language = {en},
	urldate = {2021-09-03},
	journal = {arXiv:1502.01710 [cs]},
	author = {Zhang, Xiang and LeCun, Yann},
	month = apr,
	year = {2016},
	note = {arXiv: 1502.01710},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: This technical report is superseded by a paper entitled "Character-level Convolutional Networks for Text Classification", arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction},
	file = {Zhang and LeCun - 2016 - Text Understanding from Scratch.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/CND5MYZL/Zhang and LeCun - 2016 - Text Understanding from Scratch.pdf:application/pdf},
}


@inproceedings{bai_adversarial_2020,
	title = {Adversarial {Named} {Entity} {Recognition} with {POS} label embedding},
	doi = {10.1109/IJCNN48605.2020.9207682},
	abstract = {Named Entity Recognition (NER) is dedicated to recognizing different types of named entity. Previous works have shown that part-of-speech, as an important feature, provides complementary syntactical information to NER systems. However, these studies suffer from two limitations: (i) the previous models do not consider the noise from part-of-speech; (ii) the previous models need to re-extract features from token representations. In this paper, we propose a novel approach that can alleviate the above issues as well as make full use of part-of-speech features via attention mechanism and adversarial training. We evaluate our model on three NER datasets, and the experimental results demonstrate that our model achieves a state-of-the-art F1-score of Twitter dataset while matching a state-of-the-art performance on the CoNLL-2003 and Weibo datasets.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Bai, Yuxuan and Wang, Yu and Xia, Bin and Li, Yun and Zhu, Ziye},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Adversarial training, Artificial neural networks, Attention mechanism, Decoding, Feature extraction, Named Entity Recognition, Organizations, Task analysis, Telecommunications, Training},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/QQSCDESY/9207682.html:text/html},
}


@inproceedings{naseem_towards_2020,
	address = {Glasgow, United Kingdom},
	title = {Towards {Improved} {Deep} {Contextual} {Embedding} for the identification of {Irony} and {Sarcasm}},
	isbn = {978-1-72816-926-2},
	url = {https://ieeexplore.ieee.org/document/9207237/},
	doi = {10.1109/IJCNN48605.2020.9207237},
	abstract = {Humans use tonal stress and gestural cues to reveal negative feelings that are expressed ironically using positive or intensiﬁed positive words when communicating vocally. However, in textual data, like posts on social media, cues on sentiment valence are absent, thus making it challenging to identify the true meaning of utterances, even for the human reader. For a given post, an intelligent natural language processing system should be able to identify whether a post is ironic/sarcastic or not. Recent work conﬁrms the difﬁculty of detecting sarcastic/ironic posts. To overcome challenges involved in the identiﬁcation of sentiment valence, this paper presents the identiﬁcation of irony and sarcasm in social media posts through transformer-based deep, intelligent contextual embedding – T-DICE – which improves noise within contexts. It solves the language ambiguities such as polysemy, semantics, syntax, and words sentiments by integrating embeddings. T-DICE is then forwarded to attention-based Bidirectional Long Short Term Memory (BiLSTM) to ﬁnd out the sentiment of a post. We report the classiﬁcation performance of the proposed network on benchmark datasets for \#irony \& \#sarcasm. Results demonstrate that our approach outperforms existing state-of-the-art methods.},
	language = {en},
	urldate = {2021-09-03},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Naseem, Usman and Razzak, Imran and Eklund, Peter and Musial, Katarzyna},
	month = jul,
	year = {2020},
	pages = {1--7},
	file = {Naseem et al. - 2020 - Towards Improved Deep Contextual Embedding for the.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/JNR6GGG4/Naseem et al. - 2020 - Towards Improved Deep Contextual Embedding for the.pdf:application/pdf},
}



@inproceedings{cotterell_morphological_2015,
	address = {Denver, Colorado},
	title = {Morphological {Word}-{Embeddings}},
	url = {http://aclweb.org/anthology/N15-1140},
	doi = {10.3115/v1/N15-1140},
	abstract = {Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Cotterell, Ryan and Schütze, Hinrich},
	year = {2015},
	pages = {1287--1292},
	file = {Cotterell and Schütze - 2015 - Morphological Word-Embeddings.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/VULGIXPZ/Cotterell and Schütze - 2015 - Morphological Word-Embeddings.pdf:application/pdf},
}

@article{cui_knet_2015,
	title = {{KNET}: {A} {General} {Framework} for {Learning} {Word} {Embedding} {Using} {Morphological} {Knowledge}},
	volume = {34},
	issn = {1046-8188},
	shorttitle = {{KNET}},
	url = {https://doi.org/10.1145/2797137},
	doi = {10.1145/2797137},
	abstract = {Neural network techniques are widely applied to obtain high-quality distributed representations of words (i.e., word embeddings) to address text mining, information retrieval, and natural language processing tasks. Most recent efforts have proposed several efficient methods to learn word embeddings from context such that they can encode both semantic and syntactic relationships between words. However, it is quite challenging to handle unseen or rare words with insufficient context. Inspired by the study on the word recognition process in cognitive psychology, in this article, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both words’ contextual information and morphological knowledge to learn word embeddings. Meanwhile, this new learning architecture is also able to benefit from noisy knowledge and balance between contextual information and morphological knowledge. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.},
	number = {1},
	urldate = {2021-09-08},
	journal = {ACM Transactions on Information Systems},
	author = {Cui, Qing and Gao, Bin and Bian, Jiang and Qiu, Siyu and Dai, Hanjun and Liu, Tie-Yan},
	month = aug,
	year = {2015},
	keywords = {morphological knowledge, Neural network, word embedding},
	pages = {4:1--4:25},
}


@article{salama_morphological_2018,
	title = {Morphological {Word} {Embedding} for {Arabic}},
	volume = {142},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918321653},
	doi = {10.1016/j.procs.2018.10.463},
	language = {en},
	urldate = {2021-09-08},
	journal = {Procedia Computer Science},
	author = {Salama, Rana Aref and Youssef, Abdou and Fahmy, Aly},
	year = {2018},
	pages = {83--93},
	file = {Salama et al. - 2018 - Morphological Word Embedding for Arabic.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/CISSDYSH/Salama et al. - 2018 - Morphological Word Embedding for Arabic.pdf:application/pdf},
}


@inproceedings{qian_investigating_2016,
	address = {Berlin, Germany},
	title = {Investigating {Language} {Universal} and {Specific} {Properties} in {Word} {Embeddings}},
	url = {http://aclweb.org/anthology/P16-1140},
	doi = {10.18653/v1/P16-1140},
	abstract = {Recently, many NLP tasks have beneﬁted from distributed word representation. However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent architecture. Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space. Experiment results reveal the language universal and speciﬁc properties encoded in various word representation. Additionally, strong evidence supports the utility of word form, especially for inﬂectional languages.},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Peng and Qiu, Xipeng and Huang, Xuanjing},
	year = {2016},
	pages = {1478--1488},
	file = {Qian et al. - 2016 - Investigating Language Universal and Specific Prop.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/L22RZFKV/Qian et al. - 2016 - Investigating Language Universal and Specific Prop.pdf:application/pdf},
}



@article{rastier_tropes_1994,
	title = {Tropes et sémantique linguistique},
	volume = {101},
	copyright = {free},
	url = {https://www.persee.fr/doc/lfr_0023-8368_1994_num_101_1_5845},
	doi = {10.3406/lfr.1994.5845},
	abstract = {François Rastier, Tropes and Semantic Linguistics The tradition of linguistics has reduced rhetoric to tropes. Until now, it has defined figurative sense as a deviation from proper or literal sense, thus highlighting ontological concerne typical of western metaphysics. For descriptive semantics, the trope problem becomes the problem of interpretative paths, ruled by perceptive and hermeneutical constraints. These paths allow the building of semantic shapes and backgrounds.},
	language = {fre},
	number = {1},
	urldate = {2021-09-07},
	journal = {Langue française},
	author = {Rastier, François},
	year = {1994},
	note = {Publisher: Persée - Portail des revues scientifiques en SHS},
	pages = {80--101},
	annote = {
p. 80-81 La division de la signification lexicale, nécessaire pour distinguer le figuré de ce qui ne l'est pas, peut suivre trois voies, (i) On oppose des mots : le mot propre au mot figuré ; (ii) des emploie d'un même mot : le sens propre ou le sens littéral au sens figuré ; (iii) des parties de la signification du mot : les idées principales aux idées accessoires, la dénotation à la connotation.
p. 82, Pour réfuter l'existence d'un "sens propre".

Dumareais "Le sens propre d'un mot, c'est la première signification du mot", (p. 81)
Opposition de Rastier: "Pour les sémanticiens « continentaux » dont je suis, le sens premier, qu'il soit étymologique (historique) ou originel (étiologique), n'entre pas dans la définition d'un sémème : un sémème se définit au sein d'un réseau d'oppositions en synchronie."


p. 85, sur le contexte

"Tout texte en effet relève d'un genre, et par là d'un discours (juridique, pédagogique, etc.) qui reflète par ses normes l'incidence de la pratique sociale où il prend place. "


p. 92-93, sur le mot en contexte:

" Si bien que le mot n'a pas à proprement parler de signification, mais une ou plusieurs acceptions. La signification, comme naguère le sens propre, serait un artefact du linguiste qui, sous couleur d'organiser les articles de dictionnaire, lui permet de sauver indirectement l'ontologie,"
"Convenons provisoirement que les acceptions sont des sémies-types et les sens des occurrences. Introduisons la distinction entre sémie-type et sémie-occurrence 28, et formulons provisoirement une définition conservatoire qui maintienne la notion de trope " {\textbackslash}ilya trope quand une sémie-occurrence, au lieu d'hériter par défaut tous ses traits sémantiques de la sémie-type, actualise par prescriptions contextuelles au moins un sème afferent (en cas de propagation de traits) et/ ou subit une deletion d'au moins un sème inhérent (en cas d'inhibition). "


p. 96

"Ces acceptions partagent ou non un ou plusieurs sèmes communs, qui définissent les relations d'homonymie ou de polysémie."


p. 98

"Quant à la métaphore in absentia, elle instaure une connexion symbolique qui doit être identifiée par des conjectures concordantes sur le discours, le type de l'œuvre, le genre du texte, la hiérarchisation idiolectale des isotopies."


},
	file = {Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/925CDX5S/lfr_0023-8368_1994_num_101_1_5845.html:text/html},
}


@article{kim_categorical_2019,
	title = {Categorical metadata representation for customized text classification},
	volume = {7},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kim, Jihyeok and Amplayo, Reinald Kim and Lee, Kyungjae and Sung, Sua and Seo, Minji and Hwang, Seung-won},
	year = {2019},
	note = {Publisher: MIT Press},
	pages = {201--215},
	file = {Full Text:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/YVNW3RIF/43497.html:text/html;Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/NFBZI6QG/Categorical-Metadata-Representation-for-Customized.html:text/html},
}

@article{martin_controllable_2020,
	title = {Controllable {Sentence} {Simplification}},
	url = {http://arxiv.org/abs/1910.02677},
	abstract = {Text simplification aims at making a text easier to read and understand by simplifying grammar and structure while keeping the underlying information identical. It is often considered an all-purpose generic task where the same simplification is suitable for all; however multiple audiences can benefit from simplified text in different ways. We adapt a discrete parametrization mechanism that provides explicit control on simplification systems based on Sequence-to-Sequence models. As a result, users can condition the simplifications returned by a model on attributes such as length, amount of paraphrasing, lexical complexity and syntactic complexity. We also show that carefully chosen values of these attributes allow out-of-the-box Sequence-to-Sequence models to outperform their standard counterparts on simplification benchmarks. Our model, which we call ACCESS (as shorthand for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score.},
	urldate = {2021-07-26},
	journal = {arXiv:1910.02677 [cs]},
	author = {Martin, Louis and Sagot, Benoît and de la Clergerie, Éric and Bordes, Antoine},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.02677},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Code and models: https://github.com/facebookresearch/access},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/MEQSF6L3/Martin et al. - 2020 - Controllable Sentence Simplification.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/HR5VFEQA/1910.html:text/html},
}



@article{gong_enriching_2020,
	title = {Enriching {Word} {Embeddings} with {Temporal} and {Spatial} {Information}},
	url = {http://arxiv.org/abs/2010.00761},
	abstract = {The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more reﬁned semantics for use in time-speciﬁc or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and locationstamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-speciﬁc embedding, and serves as a new benchmark for location-speciﬁc embeddings.},
	language = {en},
	urldate = {2021-09-09},
	journal = {arXiv:2010.00761 [cs]},
	author = {Gong, Hongyu and Bhat, Suma and Viswanath, Pramod},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.00761},
	keywords = {Computer Science - Computation and Language},
	file = {Gong et al. - 2020 - Enriching Word Embeddings with Temporal and Spatia.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/9TC8BHN6/Gong et al. - 2020 - Enriching Word Embeddings with Temporal and Spatia.pdf:application/pdf},
}


@article{carlo_training_2019,
	title = {Training {Temporal} {Word} {Embeddings} with a {Compass}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4594},
	doi = {10.1609/aaai.v33i01.33016326},
	abstract = {Temporal word embeddings have been proposed to support the analysis of word meaning shifts during time and to study the evolution of languages. Different approaches have been proposed to generate vector representations of words that embed their meaning during a specific time interval. However, the training process used in these approaches is complex, may be inefficient or it may require large text corpora. As a consequence, these approaches may be difficult to apply in resource-scarce domains or by scientists with limited in-depth knowledge of embedding models. In this paper, we propose a new heuristic to train temporal word embeddings based on the Word2vec model. The heuristic consists in using atemporal vectors as a reference, i.e., as a compass, when training the representations specific to a given time interval. The use of the compass simplifies the training process and makes it more efficient. Experiments conducted using state-of-the-art datasets and methodologies suggest that our approach outperforms or equals comparable approaches while being more robust in terms of the required corpus size.},
	language = {en},
	number = {01},
	urldate = {2021-09-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Carlo, Valerio Di and Bianchi, Federico and Palmonari, Matteo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {6326--6334},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/3234DINB/Carlo et al. - 2019 - Training Temporal Word Embeddings with a Compass.pdf:application/pdf},
}


@article{roy_incorporating_nodate,
	title = {Incorporating {Extra} {Knowledge} to {Enhance} {Word} {Embedding}},
	abstract = {Word embedding, a process to automatically learn the mathematical representations of words from unlabeled text corpora, has gained a lot of attention recently. Since words are the basic units of a natural language, the more precisely we can represent the morphological, syntactic and semantic properties of words, the better we can support downstream Natural Language Processing (NLP) tasks. Since traditional word embeddings are mainly designed to capture the semantic relatedness between co-occurred words in a predeﬁned context, it may not be effective in encoding other information that is important for different NLP applications. In this survey, we summarize the recent advances in incorporating extra knowledge to enhance word embedding. We will also identify the limitations of existing work as well as point out a few promising future directions.},
	language = {en},
	author = {Roy, Arpita and Pan, Shimei},
	pages = {7},
	file = {Roy and Pan - Incorporating Extra Knowledge to Enhance Word Embe.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/4Z8MM24C/Roy and Pan - Incorporating Extra Knowledge to Enhance Word Embe.pdf:application/pdf},
}

@article{zhang_ernie_2019,
	title = {{ERNIE}: {Enhanced} {Language} {Representation} with {Informative} {Entities}},
	shorttitle = {{ERNIE}},
	url = {http://arxiv.org/abs/1905.07129},
	abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
	urldate = {2021-09-09},
	journal = {arXiv:1905.07129 [cs]},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.07129},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2019},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/DZPLFWV9/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Infor.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/2YUGRF4Q/1905.html:text/html},
}

@inproceedings{wang_knowledge_2014,
	address = {Doha, Qatar},
	title = {Knowledge {Graph} and {Text} {Jointly} {Embedding}},
	url = {http://aclweb.org/anthology/D14-1167},
	doi = {10.3115/v1/D14-1167},
	abstract = {We examine the embedding approach to reason new relational facts from a largescale knowledge graph and a text corpus. We propose a novel method of jointly embedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
	year = {2014},
	pages = {1591--1601},
	file = {Wang et al. - 2014 - Knowledge Graph and Text Jointly Embedding.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/H7R28BLA/Wang et al. - 2014 - Knowledge Graph and Text Jointly Embedding.pdf:application/pdf},
}


@inproceedings{huang_neural_2019,
	address = {Florence, Italy},
	title = {Neural {Temporality} {Adaptation} for {Document} {Classification}: {Diachronic} {Word} {Embeddings} and {Domain} {Adaptation} {Models}},
	shorttitle = {Neural {Temporality} {Adaptation} for {Document} {Classification}},
	url = {https://www.aclweb.org/anthology/P19-1403},
	doi = {10.18653/v1/P19-1403},
	abstract = {Language usage can change across periods of time, but document classiﬁers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classiﬁers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classiﬁcation, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classiﬁcation model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classiﬁers more robust over time.},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Xiaolei and Paul, Michael J.},
	year = {2019},
	pages = {4113--4123},
	file = {Huang and Paul - 2019 - Neural Temporality Adaptation for Document Classif.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/GN3PJIIK/Huang and Paul - 2019 - Neural Temporality Adaptation for Document Classif.pdf:application/pdf},
}


@article{liu_semantic_2018,
	title = {Semantic {Structure}-{Based} {Word} {Embedding} by {Incorporating} {Concept} {Convergence} and {Word} {Divergence}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11942},
	abstract = {Representing the semantics of words is a fundamental task in text processing. Several research studies have shown that text and knowledge bases (KBs) are complementary sources for word embedding learning. Most existing methods only consider relationships within word-pairs in the usage of KBs. We argue that the structural information of well-organized words within the KBs is able to convey more effective and stable knowledge in capturing semantics of words. In this paper, we propose a semantic structure-based word embedding method, and introduce concept convergence and word divergence to reveal semantic structures in the word embedding learning process. To assess the effectiveness of our method, we use WordNet for training and conduct extensive experiments on word similarity, word analogy, text classification and query expansion. The experimental results show that our method outperforms state-of-the-art methods, including the methods trained solely on the corpus, and others trained on the corpus and the KBs.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Qian and Huang, Heyan and Zhang, Guangquan and Gao, Yang and Xuan, Junyu and Lu, Jie},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {nature language processing},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/JSMIEYN6/Liu et al. - 2018 - Semantic Structure-Based Word Embedding by Incorpo.pdf:application/pdf;Liu et al. - Semantic Structure-Based Word Embedding by Incorpo.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/NKYA4A4K/Liu et al. - Semantic Structure-Based Word Embedding by Incorpo.pdf:application/pdf},
}


@article{chan_german_2019,
	title = {German bert},
	journal = {URL: https://deepset. ai/german-bert},
	author = {Chan, Branden and Möller, Timo and Pietsch, Malte and Soni, Tanay and Yeung, Chin Man},
	year = {2019},
}


@inproceedings{xuan_improved_2020,
	address = {Snowmass Village, CO, USA},
	title = {Improved {Embeddings} with {Easy} {Positive} {Triplet} {Mining}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093432/},
	doi = {10.1109/WACV45572.2020.9093432},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Xuan, Hong and Stylianou, Abby and Pless, Robert},
	month = mar,
	year = {2020},
	pages = {2463--2471},
	file = {Xuan et al. - 2020 - Improved Embeddings with Easy Positive Triplet Min.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/EKJDXEW5/Xuan et al. - 2020 - Improved Embeddings with Easy Positive Triplet Min.pdf:application/pdf},
}

@article{hermans_defense_2017,
	title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1703.07737},
	abstract = {In the past few years, the ﬁeld of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person reidentiﬁcation subﬁeld is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classiﬁcation, veriﬁcation) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:1703.07737 [cs]},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = nov,
	year = {2017},
	note = {arXiv: 1703.07737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Lucas Beyer and Alexander Hermans contributed equally. Updates: Minor fixes, new SOTA comparisons, add CUHK03 results},
	file = {Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/95KL55VK/Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:application/pdf},
}

@misc{musgrave2020pytorch,
    title={PyTorch Metric Learning},
    author={Kevin Musgrave and Serge Belongie and Ser-Nam Lim},
    year={2020},
    eprint={2008.09164},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{rastier_isotopie_1985,
	title = {L'isotopie sémantique, du mot au texte},
	volume = {27},
	copyright = {free},
	url = {https://www.persee.fr/doc/igram_0222-9838_1985_num_27_1_2168},
	doi = {10.3406/igram.1985.2168},
	language = {fre},
	number = {1},
	urldate = {2021-09-07},
	journal = {L'information grammaticale},
	author = {Rastier, François},
	year = {1985},
	note = {Publisher: Persée - Portail des revues scientifiques en SHS},
	pages = {33--36},
	annote = {
p. 33 "Une isotopie est définie par ta récurrence d'un même trait sémantique"
p. 33 "Un texte ne contient certes pas tout ce que requiert son interprétation, et notamment la construction ou l'identification de ses isotopies."
p.33-34 "En effet, il serait absurde de décrire à part les composants de l 'isotopie dont l'actualisation serait strictement conditionnée par l'environnement pragmatique. "
p. 34 "L'intérêt principal de ce concept tient à ce qu'il est indépendant par principe des structures syntaxiques et de la prétendue limite de la phrase. Une isotopie peut s'étendre sur deux mots, sur un paragraphe, sur tout un texte."
p. 34 "Ce n'est pas ta récurrence de sèmes déjà donnés qui constitue l'isotopie, mais, rétroactivement, la présomption d'isotopie qui permet d'actualiser des sèmes, voire les sèmes"
p. 35 "Et surtout, on ne peut séparer la description de la connotation de celle de la dénotation puisque la plupart des isotopies tant soit peu étendues mêlent inextricablement des composants qui relèveraient de la dénotation et de la connotation. "
p. 35 " une isotopie peut être constituée de contenus relevant de différents types sémiotiques de signes linguistiques : prosodie, ponctuation, etc. . . "
p. 35 "Enfin, le contexte non linguistique a une incidence sur le texte, jusque dans l'identification des constituants des sémèmes. Or ce contexte n'est pas fait d'objets du monde qui donneraient à lire un sens, mais de conventions sociales. Disons, pour simplifier, que ce sont les galons de celui qui prononce Il fait froid qui permettent aux subalternes d'inférer il faut fermer la fenêtre. 

 },
}


@article{sprugnoli_towards_2016,
	title = {Towards sentiment analysis for historical texts},
	volume = {31},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqv027},
	doi = {10.1093/llc/fqv027},
	abstract = {This article presents the integration of sentiment analysis in ALCIDE, an online platform for historical content analysis. A prior polarity approach has been applied to a corpus of Italian historical texts, and a new lexical resource has been developed with a semi-automatic mapping starting from two English lexica. This article also reports on a first experiment on contextual polarity using both expert annotators and crowdsourced contributors. The long-term goal of our research is to create a system to support historical studies, which is able to analyse the sentiment in historical texts and to discover the opinion about a topic and its change over time.},
	number = {4},
	urldate = {2021-09-17},
	journal = {Digital Scholarship in the Humanities},
	author = {Sprugnoli, Rachele and Tonelli, Sara and Marchetti, Alessandro and Moretti, Giovanni},
	month = dec,
	year = {2016},
	pages = {762--772},
}


@article{shang_speaker-change_2020,
	title = {Speaker-change {Aware} {CRF} for {Dialogue} {Act} {Classification}},
	url = {http://arxiv.org/abs/2004.02913},
	abstract = {Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.},
	urldate = {2021-09-17},
	journal = {arXiv:2004.02913 [cs]},
	author = {Shang, Guokan and Tixier, Antoine Jean-Pierre and Vazirgiannis, Michalis and Lorré, Jean-Pierre},
	month = may,
	year = {2020},
	note = {arXiv: 2004.02913},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/DMUBBC2U/Shang et al. - 2020 - Speaker-change Aware CRF for Dialogue Act Classifi.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/N4VIL7BQ/2004.html:text/html},
}

@article{alkhwiter_part--speech_2021,
	title = {Part-of-speech tagging for {Arabic} tweets using {CRF} and {Bi}-{LSTM}},
	volume = {65},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230820300711},
	doi = {10.1016/j.csl.2020.101138},
	abstract = {Over the past few years, Twitter has experienced massive growth and the volume of its online content has increased rapidly. This content has been a rich source for several studies that focused on natural language processing (NLP) research. However, Twitter data pose numerous challenges and obstacles to NLP tasks. For the English language, Twitter has an NLP tool that provides tweet-specific NLP tasks, which present significant opportunities for English NLP research and applications. Part-of-speech (POS) tagging for English tweets is one of the tasks that is offered and facilitated by such a tool. In contrast, only a few attempts have been made to develop POS taggers for Arabic content on Twitter. In this paper, we consider POS tagging, which is one of the NLP tasks that directly affects the performance of other subsequent text processing tasks. We introduce three manually annotated datasets for the POS tagging of Arabic tweets: the ‘Mixed,’ ‘MSA,’ and ‘GLF’ datasets with 3000, 1000, and 1000 Arabic tweets, respectively. In addition, we present an exploratory analysis of the behavior of using hashtags in Arabic tweets, which is a phenomenon that affects the task of POS tagging. We also present two supervised POS taggers that are developed based on two approaches: Conditional Random Fields and Bidirectional Long Short-Term Memory (Bi-LSTM) models. We conclude that the Bi-LSTM-based POS tagger achieves the state-of-the-art results for the ‘Mixed’ dataset with 96.5\% accuracy. However, the specific-dialect taggers trained on the ‘MSA’ and ‘GLF’ datasets achieve an accuracy of 95.6\% and 95\%, respectively. The results for the ‘Mixed’ dataset indicate the effectiveness of developing a joint POS tagger without the need for a dialect-specific POS tagger.},
	language = {en},
	urldate = {2021-09-17},
	journal = {Computer Speech \& Language},
	author = {AlKhwiter, Wasan and Al-Twairesh, Nora},
	month = jan,
	year = {2021},
	keywords = {Arabic Tweets, Bidirectional Long Short-Term Memory, Conditional random fields, Part-of-speech (POS) tagging},
	pages = {101138},
	file = {ScienceDirect Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/RDKFNMBD/S0885230820300711.html:text/html},
}


@inproceedings{pasupa_comparison_2016,
	title = {A comparison between shallow and deep architecture classifiers on small dataset},
	doi = {10.1109/ICITEED.2016.7863293},
	abstract = {Many machine learning algorithms have been introduced to solve different types of problem. Recently, many of these algorithms have been applied to deep architecture model and showed very impressive performance. In general, deep architecture model suffers from over-fitting problem when there is a small number of training data. In this paper, we attempted to remedy this problem in deep architecture with regularization techniques including overlap pooling, flipped-image augmentation and dropout, and we also compared a deep structure model (convolutional neural network (CNN)) with shallow structure models (support vector machine and artificial neural network with one hidden layer) on a small dataset. It was statistically confirmed that the shallow models achieved better performance than the deep model that did not use a regularization technique. However, a deep model augmented with a regularization technique-CNN with dropout technique-was competitive to the shallow models.},
	booktitle = {2016 8th {International} {Conference} on {Information} {Technology} and {Electrical} {Engineering} ({ICITEE})},
	author = {Pasupa, Kitsuchart and Sunhem, Wisuwat},
	month = oct,
	year = {2016},
	keywords = {Data models, deep learning, Face, Feature extraction, machine learning, Machine learning, Machine learning algorithms, shallow learning, small dataset, Support vector machines, Training},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/JRHDQHPK/7863293.html:text/html},
}


@article{hartmann_portuguese_nodate,
	title = {Portuguese {Word} {Embeddings}: {Evaluating} on {Word} {Analogies} and {Natural} {Language} {Tasks}},
	abstract = {Word embeddings have been found to provide meaningful representations for words in an efﬁcient way; therefore, they have become common in Natural Language Processing systems. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for word embedding evaluation instead task-speciﬁc evaluations may be a better option; Wang2Vec appears to be a robust model; the increase in performance in our evaluations with bigger models is not worth the increase in memory usage for models with more than 300 dimensions.},
	language = {en},
	author = {Hartmann, Nathan S and Fonseca, Erick R and Shulby, Christopher D and Treviso, Marcos V and Rodrigues, Jessica S and Aluısio, Sandra M},
	pages = {10},
	file = {Hartmann et al. - Portuguese Word Embeddings Evaluating on Word Ana.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/P49ADF9D/Hartmann et al. - Portuguese Word Embeddings Evaluating on Word Ana.pdf:application/pdf},
}


@article{gensim,
	title = {Gensim—{Statistical} {Semantics} in {Python}},
	abstract = {Gensim is a pure Python library that fights on two fronts: 1) digital document indexing and similarity search; and 2) fast, memory-efficient, scalable algorithms for Singular Value Decomposition and Latent Dirichlet Allocation. The connection between the two is unsupervised, semantic analysis of plain text in digital collections. Gensim was created for large digital libraries, but its underlying algorithms for large-scale, distributed, online SVD and LDA are like the Swiss Army knife of data analysis—also useful on their own, outside of the domain of Natural Language Processing.},
	language = {en},
	author = {Řehůřek, Radim and Sojka, Petr},
	pages = {1},
	file = {Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/ZCVT25RW/Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:application/pdf},
}

@article{fasttext,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	volume = {5},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00051},
	doi = {10.1162/tacl_a_00051},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful
for many natural language processing tasks. Popular models that learn such
representations ignore the morphology of words, by assigning a distinct vector
to each word. This is a limitation, especially for languages with large
vocabularies and many rare words. In this paper, we propose a new approach based
on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated
to each character n-gram; words being represented
as the sum of these representations. Our method is fast, allowing to train
models on large corpora quickly and allows us to compute word representations
for words that did not appear in the training data. We evaluate our word
representations on nine different languages, both on word similarity and analogy
tasks. By comparing to recently proposed morphological word representations, we
show that our vectors achieve state-of-the-art performance on these tasks.},
	urldate = {2021-09-19},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jun,
	year = {2017},
	pages = {135--146},
	file = {Full Text PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/HJU4FNEM/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf},
}

@article{word2vec,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2021-09-19},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/7KQLD9CL/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/W9UDND3E/1301.html:text/html},
}


@inproceedings{yang_hierarchical_2016,
	address = {San Diego, California},
	title = {Hierarchical {Attention} {Networks} for {Document} {Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	doi = {10.18653/v1/N16-1174},
	abstract = {We propose a hierarchical attention network for document classiﬁcation. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classiﬁcation tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	year = {2016},
	pages = {1480--1489},
	file = {Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/DL549FK9/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:application/pdf},
}


@inproceedings{gao_hierarchical_2018,
	address = {Melbourne, Australia},
	title = {Hierarchical {Convolutional} {Attention} {Networks} for {Text} {Classification}},
	url = {http://aclweb.org/anthology/W18-3002},
	doi = {10.18653/v1/W18-3002},
	abstract = {Recent work in machine translation has demonstrated that self-attention mechanisms can be used in place of recurrent neural networks to increase training speed without sacriﬁcing model accuracy. We propose combining this approach with the beneﬁts of convolutional ﬁlters and a hierarchical structure to create a document classiﬁcation model that is both highly accurate and fast to train – we name our method Hierarchical Convolutional Attention Networks. We demonstrate the effectiveness of this architecture by surpassing the accuracy of the current state-of-the-art on several classiﬁcation tasks while being twice as fast to train.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of {The} {Third} {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Shang and Ramanathan, Arvind and Tourassi, Georgia},
	year = {2018},
	pages = {11--23},
	file = {Gao et al. - 2018 - Hierarchical Convolutional Attention Networks for .pdf:/home/thibault/.mozilla/firefox/dqyoiprc.default/zotero/storage/FD39IV9S/Gao et al. - 2018 - Hierarchical Convolutional Attention Networks for .pdf:application/pdf},
}


@article{magueresse_low-resource_2020,
	title = {Low-resource {Languages}: {A} {Review} of {Past} {Work} and {Future} {Challenges}},
	shorttitle = {Low-resource {Languages}},
	url = {http://arxiv.org/abs/2006.07264},
	abstract = {A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.},
	urldate = {2021-09-21},
	journal = {arXiv:2006.07264 [cs]},
	author = {Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07264},
	keywords = {Computer Science - Computation and Language, I.2.7},
}


@article{hermans_defense_2017,
	title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1703.07737},
	abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
	urldate = {2021-10-26},
	journal = {arXiv:1703.07737 [cs]},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = nov,
	year = {2017},
	note = {arXiv: 1703.07737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Lucas Beyer and Alexander Hermans contributed equally. Updates: Minor fixes, new SOTA comparisons, add CUHK03 results},
}

@article{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2021-10-26},
	journal = {arXiv:2004.11362 [cs, stat]},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv: 2004.11362},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bamman2020latin,
      title={Latin BERT: A Contextual Language Model for Classical Philology}, 
      author={David Bamman and Patrick J. Burns},
      year={2020},
      eprint={2009.10053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
