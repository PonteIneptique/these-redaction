\chapter{Constitution du corpus}

\section{Une histoire des corpus latins numériques}

Le travail sur la langue latine nécessite \textit{de facto} des corpus, et \textit{a priori} en nécessite des numériques s'il s'agit d'une approche computationnelle. Si la tradition papier des corpus académiques des Teubner ou des Belles Lettres s'inscrira bientôt dans leur troisième centenaire\footnote{Si l'éditeur Teubner semble s'attaquer dès les années 1810 à l'impression d'ouvrages philologiques, la \textit{Bibliotheca Scriptorum Graecorum et Latinorum Teubneriana} ne voit le jour \enquote{qu'en} 1849. Elle prédate les deux autres collections généralistes majeurs, la collection \textit{Oxford Classical Texts} et les \textit{Belles Lettres}. \cite{andre_cent-cinquante_1974}}, l'histoire des corpus littéraires numériques n'a fêté que très récemment son cinquantenaire avec les prémices du \textit{Thesaurus Linguae Graecae}.

Aussi, nous proposons de revenir sur les cinquante dernières années de numérisation et de mise à disposition des textes latins, principalement des textes littéraires. Nous proposons un découpage en trois périodes de cette révolution numérique des corpus: la première concerne l'apparition des disquettes et CD de corpus qui émaille les décennies 1960 à 1990; la seconde (1995-2005) concerne l'apparition en ligne de ces premiers corpus, mais aussi une autre forme de révolution, celle des corpus non académiques; la troisième (2005-aujourd'hui) concerne enfin l'expansion du numérique comme version fondamentale des corpus et l'apparition de \enquote{méga corpus}.

Il faudra cependant commencer cette introduction au chapitre par un avertissement: la documentation disponible sur la publication des corpus numériques est presque inexistante, souvent de seconde main, à travers de rares témoignages ou d'encore plus rares citations et ne permet souvent pas de retrouver avec toute l'exactitude souhaitée la première date de publication de tel ou tel ouvrage. Jusqu'à aujourd'hui, la citation des corpus numériques n'est pas entrée dans les usages, tout comme la citation des œuvres quand on en fait le commentaire: rares sont les chercheurs qui spécifient en bibliographie l'édition précise qu'ils ont utilisée quand ils mentionnent Virgile ou Martial. Aussi, nous nous excusons d'avance  si des informations présentées ici sont inexactes, si des corpus oubliés le sont, et nous invitons grandement notre champ à capturer rapidement cette histoire, l'archiver avant qu'il ne soit trop tard: si les décades 70 et 80 ne sont pas très loin, elles semblent bien floues sur le plan de l'histoire des corpus\footnote{Il nous semble propice, pour qui voudra, de s'intéresser à une histoire orale des projets fondateurs, à une recherche en archives pour chacun de ces projets, avant qu'il ne soit trop tard.}.

\subsection{Les \enquote{incunables} du numérique}

Nous nous intéressons ici à la naissance des corpus numériques littéraires, ayant pour vocation d'être lus ou utilisés pour des recherches dès leur conception numérique. À cette fin, nous excluons les travaux d'annotation linguistique et de construction de concordanciers de R.~Busa ou du LASLA  car ils avaient des ambitions plus spécifiques et ne proposaient pas comme but premier de pouvoir lire le texte\footnote{Mais nous en parlerons plus tard, \textit{cf.} \ref{lemmatisation:concordanciers}}. Or, il est difficile comme nous le disions plus haut de savoir à quand remontent les premières productions de corpus.

% La documentation d’époque et de première main est très pauvre sur ces outils (date, recherche en cours), on les retrouve principalement dans des reviews

\subsubsection{Années 60, années 80: premiers corpus, premiers CD-ROMs}

Le corpus littéraire le plus ancien dont il est fait mention est celui financé par le \textit{National Endowment for the Humanities} (NEH) et cité par Theodore F.~Brunner dans son article rétrospectif de 1993 centré sur la recherche états-unienne \textit{Classics and the Computer}\footcite{brunner_classics_1993}. En 1968, Nathan Greenberg et John. J.~Bateman obtiennent un financement de la NEH de 19.800\$ \footcite{noauthor_neh_2018} complété par 40.000\$ de financeurs secondaires, dont IBM\footnote{D'après \cite{brunner_classics_1993}: le \textit{Digital Computer Laboratory} de l'université d'Illinois, the \textit{Kiewit Computation Center} du Dartmouth College, la \textit{National Science Foundation}, la fondation Ford et l'entreprise IBM donc.}. Avec ce dernier, ils organisent une école d'été titrée \textit{Summer Institute in Computer Applications to Classical Studies}\footnote{L'équivalent de 59~800\$ au 31 janvier 1968 est de 479~745\$ en août 2021 d'après le calculateur d'inflation du \textit{Bureau of Labor Statistics}, \cite{noauthor_cpi_nodate})}. Cet événement donne naissance à un corpus d'une vingtaine d'œuvres grecques et latines plus ou moins complètes: on y trouve à côté des classiques homériques des morceaux d'œuvres, parfois inattendus, dont la découpe est particulière, tel le poème 64 de Catulle qui est édité seul, trois œuvres de l'\textit{Appendix Vergiliana}, les livres I, IV, IX et XII de l'\textit{Énéide}. En dehors d'un rapport, ce corpus ne semble pas avoir eu une vie particulièrement riche, ni de nom d'ailleurs: il est pris en charge par l'\textit{American Philological Association}, est dupliqué à la demande par des institutions, mais très vite se voit couper de tous fonds supplémentaires, là où, comme le note 20 ans plus tard Brunner, le fond des monographies n'est pas touché\footcite{brunner_classics_1993}.

Au début des années 70, nous trouvons la trace d'un seul autre corpus, celui du \textit{Thesaurus Linguae Grecae} (TLG) dont les prémices remontent à 1971\footcite{brunner_classics_1993} et dont la naissance est actée en 1973\footnote{Parmi les articles cités par T.~F.~Brunner lui-même sur la fondation du TLG, au moins un est indisponible en France: \cite{hugues_homer_1987}}. Si son nom est dérivé d'un projet humaniste du 16\textsuperscript{e} siècle et est en écho à celui du dictionnaire \textit{Thesaurus Linguae Latinae} (TLL), il ne s'agit que d'une simple inscription dans une tradition des grands travaux humanistes: à contrario des deux derniers, ce projet se veut dès les premières conférences un corpus de texte et non un thésaurus, un dictionnaire fortement enrichi. Les premières versions du corpus voient rapidement le jour pour atteindre 61 millions de mots en 1988\footcite{brunner_overcoming_1988}, via une externalisation de la copie \enquote{manuelle} des volumes en Corée du Sud (1972-1980) puis aux Philippines\footcite[p. 111]{helgerson_cd-rom_1988}. Ce corpus pose une difficulté de taille, à savoir son alphabet: en 1972, seuls les caractères ASCII\footnote{\textit{American Standard Code for Information Interchange}} existent informatiquement, ils sont au nombre de 128 et couvrent les nombres, les caractères latins hors diacritiques et les signes de ponctuation. Il faut trouver une solution pour les caractères grecs, et c'est un certain David W.~Packard qui trouve une méthode pour encoder ces derniers, le BetaCode. Cette méthode de transcription, dont il reste des traces encore aujourd'hui dans des fichiers de Perseus, propose l'encodage des diacritiques via les signes de ponctuation: ainsi, la parenthèse \texttt{)} remplace l'esprit doux, tandis que le pipe \texttt{|} représente le iota souscrit, par exemple, \textgreek{αναλαβόντες δὲ καθ᾽ ἕκαστον} donne \texttt{analabo/ntes de\ kaq`e(/kaston.}


Présent à la réunion de fondation du projet TLG et résolveur du problème d'encodage, David W.~Packard n'est pas seulement important pour ce dernier: il fonde le \acrfull{phi} (\acrshort{phi}) en 1987\footcite{helgerson_cd-rom_1988}, institut ayant pour visée de produire des corpus, dont un équivalent latin du TLG\footnote{Le corpus latin n'est qu'un des multiples corpus du PHI, même si l'on utilise souvent PHI uniquement pour se référer au corpus latin.}. Docteur en langues anciennes depuis 1967 et spécialiste des tablettes en linéaire A, il est nommé en octobre 1968 comme membre du \textit{Special Committee for Computer Problems} aux côtés de N.~A.~Greenberg, Stephen Waite, William H.~Willis et Robert Dyer, qui en est le président. La première version CD-ROM apparait en 1991 (PHI\#5), les versions précédentes n'ont pas laissé beaucoup de traces et il existe un certain flou autour de la chronologie: un article de 1991 de J.~Raben mentionne qu'il est \enquote{en cours de direction par David W.~Packard}\footcite{raben_humanities_1991}, un article de S.~Hockey parle d'environ 8 millions de mots en 1994\footcite{hockey_electronic_1994}. Il semble qu'un premier CD de textes latins, en particulier de la \textit{Bible}, soit publié rapidement, dès 1987 dans le contexte d'un projet annexe du \textit{Center for Computer Analysis of Texts} (CCAT)\footcite{groves_tovs_1990, cornell_greek_1989}.

Le fossé temporel qui sépare les deux \enquote{premiers} projets américains encore présents s'explique d'une part par le coût que représentent ces projets, d'autre part par le manque d'équipement informatique au début des années 1970. Il faut attendre l'avènement du \textit{micro-computer} (micro-ordinateur en français, terme tombé en désuétude pour ordinateur tout simplement ou bien même PC) et de l'Apple II par exemple pour voir une montée de l'équipement informatique. Plus encore, le marché de l'informatique se popularise avec l'arrivée des interfaces utilisateurs graphiques (\acrshort{gui}), notamment à travers l'Apple Lisa (1983)\footcite{noauthor_history_2021} ou l'Apple Macintosh (1984) ou encore leur équivalent DOS et Windows déployés par IBM. Et au-delà même du micro-ordinateur, c'est le standard CD-ROM qui apparait et permet de partager des données beaucoup plus importantes en 1984\footnote{Le standard est créé plus tôt, mais ne s'applique d'abord pas aux données.}. Certains historiens parlent d'une montée en puissance, dans le secondaire comme dans le milieu académique, de l'usage des ordinateurs en classe\footcite{simkin_introduction_1989, latousek_fifty_2001}. Mais le changement s'opère sur toute la population américaine: d'après un rapport de 1999\footcite{kominski1999access}, on voit doubler  entre 1984 et 1989 le nombre de foyers américains ayant un ordinateur, une augmentation de 64\% de l'usage de l'ordinateur à l'école pour les 3-17 ans, de 41.5\% pour les 18+ à l'école\footnote{Par déduction, il devrait s'agir principalement du milieu universitaire.} et de 33\% au travail (\textit{cf.} Table \ref{tab:computer-ownership}).

\begin{table}[ht]
\centering
\begin{tabular}{l|rrr}
                                               & 1984 & 1989 & 1993 \\ \hline  \hline
Foyer avec un ordinateur                       & 7.9  & 14.4 & 22.8 \\ \hline
3-17 ans ayant accès à un ordinateur à l'école & 28.0 & 46.0 & 60.6 \\
18+ ans ayant accès à un ordinateur à l'école  & 30.8 & 43.6 & 53.8 \\
18+ ans ayant accès à un ordinateur au travail & 24.6 & 36.8 & 45.8 \\ \hline
\end{tabular}
\caption{Niveau d'accès et d'usage en \% des ordinateurs aux États-Unis sur la décennie 1984-1993, d'après l'\textit{U.S.~Census Bureau, Current Population Survey, October 1984, 1989, 1993} repris par \cite{kominski1999access}}
\label{tab:computer-ownership}
\end{table}

Il faut comprendre à quel point l'histoire des projets américains est intimement liée aux grandes entreprises du domaine de l'informatique. Si elles sont souvent présentes en financement suite à des demandes, comme IBM sur le co-financement NEH de 1968-69, ou Apple comme nous le verrons pour Perseus, elles sont aussi présentes à travers les réseaux sociaux de la côte ouest. En effet, qu'il s'agisse de PHI ou de TLG, des enfants de grands patrons sont à la source du financement des projets: ainsi, David W.~Packard (\acrshort{ucla}) est le fils du co-fondateur de Hewlett-Packard et utilise cette ressource pour financer le PHI; de son côté, Marianne McDonald finance le TLG alors qu'elle n'est qu'étudiante en licence grâce à son père, patron de la \textit{Zenith corpustion}, entreprise méconnue en France, mais importante pour les États-Unis puisqu'elle y commercialise alors télévisions et bouquets de chaînes. Le financement de ces entreprises (1 million de dollars offerts par M.~McDonald, en 1972, soit environ 6,656 millions de dollars d'août 2021) est constant et semble \enquote{inévitable} pour ces projets jusqu'à la fin des années 80.

\subsubsection{La lente apparition du projet Perseus}

Dans les années 80, en parallèle du développement de PHI et du TLG, un autre futur mastodonte du corpus en lettres classiques commence à se formaliser: Perseus. Mais l'histoire de Perseus ne commence pas comme l'histoire d'un concurrent à PHI et au TLG, mais bien comme un ajout à ces derniers. 

% Contexte de la création de Perseus: le project HCCP
En effet, en 1982, Gregory R.~Crane, alors doctorant à Harvard, ainsi que Neel Smith, Kenneth Morrell et Elli Mylonas cherchent à améliorer l'écosystème pour l'étude des langues anciennes sur plateforme informatique. À cette période-ci, il faut comprendre que le TLG n'est disponible que sur sa propre plateforme matérielle et logicielle, à savoir l'Ibycus, développé spécifiquement par D.~W.~Packard et financé par HP. Or, il s'agit aussi de la période de \enquote{standardisation} de la programmation, notamment à travers le développement d'Unix et de ses clones (dont GNU). Dans un article rétrospectif sur l'histoire du champ, G.~R.~Crane\footcite{schreibman_classics_2004} parle du développement du moteur de recherche pour le TLG permettant de faire usage des données du TLG. En effet, dès 1994, alors qu'E.~Mylonas présente le projet Perseus\footcite{mylonas_perseus_1993}, elle intègre l'histoire de Perseus dans son rapport au TLG: l'équipe historique de Perseus s'intéresse d'abord à produire des ajouts pour le TLG, dont un \enquote{puissant moteur de recherche plein-texte}\footnote{\textit{\enquote{... spawned at Harvard a software project which developed a powerful full-text retrieval system.}}}. Dans son ouvrage massif \textit{Bits, Bytes and Biblical Studies} de 1986\footcite[p. 598]{hughes_bits_1987}, J.~J.~Hughes parle du \acrfull{hccp} (\acrshort{hccp}) qui cherche alors à développer pour UNIX et en particulier pour Mac un nouveau système complet autour de l'édition, de l'entrée de données et de la recherche plein texte. À cette époque, Perseus ou l'HCCP sont financés tour à tour par IBM, Apple (y compris à travers une stratégie globale d'adoption de la firme à la pomme par Harvard) et Xerox du côté des entreprises.

% HCCP et Morpheus
La fin des années 1980 montre encore l'intérêt d'abord de l'équipe Perseus pour l'amélioration de l'environnement de travail - en grec ancien uniquement pour le moment. Le TLG et le PHI-CCAT proposent depuis quelques années alors un outil pour la lemmatisation et l'annotation morphologique du grec ancien, appelé MORPH et développé encore une fois par David. W.~Packard en assembleur puis dans son propre langage de programmation, l'IBYX\footcite[p.554-555]{hughes_bits_1987}. L'équipe de Crane propose donc d'abord d'améliorer MORPH et développe Morpheus, qui gère désormais les accents et les dialectes\footcite{mylonas_perseus_1993} et propose une formalisation par règle de la langue grecque. L'ensemble se repose sur un dictionnaire central, l'\textit{Intermediate Liddell-Scott Lexicon}, ce qui permet donc aux utilisateurs d'avoir un référentiel de lemmes consultable et navigable.

% De l'HCCP à Perseus: compléter le TLG
Et c'est à travers l'ensemble de ce travail autour de l'infrastructure logicielle que l'HCCP finit par devenir le \textit{Perseus project}. Commencé en 1990, le projet ne vise pas à concurrencer PHI et TLG. G.~R.~Crane et son équipe affirment dès le départ cette absence de concurrence: \enquote{\textit{The Perseus Project, with its broad range of materials, was designed to complement the textual focus of the TLG}}\footcite[p. 134]{mylonas_perseus_1993}. Il va donc chercher à compléter ce dernier en apportant de nouvelles informations, comme - pour la première fois - des traductions des textes classiques et des ressources graphiques. Le premier Perseus vise ainsi à accompagner d'images les corpus textuels disponibles jusqu'ici - on parle alors de 10~000 images à obtenir entre 1990 et 1993 - compilées avec les textes sur \enquote{\textit{compact disks and video disks}}.

% Textes et traductions
Si l'information textuelle en langue originale n'est pas avancée comme étant au centre du projet Perseus, l'équipe promet tout de même d'amasser 100 MB de données d'ici la fin du projet. Le corpus original se veut centré autour du Ve siècle avant notre ère avec des incursions vers d'autres classiques et accompagné de traductions, anciennes, modernisées et modernes fournies par des partenaires\footnote{Les premiers auteurs mentionnés sont \enquote{Eschyle, Sophocle, Hérodote, Pindare, {[...]} Pausanias, Pseudo-Appolodore, les vies grecques de Plutarque, {[...]} Homère, Aristophane, les orateurs attiques, Thucydide, la poésie élégiaque et lyrique, Platon et un peu d'Aristote {[sic]}. Des morceaux intéressants de Diodore de Sicile et Strabon} seront ajoutés plus tard. \cite{mylonas_perseus_1993}}. Cette sélection, plus restreinte que celle du TLG, vise alors les étudiants et non les chercheurs: il s'agit d'accompagner les hellénistes en formation et les non-spécialistes - comme les historiens - dans la lecture des textes en proposant des versions numériques alignées avec leur traduction \footnote{\enquote{\textit{The choice to include translations is to allow students and other scholars who are not fluent readers of Greek to work {[...]} and to broaden the circumstasnces in which Perseus will be consulted.}}, \cite[p. 136]{mylonas_perseus_1993}}. La création des données textuelles est alors faite par copie au clavier, les technologies d'OCR étant trop génératrices d'erreurs à l'époque\footnote{L'équipe a testé l'OCR au début des années 90 et estime alors que le temps de correction n'est pas plus intéressant qu'une copie manuelle.}.

Les années 80 sont des années particulièrement riches technologiquement, nous l'avons vu, et en particulier en termes de standardisation de l'écosystème informatique: partager information et code entre entreprises et consultants, entre chercheurs ou entre projets devient une problématique importante. Et ces années-là voient apparaitre un nouveau langage, le SGML\footnote{\textit{Standard Generalized Markup Language}}, un langage à balise destiné à structurer l'information textuelle plus facilement et adopté par l'\acrfull{iso} (\acrshort{iso}) en 1986. Un an plus tard, 32 chercheurs en sciences humaines et sociales se rencontrent au Vassar College de Poughkeepsie, dans l'état de New York , et posent des principes d'interopérabilités, qu'ils nomment alors les \textit{Poughkeepsie Principles}\footcite{vanhoutte_introduction_2004}. Ces principes\footcite{noauthor_design_1988}, au nombre de 9, définissent les lignes directrices pour la fondation des \textit{Text Encoding Guidelines} et commencent ainsi par celui d'obtenir un \textit{standard} pour l'échange de données dans le contexte des recherches en sciences humaines. Cet objectif est au centre de ce qui devient, en 1990, la \textit{Text Encoding Initiative} et sa première version des \textit{guidelines} qui visent à encadrer la manière d'encoder l'information textuelle et ses métadonnées. La \textit{Text Encoding Initiative} vise alors à \enquote{fournir des \textit{guidelines} explicites qui définissent un format textuel approprié au partage de données et à leur analyse; le format doit être indépendant du point de vue matériel\footnote{Comme nous l'avons vu, la richesse matérielle à l'époque fait qu'il existe encore de grandes possibilités de conflits entre différentes manière de gérer des données à cause de l'implémentation physique du principe informatique.} et de celui du logiciel, rigoureux dans sa définition des objets textuels, facile à utiliser, et compatible avec les standards existants. On attend du SGML de fournir une base adéquate pour ces \textit{guidelines}}\footnote{\enquote{\textit{The primary goal of the Text Encoding Initiative is to provide explicit guidelines which define a text format suitable for data interchange and data analysis; the format should be hardware and software independent, rigorous in its definition of textual objects, easy to use, and compatible with existing standards. The \acrlong{sgml} (\acrshort{sgml}) is expected to provide an adequate basis for the guidelines. }}, \cite{noauthor_design_1988}}.

Il est alors compréhensible, devant cette révolution de l'encodage du texte, de voir le projet Perseus adopter SGML dès sa conceptualisation\footcite[p. 138]{mylonas_perseus_1993}, bien qu'aucun de ses membres fondateurs n'ait participé à la réunion de Poughkeepsie\footnote{Il est intéressant de voir que l'article publié en 93 ne parle pas de TEI directement, mais bien de SGML, tout au plus est renvoyée en notes et bibliographie une mention du travail de Lou Burnard sur la TEI, au même titre que de l'utilisation de la technologie SGML par le département de la défense, \textit{cf.} \cite[notes 8 et 9, p.~155]{mylonas_perseus_1993}}. Ils adoptent en effet ce standard dès le départ comme format d'archivage en estimant que seul un format d'archivage standardisé permettra de survivre aux évolutions technologiques et en particulier de survivre au logiciel utilisé à l'époque, à savoir \textit{Hypercard} sur Mac: plus de vingt ans plus tard, les corpus originaux de Perseus sont toujours disponibles, on ne peut que confirmer cette intuition. Mais cette opportunité prise, il reste aussi à l'équipe de traduire en SGML les pratiques de mise en page et d'édition du domaine de l'antiquité, à savoir ses modes de citation en structures logiques ou éditoriales (chapitre, section, vers, pages de \textit{Stephanus} pour Platon), afin de ne pas rompre avec cette tradition philologique: le passage de l'imprimé au numérique permet ainsi de traduire les informations fournies par la mise en page en métadonnées sur le texte. Ainsi, en dehors de ces informations éditoriales, une annotation supplémentaire dans le SGML de la métrique, des \enquote{types de discours dans la prose historique et rhétorique}, les noms des intervenants dans les pièces est considérée dès la conception du projet\footcite[p. 137]{mylonas_perseus_1993}.

Bien que les données textuelles soient ultimement celles qui nous intéressent pour notre travail, ignorer la partie non textuelle du projet Perseus à sa fondation ne permettrait pas de comprendre en quoi ce projet ne se pose - au départ - pas comme un concurrent au TLG. Pour les ressources sur l'archéologie, Perseus souhaite en effet se constituer comme une \textit{bibliothèque}, avec une couverture dont la sélection est le résultat d'un \enquote{\textit{opportunisme guidé}}\footcite[p. 145]{mylonas_perseus_1993}. L'objectif est de rassembler, pour la première fois sous une forme numérique, un outillage pédagogique et de recherche permettant d'aborder une grande variété d'objets et de thèmes pour la Grèce ancienne: cela comprend photographies, dessins, plans, mais aussi descriptions ou textes d'introduction thématique traitant de la sculpture ancienne par exemple. Selon les fondateurs du projet\footcite[p. 143]{mylonas_perseus_1993}, par manque d'expertise entre autres et de concurrents numériques prédatant ce projet, l'équipe de Perseus va chercher à rassembler les principes de trois modèles:
\begin{itemize}
    \item ceux d'une archive photographique, avec des descriptions sommaires qui se concentrent sur la description de l'image elle-même;
    \item d'une base de données ou d'un catalogue muséal ou de fouilles, centré sur l'objet et concentré sur la description de propriétés, mais sans projet éditorial;
    \item d'une publication plus enrichie, du type \enquote{catalogue archéologique multi-volumes}  proposant à la fois des volumes de textes et des planches, mais nécessitant une plus grande sélection, et donc omission, d'objets.
\end{itemize}
Le résultat de cette sélection doit offrir une modélisation suffisante pour découvrir, se former, et enseigner. Son implémentation suit encore les principes de SGML pour les contenus textuels et une modélisation complexe des métadonnées permettant formellement un enrichissement par des contributeurs extérieurs à l'avenir\footnote{\enquote{\textit{Perseus cannot possibly foot the costs of assembling the quantities {[of information ...]}; there, we must design a system that will not merely permit but encourage collaboration.}}, \cite[p. 148]{mylonas_perseus_1993}. Nous verrons plus tard que cet objectif deviendra un \textit{leitmotiv} de G.~Crane à travers les évolutions de Perseus.}

\subsubsection{Période manquante ? L'apparition de la patristique numérique}

% Introduction du CETEDOC et du CLCLT2
Jusqu'au projet Perseus, l'ensemble des efforts se font sur les périodes classiques, canoniques, celles du \enquote{bon grec} ou du \enquote{bon latin}, des orateurs ou dramaturges, des poètes épiques, celles des œuvres que l'on étudie pour l'agrégation en France. Les pères de l'Église sont rarement inclus dans les projets, et s'ils le sont, ils sont sous-représentés et n'y apparaissent alors que partiellement. Cette scission, entre période chrétienne et période classique, se retrouve aussi dans le travail des corpus: si on trouve le LASLA à Louvain pour s'occuper de la période classique (jusqu'à la fin du Ier siècle environ), un autre laboratoire se fonde en 1968 sous la direction de Paul Tombeur pour traiter des données \enquote{médiévales}: le Centre de Traitement Électronique des Documents, ou CETEDOC\footcite[p. 70]{gueret_analyse_1977}. Ce centre se concentre pendant vingt ans à la production de données similaires à celles du LASLA, des concordances, des données lemmatisées. En 1984\footcite{iogna-prat_centre_1984}, le centre se compose \enquote{d'un ingénieur informaticien, un analyste informaticien, une secrétaire, une (demi-!) {[sic]} assistance, deux universitaires dont P.~Tombeur {[...]} et des vacataires}. Les services que propose le centre incluent alors la reprographie de thèses, la mise à disposition des données collectées et l'accueil de chercheur pour faire traiter des textes \enquote{à la mode} du CETEDOC.

% Du CETEDOC au CLCLT
%  Rappel que Tombeur était à Poughkeepsie
Les années 80 représentent cependant un tournant pour le centre: la question de la mise à disposition de corpus \enquote{médiévaux}~-~il faut entendre ici pères de l'Église et textes médiévaux en général~-~se fait de plus en plus pressante par son absence des corpus principaux, PHI et TLG. En 1981, à Liège, au congrès mené par le LASLA sur \enquote{l'informatique et les sciences humaines}, Paul Tombeur parle alors de publier un \textit{Thesaurus Patrum Latinorum}, englobant les textes chrétiens latins et les textes médiévaux publiés dans les collections \textit{Corpus Christianorum, Series latina} et \textit{Continuatio Mediaevalis}\footcite{tombeur_constitution_1981}. Le directeur du centre est présent à Poughkeepsie en 1987 et signe l'appel, répétant ainsi ces nouvelles ambitions\footcite{burnard_report_1988}. Et de fait, en 1991 sort chez Brepols la \textit{CETEDOC Library of Christian Latin Texts on CD-ROM}, ou CLCLT, une base de données comprenant 21 millions de mots et l'équivalent de 300 volumes imprimés\footcite[p. 90]{bucknall_review_1994}. Si T.~Bucknall la compare dès lors avec les bases PHI ou TLG, la situation est légèrement différente: le CLCLT est avant tout une base à interroger plus qu'un corpus à lire, et c'est ainsi qu'il est implémenté.

% Possibilités et limites du CLCLT2: impression de 30 résultats par exemple
La base CLCLT consiste alors en une interface donnant accès à un système de recherche (par forme, par groupe de formes, par forme partielle, par proximité entre formes), mais repose sur un séquençage du texte en \textit{sententiae}, des phrases que les éditeurs ont produites dans leur édition. Choix regrettable si l'on en croit les comptes-rendus de l'époque, tant elle produit des disparités: en effet, \enquote{les uns {[éditeurs]} paraissent préférer des phrases très longues {[tandis que]} les autres s'appliquent à hacher menu le discours}\footcite{gryson_nouvelle_1992}. Et de ce séquençage dépend alors bon nombre de recherches qui ne peuvent inclure les éléments de \textit{sententiae} voisines. La base est cependant munie d'un très grand nombre de métadonnées, de notes critiques sur le texte, sur son authenticité et son attribution par exemple. On peut y lire les textes, bien que l'on ait vu plus confortable: les œuvres ne comprennent pas d'index, et si l'on veut lire le chapitre 14 d'un long ouvrage, il faudra passer de page en page manuellement. Le logiciel est uniquement disponible sur PC, en particulier sous DOS à l'époque. Les résultats sont imprimables, mais les comptes-rendus divergent: si R.~Gryson semble indiquer l'absence de limite lors de l'impression\footcite[p. 421]{gryson_nouvelle_1992}, les autres sources, dont T.~Bucknall\footcite[p. 94]{bucknall_review_1994}, semblent s'accorder sur une limite pour le téléchargement ou l'impression à 30 lignes consécutives de texte.

% L'apparition de la PLD
Le vide laissé par PHI et le TLG ont cependant intéressé d'autres éditeurs que Paul Tombeur, puisqu'un concurrent au CLCLT apparait au même moment: la \textit{Patrologia Latina Database}, ou PLD, éditée par l'entreprise Chadwyck-Healey. Basée sur une numérisation de la patrologie de Migne, une somme des textes chrétiens du IIe au bas moyen-âge éditée au XIXe siècle économique\footnote{\enquote{Migne présente sa Patrologie comme une \textit{bibliotheca oeconomica} et {[...]} comme étant du bon, bon marché}, \cite[p. 228]{tombeur_pld_1993}}, elle propose sous une interface remaniée \textit{DynaText} et à partir de fichiers en SGML TEI\footcite{smith_dynatext_1993} de lire ou de chercher à l'intérieur d'un immense corpus sur 5 CD-Roms. C'est à notre connaissance le premier projet commercial en SGML TEI, et le premier très large projet qui utilise cette technologie. Contrairement au CLCLT, la PLD fonctionne sous les OS principaux de l'époque (Mac, \enquote{UNIX avec X-Windows} et Windows\footcite{smith_dynatext_1993}). 221 volumes de la PL sont repris et acceptent une recherche plein texte plus ou moins équivalente à celle du CLCLT. Les métadonnées des premières versions sont par contre particulièrement pauvres: les périodes sont divisées sommairement en deux périodes, \textit{medieval}, avant 1500, et \textit{modern}, après cette date. Elle permet par contre la lecture ciblée de documents, et ne nécessite pas, comme le CLCLT, de faire défiler manuellement les contenus. Enfin, contrairement au CLCLT, elle permet l'export du SGML et contient l'apparat critique des textes qu'elle comporte.

Cette collision littéraire et temporelle conduit les deux bases de données à être comparées et à faire naître des controverses. D'abord, car les deux objets ne font pas le même prix: la PLD est annoncée originellement pour 50~000\$ tandis que le CETEDOC l'est pour 3~800, avec des mises à jour bi annuelles\footcite{bucknall_review_1994}. Le prix de la PLD semble varier beaucoup, y compris suite à la réaction du public: on parle de 70~000\$ quand elle fut annoncée sur bandes magnétiques au début des années 90 et de 45~000\$ en précommande dans l'article de Ron W.~Crown\footcite{crown_comparing_2000}, de 27~000£ en 1995 chez R.~Gryson\footnote{Avec un taux de change en 1995 d'environ 1.55\$ pour 1£, 41~850\$, d'après \cite{noauthor_british_2021}}, de prix négociés chez certaines petites bibliothèques aussi bas que 5~000\$\footcite[Note 10, p.~108]{crown_comparing_2000} qui la rendent alors hautement compétitive avec le CLCLT. En 1993, une discographie\footcite{pellen_les_1993} nous permet de comparer ces prix: Perseus se vend pour 230\$, le TLG pour 5~860 francs français hors taxe\footnote{Avec un taux de change à 5.66, 1~035\$ d'après \cite{noauthor_france_nodate}}. Avec des prix relativement stables pour les outils cités, un article de Beth Juhl indique un prix de 50\$ par CD en 1995 pour le PHI\footcite{juhl_ex_1995}. À cette époque, l'offre de la PLD est donc plus de dix fois plus chère que toute autre base de données majeure en lettres classiques.

Ensuite, le fond de la controverse dépasse cependant de loin les questions des possibilités des différents outils\footnote{Bien que certaines fonctionnalités de la PLD soient \enquote{discutables}: d'après R.~Gryson, les \enquote{titres et sommaires, références scripturaires, appels de notes} font partie du texte dans les résultats, et les recherches en contexte incluent, si le terme est en début d'œuvre ou en fin, le contenu de l'œuvre suivante ou précédente. \cite[p. 148]{gryson_patrologia_1997}} et celles du prix. Le principal reproche fait à la PLD concerne sa source, la patrologie de Migne. Qu'il s'agisse de T.~Bucknall ou de R.~Gryson, les comptes-rendus sont sévères: la patrologie de Migne n'est pas \enquote{conforme aux exigences de la science moderne}\footcite[p. 147]{gryson_patrologia_1997}: éditions datées du 16e siècle, reprise telles quelles par leur collateur au milieu du 19e siècle, erreurs d'attribution \enquote{inacceptables}, et pire, erreurs d'impressions qui se retrouvent ensuite dans le texte proposé par la PLD, car directement copié, sans vérification, par les équipes de Chadwyck-Healey, occasionnant, en plus de possibles erreurs de copies, une augmentation du nombre de coquilles dans la base. La controverse est clairement lancée par P.~Tombeur en 1992 lors de son article introductif au Bulletin de Philosophie Médiévale\footcite{tombeur_informatique_1992} (BPM) qui cherche à donner des perspectives au domaine médiéval dans ses projets numériques, en appelant notamment à ne pas dupliquer les efforts. Après avoir présenté son projet au CETEDOC comme \enquote{ne voulant pas être une simple mise en mémoire des œuvres {[...]} rassemblées par Migne}\footcite[p. 44]{tombeur_informatique_1992}, il présente en contraste la PLD comme une \enquote{photocopie électro-magnétique de l'œuvre de l'abbé Migne}\footcite[p. 45]{tombeur_informatique_1992} dont le contenu lui-même est douteux. Un droit de réponse peu avisé de Sir Chadwyck-Healey précise qu'il ne s'agit pas de simples fac-similés (avait-il vraiment compris que P.~Tombeur parlait de photographie ?), mais bien de textes recopiés\footcite{chadwick-healey_droit_1993}, ce que re-précise P.~Tombeur dans une réponse au droit de réponse\footcite{tombeur_reponse_1993} qu'il fera suivre ensuite d'un article plus large de critique - dans le même volume - de la PLD\footcite{tombeur_pld_1993}. L'affaire semble se clore, dans le BPM en tout cas, en 1994 avec l'ultime réponse d'un membre de la PLD\footcite{jordan_facts_1994}. Si des problèmes techniques sont évoqués, que des annonces publicitaires sont interprétées et réinterprétées\footnote{Par exemple, il ne serait pas sûr que les formes courantes du type \textit{ipse} soient cherchables car constituant des \textit{stop-words}}, le problème revient toujours sur la qualité des données originales. Dans sa comparaison des deux outils\footcite{crown_comparing_2000}, R.~W.~Crown semble absoudre rapidement les auteurs de la PLD pour recommander l'usage de cette dernière -~à condition que le budget suive~-~car elle ne nécessiterait pas l'usage de sources papiers et formerait un \enquote{véritable "e-Book"}. Si l'on en croit les autres comptes-rendus, et notamment les faiblesses en termes d'attribution des textes et l'historique derrière les éditions, cela semble loin d'être vrai en 1995.

Quoi qu'il en soit, entre 1970 et 1995, on voit apparaître de nombreux projets, dont nous n'avons retenu que les principaux et les survivants, qui cherchent à numériser les corpus, entre autres pour rendre plus rapide le travail des chercheurs. Ces \enquote{incunables}, comme les appelle R.~W.~Crown\footcite[p.~107]{crown_comparing_2000}, forment alors une évolution considérable, sans véritablement transformer les approches du texte: il s'agit de trouver plus facilement un terme, et en cela, c'est une réussite. Deux exemples d'époques sont souvent cités: John J.~Hughes, dans un article de 1986\footcite{hughes_ibycus_1986} cité par L.~W.~Helgerson\footcite{helgerson_cd-rom_1988}, qui indique qu'une recherche de \textit{\textgreek{διαθεκε}} avait pris 25 minutes sur le TLG pour 1~079 résultats alors même que cette recherche, pour des résultats moins importants, lui avait pris \enquote{la plus grande partie d'une semaine dans les bibliothèques de l'université de Cambridge}; Peter Zahn, en 1992, qui explique comment le CLCLT lui a permis rapidement d'identifier un nouveau fragment d'Augustin en cinq minutes, là où la recherche manuelle lui avait pris cinq jours\footcite[p. 427]{zahn_kirchenvater-texte_1992}.

\subsection{Web, standards et corpus: changement d'échelles, changement de pratiques}

Dès le milieu des années 1990, la connexion à internet commence à rentrer dans les foyers avant de vivre une explosion au début du troisième millénaire, en France comme dans le reste du nord économique. D'après T.~Karsenti et G.~Clermont, on parle d'un passage de seize à sept cents millions d'internautes dans le monde entre 1995 et 2006\footcite{karsenti_les_2006}. Aux États-Unis, en 1995, les chiffres sont de dix millions de foyers avec un accès internet (et dix-huit millions équipés d'un modem, mais sans connexion\footcite{nw_americans_1995}). Dans les bibliothèques américaines, 25\% d'entre elles fournissent un accès à internet en 1996, mais ce chiffre cache la spécificité du déploiement technique: 96\% des villes de 250~000 à 499~999 habitants et 84\% des villes de plus d'un million d'habitants mettent à disposition des connexions internet dans leurs espaces anciennement réservés au papier\footcite{zumalt_internet_1998}. En France, la pénétration de la technologie est un peu plus lente au démarrage, mais les chiffres de l'\acrfull{arcep} (\acrshort{arcep}) montrent une forte croissance: 1,28 million d'abonnements en 1998, 5,33 millions en 2000 (dont 68~000 xDSL), 12,648 millions en 2005; en 2020, ce chiffre atteint 30,627 millions (\textit{cf.} table \ref{tab:chap1:croissance-abonnements-internets}). Avec cette révolution de l'accès aux contenus \enquote{dématérialisé} vient donc l'ère du corpus sans CD-ROM.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|r|rrrrr}
\hline
                                   & 1998 & 2000     & 2005     & 2010     & 2015     & 2020     \\ \hline
Bas débit (en millions)                         & 1.28 & 5.26     & 3.75     & 0.48     & 0.09     &          \\
Haut débit                         &      & 0.07     & 8.90     & 20.23    & 22.66    & 15.96    \\
Très haut débit                    &      &          &          & 1.13     & 4.21     & 14.67    \\ \hline
Total (en millions)                              & 1.28 & 5.33     & 12.65    & 21.84    & 26.96    & 30.63    \\ \hline
Croissance en 5 ans (sauf 98-2000) &      & 416.45 \% & 237.27 \% & 172.69 \% & 123.42 \% & 113.62 \% \\
Croissance en 10 ans               &      &         &          & 409.74 \% &          & 140.23 \% \\ \hline
\end{tabular}%
}
\caption[Caption for LOF]{Évolution du nombre d'abonnements internet en France d'après l'ARCEP (hors abonnements mobiles)\footnotemark. La décennie 2000-2010 représente une croissance extrêmement importante, démontrant bien la pénétration de cette technologie dans les habitudes des Français.}
\label{tab:chap1:croissance-abonnements-internets}
\end{table}
\footnotetext{\cite{lautorite_de_regulation_des_communications_electroniques_indicateurs_nodate}}

\subsubsection{Du CD-ROM à internet}

En parallèle de la massification vue plus tôt de l'accès à l'ordinateur (\textit{c.f.} table \ref{tab:computer-ownership}), on assiste à la naissance du \textit{Personal Computer} et de monopoles dans le marché des systèmes d'exploitation\footcite{schlender_whos_1990}. En 1983, IBM et Apple représentent environ 40\% des ventes (aux États-Unis), les 60\% restants contenant toute une myriade d'autres OS. En 1990, les PC IBM (dont DOS) représentent 85\% du marché, Apple est second à 5\%. En 1998, 90,5\% du marché appartient à Windows pour les \acrfull{os} (\acrshort{os}), Mac tient bon à 5\% et Linux grimpe légèrement sur ce marché avec 2,1\%\footcite{miles_windows_1999, reimer_total_2005}. Cette croissance et solidification du marché autour de deux interfaces, proches, permet une simplification de l'apprentissage de l'informatique. Et avec la massification -- au moins dans les bibliothèques -- de l'accès à internet, le passage des médias CD-ROM à des sites internet a -- semble-t-il -- été une conversion évidente. \textit{Perseus} -- avant même sa version 2.0 -- s'y convertit dès 1995, la \textit{Patrologia} en 1996, la \textit{Duke Databank of Documentary Papyri}, sorti en 1982 sur bandes, en 1996 aussi. Le CLCLT du CETEDOC  et PHI restent uniquement sur CD-ROM en 1997 parmi les corpus originaux. Pour le premier, ce passage se fait vers le web avec la version 6 en 2005, soit 9 ans après son concurrent direct. P.~Tombeur prend bien en compte internet dans son rapport au \acrfull{bpm} (\acrshort{bpm}) de 1997, mais ne fait aucun lien avec son œuvre\footcite{tombeur_informatique_1997}, il faudra attendre une mention en 2004 pour voir se préciser une version en ligne\footcite{tombeur_augustin_2004}, qui sera ensuite renommée \acrfull{llt} (\acrshort{llt}) en 2009.

Dans son article de comparaison entre la PLD et le CLCLT\footcite{crown_comparing_2000}, Ron W.~Crown mentionne quant à lui la nouvelle interface de la PLD en ligne, mais aussi les nouveaux choix économiques qui l'accompagnent. Contrairement à Perseus, la PLD fait le choix du site web à abonnement, coûtant entre 400\$ par an pour les universités possédant la PLD en CD-ROM et 3~995\$ par an pour les universités aux plus hauts budgets. L'interface ne diffère pas foncièrement de la version locale et permet la lecture des documents. Des autres incunables, le TLG fait le choix du modèle payant ou semi-payant tandis que le PHI, bien que sortant extrêmement tard, entre 2011 et 2015\footnote{Archive.org donne une archive en 2011 du site des œuvres latines -- que nous avons tendance à conserver -- tandis que la \textit{review} du RIDE donne une sortie en 2015. \cite{daniel_kozak_classical_2018}}, est mis à disposition gratuitement en ligne, mais avec d'importantes restrictions concernant sa réutilisation et son partage. Ainsi, il faut seize ans, de 1995 à 2011, pour retrouver en ligne l'ensemble des incunables. 

% De Perseus à la Perseus Digital Library
\enquote{La seconde génération de ressources électroniques pour lettres classiques}, d'après le titre du premier index proposé par Maria Pantelia\footcite{pantelia_electronic_1994}, commence entre autres par le passage du CD-ROM au web, et réalisé en premier par Perseus, qui semble avoir été le plus rapide à faire cette transition. Dans un article de 1996\footcite{crane_building_1996}, G.~R.~Crane parle de cette transition -- presque \enquote{indolore} -- d'une application reposant sur des \textit{Hypercards} à un site web\footnote{On ne parle pas d'application web avant quelques années. Une recherche sur Google N-Gram, avec ses défauts, montre une apparition du terme autour des années 2000-2005, avec la naissance du web 2.0}. Selon lui, si le passage a été autant facile, c'est grâce au choix technologique coûteux du SGML (\enquote{Big costs, huge potential, growing benefits}\footcite[p. 7]{crane_building_1996}) pour encoder les textes dès l'origine du projet. Si les utilisateurs, les visiteurs ou \enquote{clients} (au sens web comme au sens mercantile) ne voient peu ou pas l'avantage de ce choix\footcite[p. 8]{crane_building_1996}, il a de fait permis de transformer facilement l'intégralité des données en HTML en un temps record.

Les années 1995 et suivantes ne sont que l'apparition du web comme média, et tout comme celle du CD-ROM, ne permettent d'abord que de faire émerger des outils assez simples. En 1995, l'interface de Perseus est identique à celle du CD-ROM, et donne accès aux données sans modification de l'interaction humain-machine. Il n'est pas facile de savoir quelle quantité de données visuelles a pu être portée dans ce passage sur le web, d'autant que, pour certaines, il est attesté que des problèmes de droits, cédés uniquement pour les versions CD, ont émergé très rapidement\footcite[p.~3]{crane_building_1996}. Cette interface est mise à jour en 1996 en même temps que la version 2.0 sur CD-ROM pour Mac (il faudra attendre 2000 pour une version CD compatible toutes plateformes)\footcite[p.~109]{rockwell_interface_2020}. D'après Rockwell et ses co-auteurs, le passage au web n'aurait pas influencé les ventes des CD-ROM. Nous pouvons attribuer -- hypothétiquement -- cette absence de fluctuation à quatre facteurs:
\begin{itemize}
    \item la relative habitude des bibliothèques d'acheter les CD-ROM et ouvrages mis à jour;
    \item le coût relativement faible du CD-ROM, comparativement aux autres \enquote{incunables};
    \item l'intérêt pour les ressources audiovisuelles qu'il contient et qui ne se trouvent pas sur le web;
    \item le possible élargissement de la base client via le produit d'appel -- involontaire -- que pouvait représenter le site web.
\end{itemize}
Perseus 2.0 est la dernière version CD-ROM de Perseus, contractuellement obligé envers l'université de Yale de produire cette version\footcite[p.~3]{crane_building_1996}. 

Contrairement au CD-ROM, le site web de Perseus permet une mise à jour en continu, incluant de nouveaux textes et de nouveaux domaines. Cet avantage se voit dès la version 2.0 Web, qui se démarque de la version CD-ROM en incluant des données hors champ des études grecques. En effet, dès 1996, Perseus rentre dans une nouvelle phase, celle de la fondation d'une \textit{digital library}: l'équipe de G.~Crane remporte un financement de la \acrshort{neh} pour un projet nommé \enquote{\textit{Digital Library on Ancient Roman Culture}\footnote{\textit{Grant ED-20456-96} avec un financement de 190.000\$ sur 3 ans, soit 334~729\$ octobre 2021.}}. Avec ce financement, qui annonce d'ailleurs l'inclusion de ses résultats sur CD-ROM sans que cela n'arrive jamais à notre connaissance\footnote{La description porte l'information suivante: \enquote{\textit{To support the development of a digital library on ancient Roman culture which will serve students of Latin and ancient Rome, and which will be published both on CD-ROM and via the World Wide Web.}}. \cite{noauthor_neh_nodate}}, Perseus étend son travail vers le latin. En 1996 aussi, l'équipe de Tufts obtient de son \textit{Tufts Provost Office and Arts\&Sciences Research funds} un financement pour inclure des œuvres de la renaissance anglaise (Marlowe, Shakespeare)\footcite{crane_perseus_1998}. Cette situation financière et cet élargissement placent Perseus comme pièce importante de l'espace internet consacré aux sources anciennes, avec des pics de 75~000 visites en vingt-quatre heures\footcite{crane_digital_1998}. En 1998, via un appel de la NEH et de la \acrfull{nsf} (\acrshort{nsf}), le deuxième financement le plus important de l'histoire de Perseus après celui qui le lança en 1987 lui fournit le budget nécessaire pour naviguer vers une version 3.0: l'équipe de G.~R.~Crane en collaboration avec Nancy Allen du \textit{Boston Museum of Fine Arts} et Ross Scaife de l'université du Kentucky obtient 2,8 millions de dollars\footnote{4~793~456\$ en dollars octobre 2021} pour un projet qui efface même l'héritage gréco-latin du projet dans son titre: \enquote{\textit{A Digital Library for the Humanities}}\footcite{crane_digital_1998}. Ce financement permet l'émergence d'une troisième version en 2000, uniquement web, et correspondant à la fin de la seconde vague de financements de Perseus (\textit{cf.} figure \ref{fig:chap1:perseus_fundings}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/chap1/part1/PerseusFinancements.png}
    \caption{Financements américains connus de Perseus, en millions de dollars non constants, d'après les pages \textit{Grants} de Perseus, le CV de G.~Crane et les archives de la Mellon Foundation et de la NEH. On distingue clairement trois vagues (1: fondation, 2: web, 3: expansion) et l'exportation de Perseus vers l'Allemagne pendant quelques années pour un retour après la fin des financements à partir de 2017.}
    \label{fig:chap1:perseus_fundings}
\end{figure}

Au milieu de la décennie 2000, la fin du financement \textit{A Digital Library for the Humanities} et un ensemble d'autres financements permettent la sortie d'une nouvelle version majeure\footcite{noauthor_gregory_nodate}, la 4.0. La 3.0 ayant subi de nombreuses modifications via les besoins émergents pour l'ensemble des financements de la seconde phase, son code n'est plus maintenable. La 4.0 est une remise à plat complète du projet Perseus avec un passage vers le langage Java pour le fonctionnement du site, le passage au XML \acrshort{tei} P4 pour ses ressources textuelles et la mise à disposition pour la première fois de ces dernières en téléchargement libre. Ce changement structurel du \textit{backend} s'opère en 2005, suivi d'une mise sous licence \textit{Creative Commons} de ses sources en 2006 et de son code en \textit{open-source} en 2007\footcite{rockwell_face_2013}. Cette troisième phase marquée par la 4.0 voit l'expansion de Perseus dans le domaine textuel se confirmer: à partir de 2000, aucun financement ne concerne la partie archéologique ou visuelle de Perseus, tandis que se développent une bibliothèque sur la guerre civile (2003), une extension pour les textes arabes (2006), le traitement des entités nommées (2007) ou de la grammaire grecque par \textit{treebank} (2008), etc. Les corpus ont grossi, toujours dans un objectif de mise à disposition des traductions\footnote{Avec comme seule source notre expérience pour le projet Perseus, des statistiques de 500~000 visites sur le site par semaine nous sont parvenues.}. En 2013, alors que G.~R.~Crane obtient la ``chaire Humboldt pour les Humanités Numériques'' à Leipzig, la troisième phase s'éteint, et peu de financements sont obtenus du côté américain, malgré une attache conservée à Tufts.
% Phase 2 de Perseus: Le latin et le reste, version 3.0 puis 4.0

Un phénomène nouveau accompagne cette apparition du web dans les foyers et bibliothèques: la naissance de corpus produits par des non-spécialistes, transformation numérique de ce que l'érudition locale et les sociétés savantes produisaient et produise en papier, avec la mise à disposition par l'effort de particuliers hors du monde académique de documents, données ou analyses scientifiques. Trois corpus existants encore en 2021 naissent sur la période 1995-1998: \textit{Curculio}\footcite{hendry_curculio_1995}, \textit{LacusCurtius}\footcite{lomarcan_lacuscurtius_1999} et \textit{The Latin Library}\footcite{carey_latin_1998} (1998). Les trois se concentrent en particulier sur la problématique des textes latins, et pour cause, ni Perseus ni \acrshort{phi} ne fournissent alors ces corpus. Ils sont rejoints par des projets francophones dans la décennie 2000, qui correspond à l'explosion de l'accès au web en France et en Europe: \textit{Remacle.org} arrive en 2003\footcite{philippe_remacle_site_2008}, \textit{Latin, Grec, Juxta} de Gérard Gréco en 2006\footcite{gerard_greco_latin_2006}. La particularité des sites francophones et de leurs fondateurs tient en leur carrière professionnelle: tous deux sont professeurs de lettres classiques de formation. Quelle que soit la situation professionnelle de ces créateurs de contenu, ils partagent tous la particularité de réaliser ces projets sans financement propre, en dehors des cadres universitaires, avec parfois une exhaustivité particulièrement importante, comme pour \textit{The Latin Library}, et avec une véritable focalisation sur le latin.

\subsubsection{Les projets nés sur le web}

La fin des années 90 et la première décade des années 2000 voient aussi l'émergence de projets nouveaux, cherchant à installer durablement dans le paysage numérique les lettres classiques. Si nous en parlons peu, car nous ne les utiliserons pas dans notre recherche, les premiers à se développer sur le web sont de loin les projets épigraphiques, dont plus d'une vingtaine est déjà répertoriée par Tom Elliott pour le compte de la société américaine d'épigraphie latine et grecque en juillet 1998\footcite{elliott_links_1998}. Mais les projets littéraires s intéressent aussi à la toile et y naissent.

Le premier mouvement de ces projets nés sur le web est celui de projets qui resteront au niveau du HTML: des sites pour lire des textes, donner accès à ces derniers avant tout. En 1996, par exemple, naît la \textit{Bibliotheca Augustana} d'Ulrich Harsch\footcite{harsch_bibliotheca_nodate}, dont il nous est malheureusement impossible de retrouver le contenu original. En 1998, c'est au tour d'\textit{Itinera Electronica} d'apparaître construite autour de deux axes: un ensemble de cours (sur quatre niveaux: acquisition, maîtrise, transmission et approfondissement) et de ressources textuelles dont la mise en ligne ne semble remonter qu'à 2002, d'après les journaux du site\footcite{meurant_itinera_nodate}. La \textit{Roman Law Library} sort en 2001: il est produit par des historiens du droit, hors du domaine des lettres classiques, fruit d'une collaboration internationale, et cherche à couvrir ``depuis les premiers textes de l'époque royale jusqu'aux compilations de la période byzantine''\footcite{lassard_roman_2001}. Les corpus naissent peu à peu aux États-Unis et en Europe, en latin comme pour les autres langues, tant que le catalogue est complexe à construire\footnote{D'autant que le passage du temps fait disparaître ces corpus et le peu de références faites dans les ouvrages ou articles scientifiques n'aident pas à en conserver la trace.}. Les outils de développement web sans apprentissage du code font leur apparition, les compétences intègrent les institutions peu à peu. En 1995 sort \textit{Vermeer Frontpage}, renommé \textit{Microsoft FrontPage} en 1996, qui permet le développement de site web sans compétences avancées en programmation, via une interface graphique. Dès 1997, on voit apparaître l'émergence de guides\footcite{la1997guide} à destination des non-spécialistes des communautés éducatives. Un projet de bibliothèque numérique de ressources slaves fait clairement mention de l'usage de \textit{FrontPage} dans son élaboration\footcite{deyrup1998character} et celle de son corpus, tandis que d'autres, tel le projet \textit{Journeys in Time 1809-1822} à l'université de Macquarie (Australie), rejettent son usage pour la production d'un code ``plus propre''\footcite[p.~41]{10.3316/informit.752609435027594}. Pour la plupart des premiers projets cités, ils survivent -- c'est ainsi qu'on les connait aujourd'hui -- et se sont enrichis, mais ne sont jamais sortis du contexte des sites web statiques, précompilés en HTML.

Or, la fin 90 et surtout le début 2000 voient des innovations majeures dans le monde du développement web et de la gestion de corpus électroniques. D'abord, les bases de données SQL, et notamment les serveurs MySQL, et le langage PHP voient le jour et dominent rapidement le monde du développement amateur tout en se faisant sa place dans le monde du développement professionnel\footnote{Malgré nos recherches, nous n'avons pas trouvé d'autres sources sur ce sujet que celles que nous citons. Et pourtant, le début des années 2000 voit l'émergence de sites à tutoriel comme celui du \textit{Site du zéro}, la réduction des prix pour l'hébergement de sites, la naissance (et la mort) des salons de discussions pour l'entraide qui favorisent clairement la formation en autodidacte d'une nouvelle génération de développeurs. C'est en tout cas notre expérience de ces années-ci.}. Associé aux serveurs Apache\footcite{smith_lamp_nodate}, facile à mettre en place pour les hébergeurs comme Free en France et d'autres, il devient facilement possible de déployer des sites dynamiques à bases de données relationnelles avec une formation rapide à la programmation. Des outils de publication (\acrshort{cms}, \acrlong{cms}) faciles à installer voient le jour et accompagnent ce mouvement technologique\footcite{purer_php_nodate}. Parmi ces applications plus complexes, on notera l'apparition entre autres de \textit{Musisque deoque}\footcite{gelderblom_musisque_2008}. Dans son compte-rendu, Gelderblom indique qu'il s'agirait du premier corpus -- il parle d'archive -- à intégrer les variantes et l'apparat critique\footnote{``\textit{The important innovation of MQDQ is that it is the first large-scale archive to include [apparatus] for a growing number of texts, and that it also provides effective search tools for them}'', \cite[p.233]{gelderblom_musisque_2008}}. D'autres projets similaires se développent, avec au centre de ceux-ci le développement de bases complexes, avec des technologies avancées comparativement à du pur HTML, comme le CGL\footcite{garcea_corpus_2010}. Ce dernier montre par ailleurs l'inconvénient de cette nouvelle couche de complexité: si \textit{MQDQ} est encore en ligne aujourd'hui, le \textit{Corpus Grammaticorum Latinorum} a complètement disparu -- bien qu'une nouvelle version soit prévue; l'hébergement de simples fichiers HTML et celui d'applications complètes ne posent pas les mêmes défis.

Ensuite, les années 2000 sont aussi celles de l'adoption par les \textit{guidelines} TEI du XML, d'abord avec la TEI P4 en 2001 puis avec la TEI P5 en 2007. La technologie prend de plus en plus de place dans plusieurs champ académiques, la liste des participants au meeting de 2003 montre par exemple cette belle diversité de domaines\footnote{\url{https://tei-c.org/Vault/MembersMeetings/2003-info/mm22.html}}. En 2007, une étape supplémentaire est passée: la portée de la réunion annuelle de la TEI change de forme, passant du nom \textit{annual members meeting} à celui d'\textit{annual conference}, et on ne publie plus la liste des participants à cette réunion\footcite{noauthor_members_nodate}. S'il n'a pas fallu ces changements organisationnels pour que la grammaire TEI soit une promesse attirante, ils en sont autant d'indice que les structures qui adoptent ou tenter d'adopter cette technologie. Dès 2001, T.~Nellhaus dresse le portait d'un outil pouvant révolutionner les libraires dans leur mise à disposition de corpus grâce à la standardisation qu'elle implique: pour l'auteur, sa souplesse et sa capacité de représenter des faits précis représentent une opportunité, bien qu'il ne soit pas sans défauts\footcite{nellhaus_xml_2001}. Dès 2002, l'\acrfull{ehess} (\acrshort{ehess}) via le laboratoire en médiévistique  \acrfull{ciham} (\acrshort{ciham}) à Lyon adopte la TEI pour l'édition de sermons et d'autres projets à travers les figures tutélaires de Marjorie Burghart et Nicole Dufournaud\footcite{burghart_edition_2011}. Dès 2002 aussi, l'\acrfull{enc} (\acrshort{enc}) adopte via sa cellule numérique le standard\footcite{poupeau_les_2006}. Chacune de ces institutions évoque la même raison: la TEI, à travers son encodage fin de phénomènes divers (linguistiques, historiques, littéraires, etc.), est un langage pivot permettant de nombreuses sorties et interprétations dont le HTML de lecture -- reproduisant presque les limites de l'imprimé augmenté des liens hypertextes -- n'est qu'une vue. C'est la même raison qui permet à G.~Crane de crier victoire quelques années plus tôt au sujet du passage de Perseus au web.

Dans le monde des lettres latines antiques, peu de projets adoptent dans un premier temps cette technologie, en dehors de Perseus qui avait parié dessus dès la fin des années 80.  D'une part, il existe le projet \textit{Hyperdonat} qui est à notre connaissance la première et seule édition scientifique d'une œuvre latine littéraire classique ou tardive raisonnablement longue\footnote{Il existe quelques extraits ici et là, ou quelques œuvres courtes comme le texte de Calpurnius dont nous parlons plus bas.} à utiliser le média web et des sources TEI\footcite{bureau2008hyperdonat}. L'usage de cette technologie est d'ailleurs justifié par Bruno Bureau comme le seul moyen d'éditer la base de données que représentent les commentaires de Donat, loin de l'édition d'un texte linéaire que serait celui d'un Victor Hugo par exemple\footcite[La comparaison est la nôtre.]{chaire_de_recherche_sur_les_ecritures_numeriques_exemple_2018}. D'autres tentatives existent cependant: mais même des initiatives aussi prometteuses que la \acrfull{ldlt} (\acrshort{ldlt}), cherchant à simplifier et promouvoir l'édition critique de textes en TEI, n'arrive à proposer qu'une seule édition de texte (Calpurnius) après des années de mise en place. Comme le note d'ailleurs Samuel J.~Huskey à propos de son projet, en 2019, ``les vraies éditions critiques sur internet sont encore rares''\footnote{``\textit{truly critical editions on the internet are still rare}'', \cite{huskey_digital_2019}}. De l'autre côté du spectre des projets en TEI, hors des objectifs d'éditions critiques, se trouve aussi le projet italien de la  \textit{Latin Digital Library of Late Antiquity} (DigilibLT)\footnote{\textit{Biblioteca digitale di testi latini tardoantichi}, d'où le LT.} dont l'objectif est de produire un nouveau corpus de textes tardifs, absents de Perseus, sans pour autant en proposer de nouveaux établissements de texte. À partir d'éditions imprimées globalement plus récentes que celles de Perseus, plutôt issues de la période 1950-2000, elle propose une collection cataloguée de textes sur la période du deuxième au huitième siècle, incluant des textes impossibles à trouver par ailleurs sous format numérique, au premier titre desquels on trouve les traités de médecine et de gynécologie. Le \textit{DigilibLT}\footcite{lana_metodologie_2012} prend par ailleurs le même chemin que celui du projet \textit{Perseus} en affirmant l'importance du caractère \textit{open access} et libre de son projet, dont le corpus est téléchargeable dès sa fondation\footnote{Dans son article, Maurizio Lana titre une de ses parties ``\textit{Accesso aperto, licenze Creative Commons, software libero}'' (fr. Accès ouvert, licence Creative Commons, logiciel libre). \cite{lana_metodologie_2012}}. Mais le monde de la TEI latine classique et tardive s'arrête là, du moins pour les œuvres ``littéraires'' (on inclut les traités de médecine): l'épigraphie a -- elle -- bien adopté l'usage de la TEI et de sa variante Epidoc\footcite{elliott2007epidoc} pour la publication de ces corpus de textes\footcite{bodard2007inscriptions,cayless2010epigraphy}.

\subsubsection{OCR et corpus de masse}

En 2011 et 2012, David Bamman, avec G.~Crane\footcite{Bamman:2011:MHW:1998076.1998078} puis avec David Smith,\footcite{bamman_extracting_2012} s'intéresse pour la première fois à un corpus en friche: celui des campagnes de numérisation, majoritairement privées dans son cas précis, et du résultat de la reconnaissance optique de caractères (\acrshort{ocr}) issue de ces documents. Il dénombre, en 2012, 27~014 textes catalogués comme latins dans l'index du projet \textit{Internet Archive}, comprenant tout autant les classiques latins que les thèses et ``commentaires de la philosophie d'Hegel'' produits en Allemagne, mais écrits en latin au dix-neuvième siècle\footcite{bamman_extracting_2012}. Puis, en triant les données, y compris manuellement\footcite{bamman_dbammanlatintexts_2018}, il obtient une liste de plus de onze mille ouvrages comprenant environ 1,38 milliard de mots\footnote{Ce chiffre est à prendre avec une certaine précaution: la méthode de calcul des mots n'est pas précisée et la définition de ce qu'est un mots ici non plus.}. À partir de cette récupération de données, il met à disposition les premiers \textit{embeddings} massifs de l'histoire du latin approché par méthode computationnelle. Cependant, ses données sont problématiques: d'une part, la méthode de vérification manuelle du catalogue n'est pas expliquée; d'autre part, le résultat final contient des documents dont l'\acrshort{ocr} est absolument inexploitable (``\texttt{tkei: SiiimiemfiBgMiffem mvemsfimUttrUffHk rejcijps}'' étant un des exemples de mauvaise texte qu'il cite lui-même).

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/chap1/part1/ocrSycophant.png}
    \caption{Récapitulatif de la chaîne de traitement appliquée sur Archive.org}
    \label{fig:chap1:workflow-sycophant}
\end{figure}

La littérature classique commence à être bien couverte: il reste quelques textes impossibles à trouver édités numériquement, par exemple les \textit{Declamatio Minores} de Quintilien ou les auteurs fragmentaires, en dehors des dépôts protégés comme ceux du PHI, mais cela reste marginal. Du côté de la littérature tardive, entre DigilibLT, les \textit{Patrologia Latina} et le CSEL, la couverture tend à être meilleure, même si de nombreux textes ne sont pas édités ou ne sont tout simplement pas encore tombés clairement dans le domaine public\footnote{La question du droit d'auteur sur les éditions de textes anciens est complexe, spécifique à chaque territoire, y compris en Europe, et pose de vrais problèmes d'interprétations, dont certains producteurs de corpus se plaignent, comme Philipp Roelli dans \cite{roelli2014corpus}. Sur le plan national, de nombreuses publications existent, comme \cite{combalbert_lediteur_2015, demonet_confiscation_2018}. Sur le plan international, et celui des textes latins \cite{fischer2017digital, dillen_digital_2016}. Sur des textes plus récents, \cite{dusollier_international_2019}}. Au contraire, la littérature latine médiévale et le néo-latin sont particulièrement absents des corpus en ligne, et leur mise à disposition tarde. Dans ce contexte, le recours à ces dépôts massifs, pour élaborer de nouveaux outils ou offrir de nouvelles approches.

L'OCR ayant progressé depuis 2013, et la méthode n'étant pas totalement claire dans l'approche de D.~Bamman, nous nous sommes donc intéressés à l'évolution du dépôt \textit{Internet Archive}. Pour faire cela, nous avons traité les données de cette bibliothèque numérique en cinq étapes (reproduites en figure \ref{fig:chap1:workflow-sycophant}). Après avoir obtenu une liste d'œuvres classées comme étant en latin par l'\textit{Internet Archive}, nous avons téléchargé l'ensemble des œuvres cataloguées. Une fois téléchargées, on prend au hasard un quart des lignes de chacune des œuvres et on applique un classificateur de langue\footcite{salcianu2018compact}. Ce classificateur indique des statistiques pour plusieurs langues s'il a des doutes sur cette classification: on retient alors qu'une ligne est classée comme latine si elle a un score supérieur à 60\% en latin\footnote{Ce seuil a été fixé pour prendre en compte l'existence d'œuvres bilingues.}. Après ce traitement, on applique un outil \textit{ad hoc}, OCR~Sycophant\footcite{Clerice_OCR_Sycophant_2021}, qui classe les lignes en termes de qualité: lisible ou non lisible. OCR~Sycophant a été développé autour de modèles de classification classiques basés sur des n-grams, et a été entraîné à partir d'un dataset de phrases du corpus Archive.org sélectionnées aléatoirement et annotées à la main: ont été classées comme ``sales'' les données qui n'étaient pas en latin (\textit{eg.} ``\texttt{" Mr Bryce's test, on account of the difficulty of pro-}''), qui étaient illisibles (eg. ``\texttt{"7 „ 7- f Ak. —2 vi rt*- ('wbrf-}'') ou difficilement lisibles (\textit{eg.} ``\texttt{(Hciucuto qucfo quob fwutuinlto}''), ou encore qui correspondaient à des lignes considérées comme trop courtes comme du bruit (\textit{eg.} ``\texttt{5}''). Ce classement disponible, on obtient un pourcentage de lignes estimées propres et on compte les mots de chaque texte (un mot étant considéré simplement comme un élément séparé par un espace).

Ce résultat donne des chiffres absolument prometteurs, tout en demandant une forme de patience. On compte dans les documents disponibles presque 5~000 œuvres avec une qualité d'OCR à plus de 90\%(\textit{cf.} table \ref{tab:chap1:latin-OCR}) représentant 635 millions de mots. Si l'on ajoute à ce corpus les textes avec plus de 80\% de qualité OCR, on atteint les 1,451 milliard de mots et un peu plus de 17~000 ouvrages. Bien sûr, il existe probablement des doublons dans ces œuvres. Mais un vrai foisonnement de données existe dans ces dépôts: avec ces deux catégories de textes propres, on atteint près de dix fois plus de mots que les œuvres contenues dans \textit{Corpus Corporum}, et en ne prenant en compte que la bibliothèque \textit{Internet Archive}.

\begin{table}[ht]
\centering
\begin{tabular}{l|rrr}
\toprule
                               & Nombre de volumes & \% du total & Nombre de mots \\ \midrule
Qualité \textgreater 90\% OCR  & 4 946              & 23.73       & 635 201 534    \\
Qualité \textgreater 80\% OCR  & 12 169             & 58.38       & 816 236 079      \\
Qualité \textgreater 60 \% OCR & 3 709              & 17.79       & 182 697 928      \\
Reste                          & 19                 & 0.09        & N/A           \\ \bottomrule
\end{tabular}
\caption{Statistiques sur les ouvrages latins disponibles sur Archive.org au début juillet 2021}
\label{tab:chap1:latin-OCR}
\end{table}

La question de l'OCR et de sa qualité (surtout pour les œuvres imprimées avant la fin du dix-neuvième siècle), deviennent donc prépondérantes dans le contexte de l'acquisition de textes fac-similaires (et donc non édités). Le problème de la reconnaissance de texte pour les œuvres de l'époque moderne, avec ses S longs et ses restes d'abréviations, des incunables est peu à peu réglé par la mise à disposition massive de données d'entraînements et de modèles adaptés. Dans ce cadre, le travail de Simon Gabay autour des imprimés du 17e siècle est absolument majeur et fondateur\footcite{simon_gabay_2020_3826894}: son usage a permis de générer de très bons modèles OCR\footcite{gabay:hal-02577236} et, à partir de quelques données latines\footcite{Clerice_CREMMA_16_18_Prints_2021}, permet de produire des données propres de textes latins imprimés avant le 19e siècle, de l'\textit{Utopie} de Thomas More à des œuvres en zoologie du 18e siècle en passant par l'\textit{Historia de duobus amantibus Euralio et Lucretia}, avec des taux de reconnaissance à plus de 96\%. 

Mais un autre enjeu pour la mise à disposition de texte arrive aussi à travers la Reconnaissance d'Écriture Manuscrite (REM, ou plus communément l'anglais HTR pour \textit{Handwritten Text Recognition}). La transcription automatique des documents de la pratique et des manuscrits littéraires, quel que soit leur genre, apportera une autre masse de données pour l'étude du latin sur le très long terme. Le développement de cette technologie, sa popularisation par le biais de Transkribus\footcite{kahle2017transkribus}, sa mise à disposition \textit{open source} par Ben Kiessling via Kraken\footcite{kiessling2019kraken} puis eScriptorium\footcite{kiessling_escripto}, ont permis l'émergence de modèles partagés extrêmement performants. Les travaux d'Ariane Pinche sur les manuscrits en ancien français\footcite{Pinche_CREMMA_Medieval_an_2021} avec une reconnaissance des caractères supérieur à 95\% ou les travaux de Dominique Stutzmann\footcite{hazem2020books} ont montré que cette approche était prometteuse et pouvait potentiellement passer à l'échelle. Il est probable que l'étude quantitative du latin soit largement redéfinie par la mise à disposition de ces corpus nouveaux sur le moyen terme, en s'attaquant de front aux dépôts institutionnels nationaux ou régionaux, comme la Bibliothèque nationale de France et son dépôt Gallica. Dans ce cadre, des projets comme le Gallicorpora, dont font d'ailleurs partie S.~Gabay et A.~Pinche, montreront rapidement ce à quoi l'on peut s'attendre sur le court terme, avec le développement d'une chaîne de traitement pour la production de documents fac-similaires encodés finement en TEI.

Si l'approche évoquée précédemment est celle d'une approche massive, bruitée et sans réelle intervention humaine, une approche qualitative des corpus en friche est aussi possible. La révolution de la qualité des données obtenues via OCR a permis aussi de développer de nouveaux projets, comme le projet VELUM dirigé entre autres par Bruno Bon pour la mise en place d'un corpus médiéval latin de texte OCRisés\footcite{bon2019challenges}. Du côté des périodes classiques et tardives, il faut alors se tourner vers l'initiative \acrfull{ogl} (\acrshort{ogl}) menée par G.~Crane depuis l'université de Leipzig puis de Tufts.

Idée née entre 2008 et 2009, le projet \acrshort{ogl} est le fruit d'un besoin ressenti par des enseignants et chercheurs en lettres classiques associés au \acrfull{chs} (\acrshort{chs}) d'Harvard\footnote{Les locaux de ce dernier sont par ailleurs complètement distincts de ceux d'Harvard, au point d'être dans deux États et deux villes différentes: Cambridge, Massachusetts et Washington DC}, Neels Smith et Christopher Blackwell\footcite{muellner2019free}. Ce projet a pour objectif dès le départ de produire un corpus pour le grec qui soit intégralement \textit{open source}, gratuit et accessible, et qu'il soit standardisé afin de pouvoir s'assurer de la collaboration et de la réutilisation par des partenaires divers. Il passe d'abord par une tentative de partenariat avec le TLG d'ouvrir leur collection, qui restera lettre morte. Ce refus se traduit en 2010-2011 par une première tentative de compilation d'un nouveau jeu de données, mais il faut attendre 2015-2016 pour que le projet prenne forme à part entière.

Neel Smith est en effet un proche ami de Gregory Crane, ils ont partagé les bancs de Harvard, ont travaillé sur les premières moutures de Perseus en équipe et ont partagé le même directeur de thèse, Gregory Nagy, directeur depuis 2000 du CHS. Or, en 2013, Gregory Crane obtient donc la \textit{Digital Humanities Chair} à l'université de Leipzig, où il a pour objectif de relancer le projet Perseus. Dans ce cadre, ses premières productions sont simples: il faut agrandir le corpus gréco-latin, en particulier sur le premier millénaire de notre ère, et ajouter un grand nombre de traductions. Les pères de l'Église et l'ensemble de la littérature tardive restent inaccessibles à ce moment précis en accès libre. Pour le latin, l'OCR a fait les progrès qui permettent à l'équipe de Perseus de mettre en place de nouveaux corpus, dont les deux premiers sont le \acrfull{csel} (\acrshort{csel})\footcite{noauthor_csel_nodate} et la \acrfull{pl} (\acrshort{pl}) de Migne. Si la \acrshort{pl} a été introduite précédemment et existait dans des versions concurrentes, le CSEL est quant à lui un corpus d'éditions critiques des pères latins de l'Église. Née en 1864, cette initiative autrichienne de 1864 toujours en activité à travers l'université de Salzburg\footcite{noauthor_history_nodate} a publié depuis sa fondation plus d'une centaine de volumes comprenant potentiellement plusieurs œuvres, comme le volume 10 constitué des œuvres complètes de Sidulius (IXe siècle), et dont une partie est tombée, avec son apparat critique, dans le domaine public.

Au niveau technique, ces corpus sont transcrits automatiquement, puis corrigés et structurés en XML TEI par des entreprises spécialisées, dont l'entreprise française Jouve et sa succursale malgache. Leur XML est ensuite adapté aux attentes de Perseus par des équipes internes puis mis à disposition sur Github. C'est ce même fonctionnement qui est repris lors de la mise en place de l'alliance entre le CHS, l'université de Mount Alison et les équipes de G.~Crane. Dans l'article de L.~Muellner, on apprend que l'OCR est réalisée par les équipes de Mount Alison, sous l'égide de Bruce Robertson qui entretient des modèles pour l'OCR grecque. Puis, les données sont envoyées à l'entreprise plurinationale \textit{Digital Divide Data} (DDD; Cambodge, Kenya, Indonésie) pour leur mise en Epidoc, le format choisi par Perseus. On y apprend que le coût budgété de numérisation et d'encodage par DDD est de 50~000\$ pour quatre millions de mots, soit bien moins que les premiers projets des années 70 et 80. Enfin, la mise en conformité et la vérification des données sont assurées par des stagiaires, étudiants de la licence au doctorat, hébergés d'abord uniquement par le CHS puis par l'université de Virginie\footcite{robertson2019optical}.

La production de l'ensemble de ces corpus a permis ensuite à Perseus de se tourner vers une nouvelle version, en partie financée par le CHS, Perseus 5\footnote{\url{https://scaife.perseus.org/}}, dont la mise à jour est automatiquement liée à l'évolution des corpus, contrairement à la version 4, et qui a été l'objet d'une refonte totale de l'infrastructure de Perseus. Cette version a abandonné -- pour le moment -- les données graphiques (archéologiques, histoire de l'art, etc.) pour ne s'intéresser qu'aux données textuelles.

Enfin, avec l'apparition de tous ces projets se pose la question de l'éclatement des corpus sur internet. Entre les données de Perseus, de DigilibLT, d'autres projets comme le \textit{Corpus Grammaticorum Latinorum} de Jussieu (CGL), l'accès à un corpus latin unifié devient problématique. C'est un problème d'autant plus important que ces sites ne partagent pas une architecture commune qui pourrait alors permettre de centraliser les recherches sur de multiples corpus. En 2011, Philipp Roelli, un éditeur de texte aussi intéressé par la linguistique de corpus, met en route le projet \textit{Corpus Corporum} à l'université de Zurich\footcite{roelli2014corpus}. Ce projet est très peu financé en dehors d'une aide de la chaire de latin et d'une partie des fonds de la COST-Action IS1005 dont l'objectif principal était la formalisation d'un réseau de recherche autour du latin médiéval\footnote{\textit{Corpus Corporum} est donc plus une externalité positive de cette dernière que l'un de ses objectifs.}. P.~Roelli le définit comme une ``meta-collection'': \textit{Corpus Corporum} ne produit pas de numérisation ou d'édition numérique, il centralise, en harmonisant, les données d'une dizaine (en 2014) puis d'une trentaine d'autres projets (en 2021), accumulant ainsi en un seul endroit presque 164 millions de tokens à la fin 2021, s'étalant du latin classique au néo-latin. Bien que techniquement ``rustique'' du côté client\footnote{L'usage des \textit{iframes} a presque complètement disparu du web, hormis sur \textit{Corpus Corporum}.}, le projet a l'avantage d'être rapide, facile d'usage -- à l'image de \textit{The Latin Library} -- et de permettre l'accès à des corpus perdus comme les CGL, indisponibles sur leur site d'origine depuis quelques années.

Après l'avènement du micro-ordinateur et du web, les corpus latins ont ainsi évolué pour atteindre aujourd'hui une ouverture sans commune mesure. Pour les plus grands classiques, il est possible d'en trouver des éditions voire des traductions -- en anglais majoritairement -- assez facilement en plein texte. Et quand cela n'est pas possible, il peut toujours être fait recours aux dépôts institutionnels ou privés tels qu'Archive.org ou HathiTrust aux États-Unis afin d'obtenir la numérisation d'un de ces ouvrages. Cette révolution, sur presque cinquante ans, est celle de l'accès aux textes latins classiques et tardifs dans leur intégralité -- ou presque -- et de manière gratuite, et permet de produire de nouveaux questionnements, de nouvelles approches.

% Méta-corpus
%% Corpus Corporum

% Côté éditorial
% \subsubsection{Le renouveau \textit{Open Greek And Latin} et l’apport de l’OCR de masse}
% Approche Github 
% Approche API ?

% Côté non-éditorial non universitaire et l'hors-classique un peu aussi
% \subsubsection{Les corpus en jachères}
%Archive.org et institutions patrimoniales qui OCRisent
% Réfléxion autour de l'OCR de Archive.org, statistiques obtenues quand on a fait des fouilles

\section{Constitution d'un corpus de sources littéraires latines}

%\begin{quote}[\enquote{The \textit{Corpus Corporum}, a new open open Latin text repository and tool}]{Philipp Roelli}
%    \textit{The idea was born from my linguistic research to create an open and non-commercial Latin text meta-collection.} \\
%    \enquote{L'idée est née de mes recherches linguistiques: créer une méta-collection ouverte et non-commerciale de textes latins.}
%\end{quote}

Avec l'histoire des corpus, il est clair que le nombre de corpus disponibles, ouverts, fermés ou à réutilisation limitée est assez important pour produire une recherche plein texte ou la compilation d'extrait. Le travail de P.~Roelli montre que, en produisant des corpus ouverts et ré-exploitables, les projets comme \textit{Perseus} et \textit{DigilibLT} ont constitué des gisements de textes analysables, indexables et réutilisables dans un contexte autre que celui d'une lecture linéaire et ``manuelle''. Pour notre recherche, nous aurons besoin de deux corpus: un premier permettant d'effectuer des recherches plein texte, d'extraire des exemples; un second constitué uniquement d'exemples d'isotopies sexuelles.

Dans un premier temps, il faut mettre en place le corpus de recherche plein texte. Beaucoup d'options s'offrent à nous, de l'utilisation d'outils commerciaux comme la \acrshort{llt} à celles de corpus en friche de l'\textit{Internet Archive}: la mise en place de critères éthiques et techniques devront donc précéder la compilation ou réutilisation d'un tel corpus. Nous étudierons ensuite l'importance de l'encodage fin de textes et de l'impact que son manque peut avoir sur la production de savoir via des analyses statistiques. Enfin, nous discuterons de la compilation du corpus effectuée à partir de l'œuvre scientifique de J.~N.~Adams, des limites de cette dernière, mais aussi des méthodes employées pour compiler cette collection.

\subsection{Objectifs et prérequis d'un corpus pour une analyse automatisée}

\begin{quote}{J.-B.~Camps}
\enquote{La distinction entre humanités « numériques » et « computationnelles » est dans l’air. Au‑delà d’un pur choix terminologique distinctif ou d’un retour aux \textit{humanities computing} du XXe siècle, la revendication d’une dimension computationnelle rend compte d’un basculement, à mon sens éminemment souhaitable, d’une perspective tournée vers la diffusion et la publication électronique, à un accent mis sur les données et leur exploitation pour la création de nouveaux savoirs scientifiques.}\footnotemark
\end{quote}
\footnotetext{\textcite{camps_ou_2018}}

Le constat de Jean-Baptiste Camps est juste: depuis les années 2000 et 2010 en particulier, la technicisation de l'analyse des documents et des sources, à travers la stylométrie par exemple, et le besoin d'une reconnaissance à part de cette technicisation a donné lieu à de nouvelles sous-communautés des humanités numériques, avec leurs réseaux parallèles de conférences. En juillet 2019, à la suite de la conférence annuelle de l'association internationale principale des humanités numériques\footnote{Cette association, l'\acrshort{adho}, organise à ce moment-là DH2019 à Utrecht.}, un groupe d'intervenants décide à trouver une solution à cette frustration. Quelques semaines plus tard, un appel à avis est publié, et son phrasé est clairement revendicatif: \enquote{Malgré l'essor indéniable de ce nouveau domaine de recherche [les humanités computationnelles], de nombreux chercheurs estiment qu'il n'existe pas d'endroit approprié, axé sur la recherche, pour présenter et publier leurs travaux informatiques sans perdre de vue les questions relatives aux sciences humaines\footnote{\enquote{\textit{And yet, despite the undeniable growth of this new research area, many scholars still feel that there is no suitable research-oriented venue to present and publish their computational work that does not lose sight of questions relevant to the humanities.}}\textcite{noauthor_computational_nodate}}.}. Suite à cet appel de ce qui s'appelle alors la CoHuRe (Computational Humanities Research, plus tard CHR), on voit l'émergence de \enquote{contre conférences DH} avec la CHR 2020 puis 2021, ou encore des lieux encore plus spécialisés comme NLP4H\footcite{noauthor_workshop_nodate}. Si la reconnaissance du travail d'analyse technique est importante, nous différons sur l'apparente exclusion mutuelle qu'opèrerait ce \enquote{nouveau} paradigme de la recherche numérique. Dans un champ éminément protéiforme comme les humanités numériques, une sorte de science auxiliaire des sciences humaines, il faut trouver un équilibrage, un balancement, plus qu'un \enquote{basculement}, entre le travail de génération de données, fruits d'un travail de recherche, et celui d'analyses. La constitution de corpus, la collection de textes, leur contrôle qualité, voire leur édition sont autant de missions qu'il ne faut pas négliger dans une approche plus ``computationnelle'' des sciences humaines, sans quoi les humanités computationnelles ne sont plus des humanités, mais de l'informatique appliquée.

\subsubsection{Le choix d’un corpus open source: Traçabilité des textes, textes et reproductibilité}

Sans redéfinir ce qu'est un corpus, il est intéressant de s'arrêter un temps aux définitions aux deux dictionnaires français les plus vendus, \textit{Le Robert} et le \textit{Larousse}, pour questionner les traits définissant ce que nous voulions produire ou réutiliser. \textit{Le Robert} définit le corpus comme un ``Ensemble fini de textes choisis comme base d'une étude.'' tandis que le \textit{Larousse} prend -- par un heureux hasard -- l'exemple des corpus grecs pour appuyer sa première définition: ``Recueil de documents relatifs à une discipline, réunis en vue de leur conservation : Corpus des inscriptions grecques.'' et rejoint légèrement le \textit{Robert} pour sa seconde (``Ensemble fini d'énoncés écrits ou enregistrés, constitué en vue de leur analyse linguistique.''). Si les deux dictionnaires se rejoignent sur un point, le corpus est une collection de documents, potentiellement de textes, ils évoquent deux finalités différentes. 

La première, la conservation -- sous-entendue la maintenabilité et l'accessibilité en un même endroit, physique ou numérique, d'un ensemble documentaire -- n'est mentionnée comme définissant que par le \textit{Larousse}. Dans son article de 2013, Alex H.~Poole fait le constat tour à tour que ``les humanités numériques pivotent autour des données''\footnote{``\textit{The digital humanities pivot around data.}''\cite{poole_now_2013}} mais aussi que ces dernières, au format numérique, étaient ``notoirement fragiles, d'une courte espérance de vie, et facile à manipuler sans laisser forcément de traces, rendant la fraude difficile à détecter [..., sachant que] la plupart des données collectées n'étaient ni organisées ni publiées''\footnote{``\textit{Our Cultural Commonwealth} report characterized digital data as “notoriously fragile, short-lived, and easy to manipulate without leaving obvious evidence of fraud”. Worse, much collected data were neither curated nor published whatsoever;'', \cite{poole_now_2013} citant \cite{unsworth2006our}}. Nous partageons ce constat, cette importance de la conservation pour que corpus existe, et établissons ce point comme premier objectif autour de notre corpus. 

La seconde finalité évoquée est celle de l'analyse (``la base d'une étude'', ``en vue d'une analyse linguistique''). Si nous estimons que cette finalité peut être déplacée (le corpus peut être compilé pour qu'une tierce personne s'en empare), elle est bien évidemment centrale dans notre projet. Et elle demande ainsi de définir l'objectif de notre corpus, car celui-ci définira la forme, l'outillage et les informations nécessaires à y retrouver. Nous reviendrons plus tard sur l'impact qu'a cette finalité sur le corpus, dans son annotation et sa documentation(\textit{cf.} \ref{chap1:method-annotation}).

Il faut cependant ajouter à la notion de corpus un autre point: celui de son ouverture, en droit et en accès. A.~H.~Poole le mentionne partiellement dans la citation avec la question de la ``fraude'', mais la citation de Borgman\footcite{borgman2012conundrum}, reprise par J.-B.~Camps\footcite{camps_ou_2018} est à notre sens assez complète. Un corpus doit être ouvert pour
\begin{enumerate}
    \item reproduire ou vérifier la recherche,
    \item rendre les résultats d'une recherche publique disponible pour le public,
    \item rendre la possibilité à d'autres de poser de nouvelles questions aux données
    \item avancer l'état de la recherche et de l'innovation.\footnote{``(1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation.''\footnote{\cite{borgman2012conundrum} chez \cite{camps_ou_2018}}.}
\end{enumerate}

La question de la reproductibilité, par l'ouverture du corpus et sa documentation, est centrale pour Borgman, Poole et Camps. Si la traçabilité des sources n'est pas une nouveauté pour les lettres et l'histoire -- la citation de ces dernières est extrêmement codifiée afin d'être compréhensible et exhaustive, la transcription des sources de la pratique souhaitée pour les publications --, la reproductibilité des expériences est définitivement nouvelle. D'abord, car la notion d'expérience en lettres comme en histoire est nouvelle, bien qu'elle ne le soit pas nécessairement partout dans les sciences humaines. Ensuite, car la notion de reproductibilité est tout autant complexe dans le monde des sciences dites dures. Comme le dit J.-B.~Camps, ``au fur et à mesure que l’analyse de données prend de l’importance dans la constitution de nouveaux savoirs, le besoin se fait plus criant de vérifier l’intégrité des données, de reproduire les expériences, de vérifier ou infirmer les énoncés qui en découlent.''\footcite{camps_ou_2018}. L'arrivée de ces questionnements scientifiques et la "crise de la reproductibilité" en 2000, que mention J.-B.~Camps est suivi peu à peu par une crise en intelligence artificielle\footcite{hutson2018artificial}, traitement automatique des langues\footcite{belz2021systematic} et en histoire\footcite{eijnatten_big_2013}.

Un autre avantage, en partie lié à la reproductibilité, des corpus ouverts est celui de son analyse et en particulier de ses biais. En accumulant des données dont on essaie de tirer des analyses, des conclusions et même simplement des faisceaux d'indices, la possibilité d'introduire, inconsciemment, des biais de corpus et -- à travers eux -- d'établir des conclusions invalides est un danger éminemment connecté aux corpus fermés. Les conséquences peuvent être importantes dans le domaine de l'intelligence artificielle, le \textit{machine learning} ne pouvant qu'apprendre ce qu'on lui montre. L'exemple le plus connu des dernières années est celui de la reconnaissance d'image de Google, qui, en 2019, avait tout simplement catégorisé des personnes afro-américaines comme gorilles\footcite{lohr2018facial, chokshi2019facial}. Si les conséquences pour notre corpus ne pouvaient être aussi graves et socialement problématiques, il reste que la question du biais est à prendre en compte. Il ne s'agit pas de promettre l'exhaustivité ni la représentativité: le domaine des lettres classiques a depuis longtemps admis la partialité -- dans les deux sens -- de ses sources ainsi que les pertes de nombreuses autres sources. Sur ce sujet, nous reprendrons l'exemple de l'article de I.~D.~Raji \textit{et al.}\footcite{raji2021ai}:

\begin{quote}{\cite{raji2021ai}}
    \enquote{Dans le livre d'histoires pour enfants \textit{Sesame Street}, ``Grover and the Everything in the Whole Wide World Museum''[Stiles et Wilcox, 1974], le monstre \textit{Muppet Grover} visite un musée qui prétend présenter "tout ce qui existe dans le monde entier". Des exemples d'objets représentant certaines catégories remplissent chaque pièce. Plusieurs catégories sont arbitraires et subjectives, notamment les salles d'exposition des "choses que l'on trouve sur un mur" et de "la salle des choses qui peuvent vous chatouiller". Certaines sont étrangement spécifiques, comme "La salle des carottes", tandis que d'autres sont inutilement vagues comme "La grande salle". Alors qu'il pense avoir vu tout ce qu'il y a, Grover arrive à une porte intitulée "Tout le reste". Il ouvre la porte et se retrouve dans le monde extérieur.\footnote{\textit{``In the 1974 Sesame Street children’s storybook Grover and the Everything in the Whole Wide World Museum [Stiles and Wilcox, 1974], the Muppet monster Grover visits a museum claiming to showcase “everything in the whole wide world”. Example objects representing certain categories fill each room. Several categories are arbitrary and subjective, including showrooms for “Things You Find On a Wall” and “The Things that Can Tickle You Room”. Some are oddly specific, such as “The Carrot Room”, while others unhelpfully vague like “The Tall Hall”. When he thinks that he has seen all that is there, Grover comes to a door that is labeled “Everything Else”. He opens the door, only to find himself in the outside world.''}}}
\end{quote}

Tout comme l'idée d'un musée du monde est absurde, l'absence de biais dans un corpus l'est tout autant. Mais la possibilité de les décrire et de les vérifier à travers un corpus ouvert est primordiale pour la critique des résultats.

Nous ajouterons cependant une dernière possibilité derrière l'ouverture de ces données, particulière à leur dimension numérique: l'\textit{open access} et l'\textit{open source} dans ce contexte permet aussi la croissance et la modification des données sur le long terme, ne figeant pas le corpus en un instant T (bien qu'il soit important de pouvoir revenir à ce dernier pour la reproductibilité). Le corpus de notre recherche doit non seulement survivre à sa publication, mais aussi se corriger, s'arranger: il serait probablement présomptueux de le croire exhaustif, sans erreurs, et d'autres seront -- nous l'espérons -- intéressés par sa correction ou son extension à d'autres textes, d'autres périodes.

Enfin, le corpus doit être \textit{sourçable}, \textit{traçable}. La première raison derrière cet objectif est celle de la \textit{vérifiabilité} et de la possibilité de recompiler ce même corpus au besoin, pour les mêmes raisons qui nous poussent à citer, de manière enrichie et à l'envie, toute bibliographie. Ensuite, car les recherches que nous avons compilées sur l'histoire des corpus a montré une capacité hors norme pour notre champ à ignorer le travail de production que ces projets ont réalisé, réalisant alors un silence parfois dommageable pour les carrières de nombreux chercheurs et chercheuses. Enfin, car cette traçabilité, en dehors d'un respect de la paternité de la numérisation ou de l'établissement du texte, assure d'intégrer la naissance de notre corpus à un point de la vie de ces derniers, y compris avec leurs fautes, leurs manques.

\subsubsection{Un corpus \textit{machine actionable} ?}
\label{chap1:method-annotation}

% De la question de traçabilité découle la question  Capitains
% Citabilité, manipulabilité, compatibilité: XML-TEI et Capitains ? (ou dans le .2 ?)
% Le machine actionnable / readable est pas mentionné au final.?
D'après ces considérations, notre corpus doit donc être: (1)~traçable, en termes de paternité au minimum, (2)~ouvert et libre, tant dans son accès que dans sa réutilisation (3)~conservable, car la recherche ne pourrait contredire ou augmenter les propos ici tenus en s'appuyant sur des données perdues.

Il reste une question à ouvrir, qui est celle du format, et de son rôle dans notre recherche. En ignorant les formats inappropriés à la conservation et à la complexité des textes tels que le \acrshort{csv}, on peut se concentrer sur deux formats, qui répondent à des objectifs et contraintes différents: le format plein texte, communément suffixé par un \texttt{.txt}, et un format XML suivant les \textit{guidelines} TEI. Si les deux formats sont \textit{machine readable}, c'est-à-dire lisibles pour la machine, ils diffèrent dans leur habilité à être \textit{machine actionable}. La \textit{Data Documentation Initiative}, un \enquote{standard pour la description des données produites par les enquêtes et autres méthodes d'observation dans le domaine des sciences sociales, comportementales, économiques et de la santé.}, définit la \enquote{\textit{machine actionability}} comme le fait de produire des \enquote{informations structurées de manière cohérente afin que des machines, des ordinateurs, puissent être programmés en fonction de cette structure\footnote{\enquote{\textit{This term refers to information that is structured in a consistent way so that machines, or computers, can be programmed against the structure. DDI provides machine-actionable metadata.}}\textcite{noauthor_machine_actionable_nodate}}}. D'après Martin Mueller, ancien directeur du \textit{board} du consortium TEI, un texte peut être considéré comme intégralement \textit{machine actionable} si \enquote{c'est une structure de données dans laquelle le texte en tant que séquence de mots est complété par un catalogue de ses parties qui sont enrichies de diverses manières. Ces données supplémentaires, souvent appelées \textit{métadonnées}, peuvent être récupérées en fonction de ces enrichissement et \textit{sans attente}}\footnote{\enquote{\textit{The fully machine actionable text is a data structure in which the text as a sequence of words is supplemented by an inventory of its parts that are classified in various ways. These supplementary data, often called metadata, can be retrieved by those classifications and in a “just in time” manner.}}, \textcite{mueller_shakespeare_2014}}. Or, seule la TEI, parmi les deux formats cités, peut être considérée -- au moins en partie -- comme \textit{machine actionable}, car elle peut intégrer des métadonnées à chaque niveau du texte, celui de l'œuvre, des passages et des mots, d'une manière cohérente et surtout standardisée.

Le format XML-TEI est de fait le modèle le plus approprié ici pour constituer ce corpus. Il a montré sa maintenabilité: le corpus \textit{Perseus} a environ trente ans. Il a prouvé sa réutilisabilité et sa flexibilité: si \textit{Perseus} est passé au web rapidement, \textit{Corpus Corporum} a réexploité son intégralité tout aussi facilement. Il permet une description très fine des données, et nous permettra donc de constituer des sous-corpus facilement. Il reste à se poser la question des métadonnées qui nous intéressent. Dans leur ouvrage \enquote{\textit{Quantitative Historical Linguistics}}\footcite{gillivray_quantitative_2017}, Barbara McGillivray et Gard B.~Jenset développent une séparation des métadonnées à trois niveau: un niveau bibliographique, donnant accès entre autres aux informations extralinguistiques (période d'écriture, auteur, région), un niveau structurel (les structures logiques de citations ou de présentation de texte: les paragraphes, mais aussi les vers, les poèmes, les chapitres...) et un niveau \enquote{syntaxique} (relation entre les mots, informations sur chacun des mots: lemmes, annotations morphosyntaxiques, etc.). Ce découpage a l'avantage d'être pris en compte par la TEI: les auteurs de la théorie selon laquelle un texte est une hiérarchie ordonnée de contenus textuels (\acrshort{ohco})\footcite{derose_what_1990} ont fait partie des premiers utilisateurs et des premiers membres de la communauté TEI, infléchissant ainsi son cours dans une direction prenant en compte OHCO\footnote{On retrouve notamment dans les auteurs Elli Mylonas, l'une des fondatrices de Perseus et la responsable de l'usage de la TEI dans ce projet dès sa fondation.}.

Dans cette perspective des métadonnées bibliographiques, G.~B.~Jenset et B.~McGillivray recommandent l'identification et l'intégration dans un corpus des métadonnées représentant la période (\textit{when}), l'auteur (\textit{who}), mais aussi le lieu de production (\textit{where}) et sa \enquote{méthode} (\textit{how}). 
Les deux premiers sont faciles à identifier pour la littérature latine classique pour une vaste majorité de cas. La situation est un peu plus complexe lorsque l'on étend le corpus à la période tardive, jusqu'à la mort du dernier père de l'Église, Isidore. Là, de nombreuses œuvres ont une paternité douteuse -- on ne compte plus les textes faussement attribués à Augustin d'Hippone -- ou même une datation difficile, que l'auteur soit anonyme ou non. La datation des \textit{Priapées} fait encore débat\footcite{oconnor_carminis_2019}; les dates de Cornélius Labeo ou Marcus Cetius Faventinus sont presque inconnues, et comme bon nombre de petits auteurs, ils sont uniquement classés grâce à des jeux de renvois ou de mention quand cela se trouve possible. L'annotation des dates du corpus a fait l'objet d'un travail nouveau, méticuleux et documenté: chacun des auteurs ou des œuvres a eu une inscription de date de naissance et de mort, chacune qualifiée d'un niveau de certitude (basse, moyenne, moyenne-haute, haute) correspondant au niveau de certitude trouvé dans la littérature scientifique\footnote{Les mentions du type \enquote{Toutes dernières années du IVeme siècle} écopent d'un classement bas, les dates modulées d'un \enquote{environ} sont annotées comme moyennes, quand deux dates proches sont fournies elles sont qualifiées de moyenne-haute, et haute est réservée aux données précisément chiffrées sans modulation.}. Trente-et-une sources différentes ont été utilisées pour dater les différentes œuvres et auteurs, dans un ordre de préférence allant du travail de Jean-Claude Fredouille, suivi du dictionnaire des auteurs grecs et latins du moyen âge, puis de toute autre source scientifique récente et enfin de \textit{Wikipedia}. 840 œuvres sont datées ainsi (\textit{cf.} table \ref{tab:chap1:sources-fredouilles}).


\begin{table}[]
    \centering
    \begin{tabularx}{\textwidth}{X|r}
    \toprule
    Source & Total \\ \midrule
    \cite{zehnacker_litterature_2013} & 674 \\ 
    & \\
    \cite{noauthor_base_nodate} & 42 \\
    & \\
    \cite{lana_metodologie_2012} & 34 \\
    & \\
    \cite{buchwald_dictionnaire_1991} & 26 \\
    & \\
    \cite{hornblower_oxford_1996} &     13 \\
    & \\ \midrule
    Autres & 51 \\ 
    - dont sources à utilisations uniques & 15 \\\bottomrule
    \end{tabularx}
    \caption{Sources utilisées pour dater les œuvres du corpus et le nombre de fois qu'elles ont été utilisées comme références pour une datation particulière. Si un auteur a plusieurs œuvres, comme Cicéron, chacune de ses œuvres est comptabilisée une fois dans le calcul. Les sources uniques sont des articles, éditions qui ont fourni l'information, inaccessible par ailleurs.}
    \label{tab:chap1:sources-fredouilles}
\end{table}

Quelques œuvres ont posé un vrai problème de datation. Dans ces cas, une approche de cohérence avec le reste du corpus a été préférée à une justesse ou une précision des informations. Les œuvres composites, comme l'\textit{Anthologie latine}, ont reçu des datations extrêmement larges quand la littérature donne une information sur l'étendue temporelle d'écriture. Les œuvres dont l'attribution est mise en doute ont obtenu la datation de l'auteur supposé quand elle est donnée, ou de l'auteur d'origine quand la littérature scientifique n'arrive pas à se fixer.

Les deux autres types de métadonnées, par contre, posent un réel problème de fond pour le corpus latin. Le \textit{where} n'a pas beaucoup d'importance pour le latin hors de cas précis (dialectométrie par exemple): une grande part de la littérature classique est écrite à Rome, centre des pouvoirs où se trouvent les mécènes\footnote{On pense par exemple à la communauté hispanique représentée par Sénèque, Martial et Lucain au Ier siècle.}. Dans ses exemples, B.~McGillivray retranscrit le \textit{how} par le genre littéraire appliqué au texte. Et cette catégorie semble très problématique, sans demander un véritable travail de fond sur les genres littéraires à travers la littérature latine du premier millénaire. Comme le fait justement remarquer la chercheuse, nous n'avons plus de locuteurs de la période, ce qui implique deux choix possibles: ou nous essayons de plaquer les genres littéraires que la recherche en lettres a produits au fil des siècles, où nous essayons d'appliquer ceux qui sont mentionnés -- rarement -- par les auteurs de l'époque. La première est complexe: les \textit{Lettres à Lucilius} sont-elles un roman épistolaire, une forme de dialogue philosophique à une voix, un ensemble de traités philosophiques ? La seconde l'est tout autant, et tout choix restera critiquable, comme le montre le compte-rendu de Henry Bardon\footcite{bardon_review_1983} citant Florence Dupont\footcite{dupont_ciceron_1982} à propos de l'ouvrage de collectif \textit{Les Genres littéraires à Rome}\footcite{martin_les_1981}: si l'on se base sur des catégories comme le narratif et le descriptif, où peut-on placer des œuvres aussi complexes que le \textit{De Natura Rerum} ? Ces remarques ne signifient pas qu'il est impossible de le faire, mais il faudrait, pour classer en genre l'ensemble des œuvres latines, et non uniquement celles des périodes préchrétiennes, des choix multiples et combinables, valides synchroniquement, et applicables à des extraits de texte. Tous les épigrammes de Martial ne relèvent pas de la satire.  

Il reste enfin la question des métadonnées structurelles. Dans les œuvres latines, comme dans la plupart des livres modernes, les ouvrages publiés sont généralement divisés en plusieurs unités textuelles plus petites, qui peuvent être des chapitres, des recettes, des poèmes, etc\footnote{Ce texte a été réutilisé dans la publication en cours, \enquote{Thibault Clérice. \textit{"Don't worry, it's just noise": quantifying the impact of files treated as single textual units when they are really collections}. Workshop on Natural Language Processing for Digital Humanities (NLP4DH), Dec 2021}}. Pour les travaux en prose tels que les romans ou les livres d'histoire, les chapitres et les paragraphes sont généralement l'unité à laquelle on peut se référer. Cette segmentation est souvent une manière pour l'éditeur ou l'auteur d'indiquer des changements de thèmes légers ou forts, ou des ellipses narratives. En poésie, la plupart des poèmes sont publiés sous forme de recueils, et, du moins pour la littérature latine, on ne s'attend pas à ce qu'ils soient séquentiels : il y en a très peu, voire aucun, qui peuvent se lire comme une séquence narrative dans les \textit{Épigrammes} de Martial, et quand il y a des connexions, elles sont probablement plus des échos que le résultat d'une progression. Il existe d'autres genres et formes textuels que nous avons pu conserver au fil des millénaires, comme les \textit{Recettes} d'Apicius, les œuvres de médecine telles que le \textit{Gynaeciorum Sorani} de Caelius Aurelianus ou les commentaires grammaticaux de scholiastes tels que ceux de Porphyrion: là encore, il ne s'agit pas d'une seule séquence narrative cohérente, mais plutôt d'une collection de courtes unités, reliées par un thème global. 

Et contrairement à la littérature moderne, où l'on s'attendrait à ce que les chapitres et les paragraphes soient des marques d'auteur sur le texte, le statut de ces marques peut différer d'un genre à l'autre pour la littérature antique. Ces textes ont été transmis, réinterprétés et - en tant que tels - modifiés quelques siècles seulement après leur première publication. Et pour certaines des œuvres que nous connaissons sous un seul nom d'auteur et un seul titre d'œuvre, nous savons avec certitude qu'il y avait soit plusieurs auteurs (par exemple, la \textit{Guerre des Gaules} de César), de multiples œuvres originales rassemblées par des compilateurs (par ex., la Bible) ou les deux, comme Sulpicia dont les élégies se trouvent dans le \textit{Corpus Tibullianum}. 

Nous avons que certaines de ces divisions sont anciennes voire d'origine. La catégorie \enquote{livre} par exemple se retrouve parfois citée par les auteurs eux-mêmes -- cela arrive avec plusieurs épigrammes d'introduction de Martial -- ou par d'autres auteurs sous le nom de \textit{volumen} pa rexemple\footcite[p. 13]{canfora_conservazione_2016}. Pour la poésie, l'existence de multiples poèmes dans les \enquote{éditions originales} est acquise, bien que l'ordre original puisse faire l'objet de changements au fil de la transmission: certains éditeurs proposent des ordres différents, comme Léon Herrmann\footcite{catulle_les_1957}, éditeur de Catulle, qui propose un arrangement tout à fait différent des autres comme ceux de Lafaye \footcite{catulle_poesies_1932}). Les textes peu transmis et à la portée plus faible comme les \textit{Priapées} ont plus de probabilité d'être victimes de ces réarrangement ou interprétations tardive: on y segmente les poèmes différemment car le nombre réduit de témoins n'a pas toujours permis à un consensus d'émerger sur la quantié ou les coupures entre poèmes. Il reste par ailleurs que nous avons la certitude que certaines de ces segmentations soient tout à fait ultérieures à la rédaction de l'œuvre: c'est le cas des \textit{scholia} et des commentaires en général. L'hypothèse actuelle veut qu'ils aient pour origine des notes relié par un lemme (\textit{hypomnemata}) ou qu'ils furent de simples notes intralinéaires et marginales: leur transmission les a bien souvent transformés en textes continus dans lesquels le texte a été inséré\footcite{bureau_quelques_2012}. 

Dans la plupart des autres situations, la segmentation actuelle du texte est soit l'effet des érudits médiévaux, comme pour la numérotation des versets de la Bible, des éditeurs du XVI--XVIIème siècle ou des contemporains: c'est le cas pour le \textit{Pro Murena}, comme le démontre Fotheringham \footcite{fotheringham_numbers_2007}. Non seulement ce texte existe avec deux segmentations concurrentes, mais les paragraphes, quand ils ne sont pas numérotés et identifiés, ne sont parfois pas les mêmes d'un éditeur à l'autre. Bien sûr, il existe des traditions textuelles encore plus complexes qui remettent parfois en cause l'ordre des textes, comme celle du \textit{Satyricon} de Pétrone ou des pièces de Plaute, et proposent des formes d'œuvres complètement différentes, comme l'\textit{Epistola Alexandri ad Aristotelem}, une œuvre anonyme qui existe dans deux \textit{recensio} différents.

Quiconque a segmenté les textes, auteurs ou éditeurs, a fourni des informations sur la manière dont l'œuvre complète doit être lue par un être humain. Ces systèmes de segmentation ont aussi fait l'objet d'une indexation, permettant aux chercheurs de faire référence à un passage d'un texte de manière quasi universelle. Ainsi, rencontrer dans un texte \enquote{Martial, 2, 73} identifiera l'épigramme 73 du livre 2 des \textit{Épigrammes} de Martial (l'une de ses deux seules œuvres). Ce système de citation canonique a fait l'objet d'un effort de passage au numérique, sous la direction de Neel Smith et Christopher W.~Blackwell, prenant le nom de \textit{Canonical Text Services}\footnote{CTS est beaucoup plus large qu'un simple système d'identifiants et inclut par exemple la définition d'une architecture d'API. Cependant, CTS est particulièrement limité, voire décrié, particulièrement à cause de la centralisation des décisions autour des deux auteurs originaux et de son incapacité à passer à l'échelle. D'autres solutions ont été développées par la communauté dont le \textit{Distributed Text Service}. \textcite{blackwell2019cite}. \textcite{almas_distributed_2021}}. Il repose sur une identification à deux parties, un identifiant de texte et un identifiant de passage qui permettent alors de garantir une traçabilité du texte, mais aussi des extraits au moment (\textit{cf.} image 2.73). Donner la capacité de qualifier les passages d'un identifiant permettrait à la fois de répondre au besoin de métadonnées structurelles exprimées par B.~McGillivray et d'assurer une capacité de retourner à la source.

\begin{figure}
    \centering
    \includegraphics[height=3cm]{figures/chap1/part2/cts-urn.drawio.png}
    \caption{Déconstruction d'un identifiant CTS équivalent au \enquote{Martial, 2, 73}: \texttt{urn:cts:latinLit:phi1294.phi002.perseus-lat2:2.73}}
    \label{fig:chap1:cts_urn}
\end{figure}

% Métadonnées générales (Gillivray ?)
%   -> Reprendre la méthode de datation
% Métadonnées structurelles
%   -> Poser la question de la citabilité et de la section des textes

% Reprendre le travail de Mc Gillivray ici
% La question de la datation
% Poser la question de la citabilité et de la section des textes
% Métadonnées de “lecture”: modèles de citation, niveau de citation recommandé (Introduction du concept de SATU ?)

\subsection{Production et transformations de corpus}

Nous avons défini un cadre théorique à la production de corpus via ses obligations en termes d'éthique scientifique. Il reste donc à faire la transition vers un cadre technique, apte à remplir nos objectifs: obtenir un corpus tel qu'il sera possible d'y retrouver les exemples d'isotopie et d'y faire des recherches lexicographiques. Les conditions de sa production sont donc des conséquences de ces choix: le corpus doit être standardisé et donc en XML-TEI, doit posséder des métadonnées fines, y compris un système d'identification de passages afin de pouvoir revenir à la source ou comparer le texte d'une édition avec une autre. Enfin, il doit avoir une couverture temporelle la plus large possible pour convenir à la source scientifique utilisée pour la compilation, c'est-à-dire l'oeuvre d'Adams, qui s'étend entre autres jusqu'à Isidore\footnote{Le nombre de textes plus tardifs qu'Isidore est très réduit. On ignorera aussi les sources épigraphiques.}. Nous nous intéresserons ici d'abord à la méthode d'encodage et à l'outillage pour produire ce corpus. Ensuite, nous discuterons de sa compilation, à la fois en tant que meta-corpus et en tant que générateurs de versions inédites de textes. Enfin, nous proposerons une analyse des textes contenus, du point de vue des périodes, des quantités et de la diversité d'auteur.


\subsubsection{Choix techniques: XML TEI et Capitains}

La TEI est une \enquote{grammaire}, en cela, elle a l'avantage d'être partagée par de nombreux projets et d'être compréhensible. Mais elle n'est \enquote{que} descriptive: l'exploitation de sa syntaxe ne peut se faire qu'à travers des outils développés pour elle, qu'ils soient \textit{ad hoc} ou partagés par la communauté. Dans ce contexte, nous avons développé depuis 2013 un ensemble d'outils tournés vers l'exploitation et la description des structures logiques de citations et de leurs identifiants. Cet ensemble d'outils, nommé \textit{Capitains}, propose deux pans: un tourné vers l'encodage et l'organisation de corpus, un second cherchant à valoriser, contrôler et exploiter les données ainsi encodées\footcite{clerice_capitains_2015}. Ce cadriciel démarre dans le cadre des projets Perseus et Perseids, sous les demandes de Bridget Almas, ingénieure principale des deux projets\footnote{Officiellemet uniquement de Perseids.}, afin de mettre en place un service de distribution de textes qui permette une maintenance rapide et une citabilité pour CTS\footnote{Ce travail est largement décrit, et plus techniquement qu'ici, dans les articles suivant: \textcite{almas_continuous_2018, clerice_les_2017}}. 

Les \textit{guidelines} Capitains identifient dès leur création deux problèmes importants pour la gestion des corpus de \textit{Perseus}. D'une part, les métadonnées bibliographiques sont particulièrement difficiles à maintenir: chaque fichier du corpus doit contenir des métadonnées sur l'auteur et l'oeuvre, quand bien même ces dernières sont multipliées à l'envie, le corpus étant composé d'auteurs prolixes -- comme Cicéron -- et d'oeuvres représentés par de multiples éditions et traductions.  D'autre part, les textes doivent être eux-mêmes manipulables, facilement, et répondre à l'ensemble des besoins de l'API CTS, entre autres ceux de la citabilité et de l'obtention automatique de passages via leur identifiant.

Pour le premier problème, celui de la gestion de corpus et de ses métadonnées bibliographiques, on a procédé à l'extraction de ces dernières en dehors du fichier lui-même. En décrivant les auteurs ou les oeuvres hors de celles-ci, la maintenance des informations communes est centralisée et permet de rapidement mettre à jour le catalogue. Pour simplifier encore ce travail, les dépôts \textit{Capitains} doivent suivre une \enquote{syntaxe} pour l'arborescence des fichiers, permettant ainsi de trouver facilement un auteur, ou un oeuvre, parmi plusieurs centaines de fichiers. Avec le recul et l'expérience accumulée au fil des années, cette externalisation est à la fois utile -- la compilation du catalogue est extrêmement rapide car elle ne nécessite pas d'ouvrir de gros fichiers TEI -- mais extrêmement inadaptée à de plus petits corpus, qui souffrent d'avantage de la duplication des efforts (métadonnées d'en-têtes TEI, métadonnées de catalogue \textit{Capitains}).

La seconde partie du problème -- plus intéressante pour nous ici -- s'intéresse à la manière de rendre intelligibles à un programme les structures logiques de citations. Grâce à l'explicitation des rôles et des hiérarchies textuelles des noeuds XML, il est possible de produire par exemple des table des matières pour un logiciel: le fichier devient donc \textit{machine-actionable}. Or, si l'usage de l'expression \enquote{encodage CTS} peut se trouver, y compris chez L.~Muellner\footcite{muellner2019free}, CTS ne déclare aucun moyen de constituer les données et se déclare même \enquote{\textit{technology independant\footcite{smith_brief_nodate}}}: il faut donc trouver une solution technique, pour des sources TEI, à ce problème. Au moment de la production des \textit{guidelines} Capitains, vers 2014, une seule méthode avait retenu notre attention et semblait offrir une solution à ce problème. Nous discuterons ensuite brièvement d'une seconde option, arrivée en 2021 dans les \textit{guidelines TEI}.

CTS définit le texte comme une structure OHCO extrêmement rigide: le texte est un arbre, dont chaque feuille ou branche est un élément citable construisant son propre identifiant à partir de ceux de l'ensemble de ses ancêtres. Ainsi, le numéro de vers d'un poème de Martial ne peut exister qu'avec la référence de son poème et du livre de ce dernier: 2.72.1 est ainsi l'identifiant du premier vers, du soixante-douzième poème du deuxième livre. CTS requiert des identifiants qu'ils soient uniques et les définit comme inséré dans une séquence et une hiérarchie. Les \textit{guidelines} Capitains jusqu'à leur version 2.0 incluse ne peuvent accepter qu'une structure hiérarchique simple, un arbre dont les branches de citation ne peuvent avoir qu'un seul type d'enfant et un seul squelette de chemin XML (appelé xPath): chaque type de passage ne peut être parent que d'un seul type d'enfant, ici livre, poème, vers. Pour décrire cet arbre de citation, les \textit{guidelines} Capitains font appel aux éléments \texttt{cRefPattern} (\textit{Canonical Reference Pattern}) du \texttt{refsDecl} (\textit{References System Declaration}) des \textit{guidelines} TEI. Les niveaux, décrits les uns après les autres, du plus profond au plus proche de la racine, utilisent un système de xPath et d'expressions régulières pour fournir à la machine tout un outillage pour la compréhension des identifiants (1.2.3 est à tel endroit) ou leur reconstitution (1.2.1, 1.2.2, 1.2.3, etc.), comme dans l'exemple \ref{chap1:xml:cRefPattern}.

\begin{figure}[ht]
    \centering
    \lstset{language=XML}
    \begin{lstlisting}[language=XML]
<refsDecl n="CTS">
    <cRefPattern n="line"matchPattern="(\w+).(\w+).(\w+)"
        replacementPattern="#xpath(/tei:TEI/tei:text/tei:body/tei:div/tei:div[@n='$1']/tei:div[@n='$2']/tei:l[@n='$3'])">
    </cRefPattern>
    <cRefPattern n="poem" matchPattern="(\w+).(\w+)"
        replacementPattern="#xpath(/tei:TEI/tei:text/tei:body/tei:div/tei:div[@n='$1']/tei:div[@n='$2'])">
    </cRefPattern>
    <cRefPattern n="book" matchPattern="(\w+)"
        replacementPattern="#xpath(/tei:TEI/tei:text/tei:body/tei:div/tei:div[@n='$1'])">
    </cRefPattern>
</refsDecl>
    \end{lstlisting}
    \caption{\texttt{resfDecl} des \textit{Épigrammes} de Martial dans le dépôt Github de Perseus. Les niveaux sont déclarés du plus profond (les vers) au plus proche de la racine (les livres). Chaque \textsc{replacementPattern} fournit un xPath où les variables \$1, \$2 ou \$3 sont remplacées via le découpage des identifiants autour des points. Le passage 2.72.1 donnera \$1=2, \$2=72, \$3=1 et donc un xPath \texttt{/TEI/text/body/div/div[@n='2']/div[@n='72']/l[@n='1']}}
    \label{chap1:xml:cRefPattern}
\end{figure}

Depuis 2021, une autre option, beaucoup plus élastique et plus simple à l'utilisation pour les encodeurs comme pour les développeurs, a été mise à disposition dans les \textit{guidelines} TEI\footcite{cayless_introducing_2021}. Cette nouvelle méthode de déclaration permet d'échapper non seulement à la complexité des \texttt{cRefPattern} et de clarifier la hiérarchie entre les système de passages, mais aussi d'intégrer des variations bienvenues dans la ridigité de ceux-ci, en autorisant des systèmes différents de citation. Les \texttt{citeStructure} permettent ainsi de générer nom seulement des arbres de citation mais aussi de les qualifier par des métadonnées, avec une complexité technique beaucoup plus faible que celle des premières options susmentionnées (\textit{cf.} \ref{chap1:xml:citeStructure}).

\begin{figure}[ht]
    \centering
    \begin{adjustbox}{width=0.9\textwidth,keepaspectratio}
    \lstset{language=XML}
    \begin{lstlisting}[language=XML]
<refsDecl n="CTS">
  <citeStructure unit="book" match="/TEI/text/body/div" use="@n">
    <citeData property="http://purl.org/dc/terms/title" use="head"/>
    <citeStructure unit="poem" match="div" use="@n" delim=".">
      <citeStructure unit="line" match=".//l" use="@n" delim="."/>
    </citeStructure>
  </citeStructure>
</refsDecl>
    \end{lstlisting}
    \end{adjustbox}
    \caption{Équivalent de la figure \ref{chap1:xml:cRefPattern} utilisant les déclarations \texttt{citeStructure}. Chaque élément repart de la déclaration parente. \texttt{citeData} permet d'ajouter des qualifications (métadonnées) de chaque passage pour la machine.}
    \label{chap1:xml:citeStructure}
\end{figure}

Mais les \textit{guidelines} Capitains ne sont que la première étape, celle de l'explicitation, dans la gestion des structures logiques de citation: elles ne sont qu'une forme de \enquote{sur-TEI}. L'exploitation de ces structures d'encodage est fournie à travers plusieurs outils dont la principale brique est \textit{MyCapytain}, une librairie fournissant un grand nombre de fonctionnalités \enquote{nues} et sans objectif propre à part celle de rendre plus facile le développement autour des \textit{guidelines}. \textit{MyCapytain} permet \enquote{de mettre en action} les informations fournies par les différentes déclarations citées: dans notre cas, elle nous permet de produire une liste des passages extrêmement détaillées, liant chacune des phrases ou des mots à un identifiant complet du type \texttt{urn:cts:latinLit:phi1293.phi002.perseus-lat2:2.72.1} autour d'un code réutilisable et compréhensible. Le corpus peut être ensuite intégralement appelé et chargé à l'aide de ces quelques lignes\footnote{Le traitement du corpus, du téléchargement des nouvelles versions des corporas à leur lemmatisation en passant par leur découpage en segments identifiés par le système d'URN CTS se retrouve dans les notebooks \enquote{Data Preparation - Corpora}. Ce morceau de code est dérivé librement du notebook \enquote{\textit{Step 1}}.}:

\begin{lstlisting}[language=Python]
import glob
from MyCapytain.resolvers.cts.local import CtsCapitainsLocalResolver

repositories = list(glob.glob(CorporaPathList, recursive=False))
resolver = CtsCapitainsLocalResolver(repositories)
for text in resolver.texts:
    if text.lang == "lat":
        # Traitement du texte
\end{lstlisting}

Avec la complexité que représentent l'encodage des structures logiques de citation, leur interprétation dynamique par des logiciels tiers et les restrictions produites par les recommandations CTS, la constitution de corpus Capitains est plus à même de contenir des erreurs d'encodage que ne sauraient détecter de simples schémas XML. À travers la TEI comme les \textit{guidelines} Capitains, le texte devient un programme: il est donc capable de contenir des bugs, qui sont de l'ordre de la gestion technique de la donnée, à côté de problèmes d'usages, qui relèvent de la gestion qualitative de la donnée. Nous avons donc introduit des outils de contrôle pour cet encodage particulier qui, en testant automatiquement ce dernier, simplifie la validation \enquote{technique} et laisse ainsi le loisir de s'intéresser principalement à la qualité de ce dernier. C'est pourquoi l'ensemble des textes Capitains peuvent être validés par un outil appelé \textit{HookTest}. Cet outil, en plus d'intégrer des tests de schémas TEI, vérifie que chacun des systèmes de déclaration (1) corresponde à des passages uniques, (2) ne croise pas un autre système de citation, (3) ne produise pas de conflit d'identifiants (\textit{cf.} figure \ref{fig:annx:digiliblt-hooktest} en annexe).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/chap1/part2//capitains.png}
    \caption{Mode de fonctionnement de \textit{Capitains}. \textit{MyCapytain} est une brique commune  l'ensemble des programmes python du monde \textit{Capitains}, elle permet de faire fonctionner autant \textit{HookTest} que \textit{Nautilus}, l'application derrière les serveurs de textes de \textit{Perseus}.}
    \label{fig:chap1:Capitains}
\end{figure}

% Dans la production de corpus, HookTest se situe à la fois comme un outil à utiliser individuellement, pendant la création de fichiers TEI, mais aussi de manière continue dans la vie du corpus. Nos corpus
%Le contrôle qualité des données est un pré-requis important à la construction d'application ou d'analyse. 



\subsubsection{Méthode de compilation, conversion et sources des corpus}

Tous ces choix permettent d'enfin compiler un meta-corpus permettant de retrouver et créer notre corpus exemplier. Cette phase de collection et de production est un processus itératif, conditionné à la fois par la mise à disposition de nouvelles sources au format numérique, par la gestion de priorité d'acquisition en fonction de la rencontre d'oeuvres chez Adams mais indisponibles dans les corpus au format CapiTainS, et par les opportunités de traitement et d'externalisation de cette mission. Nous discuterons d'abord du travail effectué sur les corpus pré-existants et adaptés à notre cadre technique, puis à l'adaptation de corpus existants.

En effet, l'opportunisme et l'économie d'énergie régissent d'abord notre collecte: nous pouvons commencer par adopter les corpus qui sont déjà adaptés à nos pré-requis techniques. \textit{Capitains} ayant été adopté par la sphère de \textit{Perseus}, et presque uniquement par celle-ci, ce projet et son alter-ego \textit{Open Greek and Latin} sont les premières sources de notre corpus. Pour autant, chacun des corpus disponibles dans ces projets (\textit{Canonical Latin Literature} de \textit{Perseus}, \textit{CSEL} et \textit{PL} d'\textit{OGL}) pose un problème spécifique.

Pour reprendre l'historique de la production de ces corpus et projets, \textit{Perseus} n'avait pas, au début de nos travaux de recherche, converti l'ensemble de ses textes aux \textit{guidelines Capitains}, encore moins pour ses textes latins. De 2013 à 2016, l'ensemble des conversions de textes latins des dépôts Perseus datant d'avant l'obtention de la chaire à Leipzig avait été réalisé par nos soins, sans que cela soit une mission prioritaire. La majeure partie des efforts se concentre d'abord sur les textes du CSEL puis sur les textes de la PL. En avril 2017, au moment de notre départ de Leipzig, le corpus ne contenait que 390 textes de convertis sur 674, traductions inclues, pour un total d'environ 5,07 millions de mots en latin\footcite{travis_hook_2017, travis_hook_2017_logs}. En décembre 2021, ce total est monté à 418 textes sur 686, avec une augmentation de plus d'un million de mots\footnote{6~159~948 mots latins pour êtres précis, d'après \textcite{travis_hook_2021_logs}}. L'une des sources principale de cette augmentation est la conversion de l'ensemble du corpus de Cicéron. Il a donc fallu s'assurer de convertir ces corpus, et que ces corrections rentrent dans le calendrier de production des données de Perseus. Sur ce dernier point, on notera qu'en décembre 2021 nous avions encore deux conversions\footcite{noauthor_pull_nodate} en attente de validation par l'équipe actuelle du projet: elles ne rentraient pas dans le calendrier de rétro-conversion des sources, principalement tourné vers le grec et les traductions\footnote{Il n'est en aucun cas question ici de jeter la pierre sur d'anciens collègues, mais de mettre en avant les problèmes que peuvent poser parfois ce genre de collaboration, à travers l'existence de priorités, d'attentes et de missions différentes.}. 

Un cas particulier de conversion pour le corpus\textit{ Canonical Latin Literature} fut celle des \textit{Annales} de Tacite. Ce texte -- dont Perseus possédait plusieurs éditions -- fut victime d'un problème de conversion entre les diverses versions de la plate-forme et donna lieu à sa segmentation involontaire en quarante sept oeuvres parmi lesquelles plusieurs éditions furent mélangées\footcite{noauthor_canonical_latinlitphi0660phi003perseus_lat2xml_nodate, clerice_phi0914phi001lat2_nodate}. Sa transformation était impossible à partir des fichiers \textit{open source} trouvables dans les dépôts du projet, demandait une connaissance de l'historique des migrations pour en déduire l'origine des problèmes et un travail minutieux de transformation d'une TEI P4 vers une TEI P5 Capitains. C'est à travers une collaboration entre d'anciens employés et des employés actuels que cette conversion fut rendue possible.

Outre le corpus original \textit{Canonical Latin Literature}, Perseus et le projet \textit{Open Greek and Latin} ont mis à disposition deux autres corpus: le CSEL et la PL. Ces deux derniers ont la particularité d'être des corpus extrêmement récents dans l'histoire de Perseus, produits majoritairement via OCR et dont la structuration en XML TEI a été externalisée à une entreprise n'étant pas formée de latinistes ou d'hellénistes mais d'encodeurs de documents administratifs. Cette méthodologie de production de données a engendré deux grands problèmes. D'une part, l'OCR peut présenter des erreurs, anodines souvent, particulièrement problématiques pour notre étude parfois. On notera par exemple l'erreur répétée de segmentation d'\textit{oraculus} en deux mots (\textit{ora} et \textit{culus}) suite à la non résolution d'une hyphénation qui n'avait plus lieu d'être dans le document XML, mais aussi de \textit{sae-culum}, d'\textit{o-culus}, de \textit{claui-culus}, \textit{taberna-culum}, etc\footnote{Quelle ne fut pas notre surprise quand nous trouvions, par moteur de recherche, un grand nombre d'emploi de \textsc{culus} chez Augustin d'Hippone.}. D'autre part, l'encodage des structures logiques de citation a pu poser des problèmes, car sa formalisation typographique peut fluctuer d'une impression à l'autre, d'autant plus sur des périodes de productions comme celle de la CSEL (plus d'un siècle), et il peut être difficile de faire la différence entre un numéro de ligne, un numéro de section, etc. Cette reproduction de l'information demande alors à la fois une compréhension des mécanismes d'édition et de langues, afin de séparer les identifiants de section du texte: il ne s'agit pas seulement de reconnaître les sauts de lignes qui sont des vers ou des changements de paragraphes, mais aussi le rôle du paratexte qui les accompagne.

La constitution de ces corpus a aussi eu le malheur d'avoir lieu pendant la formalisation des \textit{guidelines} Capitains. Il a donc fallu, après la réception des données, non seulement reprendre les structures logiques mais aussi intégrer les informations requises pour leur exploitation. Ce travail était achevé pour la CSEL au cours de l'année 2017 par Matt Munson, encore employé de Leipzig. Cependant, ce travail ne fut jamais achevé depuis 2017, laissant le corpus de la PL \enquote{en friche} du point de vue de sa compatibilité avec les outils de l'écosystème \textit{Perseus}. Cet abandon du corpus, au moins momentanné, est un exemple supplémentaire du changement politique et économique de la production de corpus de Perseus. Le projet \textit{First Thousand Years of Greek} du CHS finance plus, les textes de la patrologie latine sont disponibles par ailleurs sur d'autres plate-formes: le projet change de priorités de production de corpus. Un seul corpus supplémentaire est né pendant depuis 2017 en latin: le corpus OGL/Latin. Il contenait deux textes au moment de la compilation du corpus.

CSEL et canonical-latinLit mis ensemble, le corpus représente environ douze millions de mots, dont les épicentres thématiques et chronologiques sont respectivement les pères de l'Église de l'antiquité tardive et la littérature classique de la Rome pré-chrétienne. Il reste alors, en vue des périodes traitées par Adams, en plus de quelques textes non convertis chez Perseus, trois secteurs qui ne sont pas adressés:

\begin{enumerate}
    \item l'épigraphie. Elle est intégralement ignorée dans notre corpus pour des raisons évoquées plus tôt dont le problème important de sa lemmatisation.
    \item la période tardive non-chrétienne, en particulier les médecins (importants pour notre recherche) et les grammairiens.
    \item la littérature de la période classique peu connue ou peu étudiée avant le master: Perseus se concentre d'abord historiquement sur les corpus les plus communs pour rendre leur accès plus facile. 
\end{enumerate}

Pour le second secteur comme pour le troisième, la règle principale pour l'incorporation de textes au meta-corpus a été leur mention par Adams dans son ouvrage: tout texte dans les bornes chronologiques mentionnées -- c'est-à-dire avant la mort d'Isidore de Séville -- a fait l'objet d'une recherche d'une édition numérique dans un corpus tiers. Dans le cas où le code pour la conversion d'une édition XML-TEI, HTML ou plein texte était facilement réutilisable pour convertir d'autres oeuvres similaires d'un même site, nous ajoutions à l'oeuvre mentionnée par Adams les oeuvres similaires convertissables à bas coût. Ces conversions ont été mises à disposition dans deux corpus distincts: pour les oeuvres du projet \textit{DigilibLT}, projet encore actif et centré sur un des angles morts des corpus Perseus, la littérature tardive non chrétienne, un corpus miroir \textit{DigilibLT-Capitains}\footcite{Clerice_Digital_Latin_Library_2021} a été mis en développement par nos soins; pour les autres oeuvres, issues de sources diverses (\textit{cf.} table \ref{tab:chap1:corpusutilises}), un corpus sobrement nommé \enquote{\textit{Lasciva Roma - Additionnal Texts}}\footcite{Clerice_Corpora_of_rare_2021} a vu le jour. Le premier compte 103 textes\footnote{sur 216 téléchargés de \textit{DigilibLT}} et 2,8 millions de mots, le second 134 textes et 3,8 millions.


\begin{table}[h]
    \centering
    \begin{tabular}{lr}
        \toprule
        Corpus numérique ou édition numérique utilisée & Fréquence \\
        \midrule
         OBI\footnotemark            & 73 \\
         Lacus Curtius\footnotemark         & 18 \\
         Corpus Corporum\footnotemark & 12 \\
         Horatius.net\footnotemark    & 12 \\
         The Latin Library\footnotemark   &  4 \\
         Musisque Deoque\footnotemark &  3 \\
         Perseus        &  3 \\
         Remacle\footnotemark        &  2 \\
         ALIM\footnotemark           &  1 \\
         Lesley Bolton\footnotemark         &  1 \\
         Latin Law Library\footnotemark    &  1 \\
         Monumenta Germanica Historica\footnotemark            &  1 \\
         Corpus Scriptorum Latinorum\footnotemark            &  1 \\ \midrule
         Éditions originales à partir d'OCR    &  2 \\
        \bottomrule
    \end{tabular}
    \caption{Source des corpus utilisés. Deux sortent du lot. OBI présente 73 éditions, un nombre surgonflé: il y a 73 livres pour la \textit{Vulgate} d'après CTS). Nous remercions chaleureusement Lesley Bolton, docteure canadienne, de nous avoir permis de réutiliser son travail de thèse pour en faire une édition TEI. Les éditions originales à partir d'OCR ont été produites à partir des données d'\textit{Internet Archive}.}
    \label{tab:chap1:corpusutilises}
\end{table}
\footnotetext{\textcite{barry_schein_open_1994}}
\footnotetext{\textcite{lomarcan_lacuscurtius_1999}}
\footnotetext{\textcite{roelli2014corpus}}
\footnotetext{\textcite{horatiusnet}}
\footnotetext{\textcite{carey_latin_1998}}
\footnotetext{\textcite{gelderblom_musisque_2008}}
\footnotetext{\textcite{philippe_remacle_site_2008}}
\footnotetext{\textcite{d2019alim}}
\footnotetext{\textcite{bolton_edition_2015}}
\footnotetext{\textcite{lassard_roman_2001}}
\footnotetext{\textcite{noauthor_openmgh_nodate}}
\footnotetext{\textcite{csl}}

Pour le troisième point, un texte fondamental pour notre recherche est absent des corpus Perseus: les \textit{Priapées}\footcite{Clerice_Digital_edition_of_2020}. Il s'agit d'un ensemble de poèmes plutôt courts et centrés autour de la personnalité de Priape, dieu gardien des vergers dont l'arme principal est un sexe en érection. Elle est constituée de deux blocs, une partie de ses poèmes y ayant été ajouté par analogie au cours des éditions du texte\footcite{callebat_les_2008}. Pour la mettre à disposition, nous avons réutilisé le texte et les traductions de R. F. Burton et L. C. Smithers de 1890\footcite{burton_priapeia_1890} comme première version du texte. Cette base de texte nous a évité de passer par la fastidieuse case de l'OCR, de sa correction et de la qualification de ses lignes en vers, poèmes ou notes de bas de page. Nous l'avons ensuite adapté en fonction de l'édition scientifique d'Emil Baehrens paru chez Teubner\footcite{baehrens_poetae_1910}. Ce texte a été intégralement traduit pour la présente recherche et annoté linguistiquement en lemme et en morpho-syntaxe: cette phase d'annotation a permis de relire intégralement le texte, et de le comparer avec l'édition de Louis Callebat paru en 2012 aux Belles lettres et dont nous avons utilisé la réimpression de 2017\footcite{callebat_priapees_2012}. Si les variations entre les deux éditions sont faibles, on a préféré donner en apparat critique quelques choix de l'éditeur français quand elles respectaient l'intégralité des manuscrits là où l'éditeur allemand en produisait une minoritaire. Comme la majeure partie des éditions de Perseus, cette édition numérique des Priapées a plus pour vocation de donner un texte qu'une édition critique, hormis ces quelques notes importées de la comparaison avec l'édition la plus récente.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/chap1/part2/hooktest.png}
    \caption{Processus automatisé}
    \label{fig:chap1:ci}
\end{figure}

Une partie du travail sur \textit{DigilibLT-Capitains} et la majeure partie de la conversion de Tacite ont été réalisé dans le cadre de devoirs fournis aux étudiants et étudiantes des master \enquote{Technologies Numériques Appliquées à l'Histoire} (TNAH) et \enquote{Humanités Numériques} de l'ENC et de PSL. Dans le cadre de l'apprentissage des outils de versionnage Git, cinquante-six éditions du projet italien ont été \enquote{capitainisé}, sur choix libre des étudiants. Cet exercice permettait à la fois de comprendre les enjeux de la collaboration sur plate-forme numérique, avec une dizaine de collaborateurs travaillant sur des données partagées, ses bonnes pratiques et la gestion des contrôles qualités. En effet, \textit{HookTest} permet aussi d'être utilisé comme un outil de contrôle automatique des corpus sur des plate-forme comme Github dans le cadre d'intégration continue, c'est-à-dire dans le cadre de tests automatisés et décentralisés lancés à chaque modification du dépôt contenant les données (\textit{cf.} figure \ref{fig:chap1:ci}).

Le meta-corpus final est formé des six corpus \textit{open-source} et libres de droit mentionnés pour un total d'une vingtaine de millions de mots (\textit{cf.} table \ref{tab:chap1:corpora}).

\begin{table}
    \centering
    \begin{tabular}{lr}
    \toprule
    Corpus                         & Version utilisée \\ \midrule
    PerseusDL/canonical-latinLit & 0.0.843 \\
    OpenGreekAndLatin/csel-dev   & 1.0.223 \\
    lascivaroma/priapeia         & 1.1.18  \\
    lascivaroma/additional-texts & 1.0.193 \\
    lascivaroma/digiliblt        & 0.0.64  \\
    OpenGreekAndLatin/Latin      & v1.11.0 \\ \bottomrule
    \end{tabular}
    \caption{Corpus utilisés pour la formation du meta-corpus de sources.}
    \label{tab:chap1:corpora}
\end{table}
% Ces corrections demandent à la fois une compréhension de la langue et des règles de mise en page. 
% Description des corpus ciblés et du processus itératif d'adaption du corpus

\subsubsection{Analyse de la diversité du corpus par période, auteur et genre}

Le corpus final, une fois aggloméré
% Enfin, statistiques sur le corpus.
    % Statistiques sur les corpus Perseus et autres
    % La conversion de DigilibLT
    % Présentation des corpus Lasciva Roma
    %% Priapées
    %% Additional-Texts

\subsection{Du corpus au document: qu’est-ce qu’un document pour l’ordinateur ?}

Dans ce contexte, nous proposons de classer les unités formées par ces segmentations en deux types : d'une part, celles qui sont clairement non séquentielles -- comme les poèmes -- comme \textit{unités textuelles autonomes}~(ATU), d'autre part, les éléments plus lâchement connectés -- comme les chapitres -- comme \textit{unités textuelles semi-autonomes}. (SATU). Dans ce contexte, l'autonomie textuelle est atteinte lorsqu'un mot d'une unité textuelle et le mot des unités suivantes ou précédentes ne peuvent pas être classés comme cooccurrents, comme les poèmes, parce qu'ils ne sont pas liés du point de vue narratif, thématique ou discursif. Pour SATU, le caractère semi-autonome peut être discuté, mais les chapitres ou les livres présenteraient certainement un certain niveau d'autonomie discursive les uns par rapport aux autres, tout en permettant une progression discursive. Dans les corpus latins tels que ceux qui suivent les directives d'encodage de CapiTainS pour la TEI \footcite{clerice_capitains_2015}, chaque texte a été soigneusement annoté avec un schéma de citation, tel que Book~$\rightarrow$~Poem~$\rightarrow$~Line, par l'équipe éditoriale de leur corpus. 

% Environ cinq à dix pages

% L’importance du choix de CapiTainS, rerédaction de l’article précédemment écrit

\section{Constitution d’un corpus sur l’expression de la sexualité} % Environ dix pages

\subsection{Le choix d’une source: Adams et histoire des tentatives de vocabulaires de la sexualité latine ?} 

\subsection{Conséquence du choix de Adams}

% TLL et problème du TLL chez Adams

\subsection{Méthode de compilation}


% Les données épigraphiques: pourquoi non.
%% Difficulté de lemmatisation
%% Présence relativement faible

% Les bornes “chronologiques” du corpus
% Notes sur quelques données absentes

\subsection{Corpus résultat: format, métadonnées, limites}

% Format et tags: interprétation



% Représentativité
% Les périodes creuses ?

\subsection{Analyse du corpus sexuel final (stats et autres)}

% Représentation et sur-représentation des auteurs
%% L’angle mort de l’étude d’Adams: la période Chrétienne sous-représentée ?
% Une analyse lexicométrique du corpus: termes les plus fréquents “hors” stopword ?
% Création d’un corpus négatif:
%% Spécificité des termes du corpus: nombre de termes commun (lemme comme formes, stop-words inclus)
%% Nombre de textes très communs (% de lemmes communs importants) via une analyse à la Tesserae ou autre ?

\section{De la bande magnétique à l'exemplier numérique: changements épistémologiques et apports du numérique \enquote{libre}}

- le saut épistémologique introduit par les corpus électroniques
- repenser la catégorie de corpus