\chapter{Constitution du corpus}

\section{Une histoire des corpus latins numériques}

Le travail sur la langue latine nécessite \textit{de facto} des corpus, et \textit{a priori} en nécessite des numériques s'il s'agit d'une approche computationnelle. Si la tradition papier des corpus académiques des Teubner ou des Belles Lettres s'inscrira bientôt dans leur troisième centenaire\footnote{Si l'éditeur Teubner semble s'attaquer dès les années 1810 à l'impression d'ouvrages philologiques, la \textit{Bibliotheca Scriptorum Graecorum et Latinorum Teubneriana} ne voit le jour \enquote{qu'en} 1849. Elle prédate les deux autres collections généralistes majeurs, la collection \textit{Oxford Classical Texts} et les \textit{Belles Lettres}. \cite{andre_cent-cinquante_1974}}, l'histoire des corpus littéraires numériques n'a fêté que très récemment son cinquantenaire avec les prémices du \textit{Thesaurus Linguae Graecae}.

Aussi, nous proposons de revenir sur les cinquante dernières années de numérisation et de mise à disposition des textes latins, principalement des textes littéraires. Nous proposons un découpage en trois périodes de cette révolution numérique des corpus: la première concerne l'apparition des disquettes et CD de corpus qui émaille les décennies 1960 à 1990; la seconde (1995-2005) concerne l'apparition en ligne de ces premiers corpus, mais aussi une autre forme de révolution, celle des corpus non académiques; la troisième (2005-aujourd'hui) concerne enfin l'expansion du numérique comme version fondamentale des corpus et l'apparition de \enquote{méga corpus}.

Il faudra cependant commencer cette introduction au chapitre par un avertissement: la documentation disponible sur la publication des corpus numériques est presque inexistante, souvent de seconde main, à travers de rares témoignages ou d'encore plus rares citations et ne permet souvent pas de retrouver avec toute l'exactitude souhaitée la première date de publication de tel ou tel ouvrage. Jusqu'à aujourd'hui, la citation des corpus numériques n'est pas entrée dans les usages, tout comme la citation des oeuvres quand on en fait le commentaire: rares sont les chercheurs qui spécifient en bibliographie l'édition précise qu'ils ont utilisée quand ils mentionnent Virgile ou Martial. Aussi, nous nous excusons d'avance  si des informations présentées ici sont inexactes, si des corpus oubliés le sont, et nous invitons grandement notre champ à capturer rapidement cette histoire, l'archiver avant qu'il ne soit trop tard: si les décades 70 et 80 ne sont pas très loin, elles semblent bien floues sur le plan de l'histoire des corpus\footnote{Il nous semble propice, pour qui voudra, de s'intéresser à une histoire orale des projets fondateurs, à une recherche en archives pour chacun de ces projets, avant qu'il ne soit trop tard.}.

\subsection{Les \enquote{incunables} du numérique}

Nous nous intéressons ici à la naissance des corpus numériques littéraires, ayant pour vocation d'être lus ou utilisés pour des recherches dès leur conception numérique. À cette fin, nous excluons les travaux d'annotation linguistique et de construction de concordanciers de R. Busa ou du LASLA  car ils avaient des ambitions plus spécifiques et ne proposaient pas comme but premier de pouvoir lire le texte\footnote{Mais nous en parlerons plus tard, \textit{cf.} \ref{lemmatisation:concordanciers}}. Or, il est difficile comme nous le disions plus haut de savoir à quand remontent les premières productions de corpus.

% La documentation d’époque et de première main est très pauvre sur ces outils (date, recherche en cours), on les retrouve principalement dans des reviews

\subsubsection{Années 60, années 80: premiers corpus, premiers CD-ROMs}

Le corpus littéraire le plus ancien dont il est fait mention est celui financé par le \textit{National Endowment for the Humanities} (NEH) et cité par Theodore F. Brunner dans son article rétrospectif de 1993 centré sur la recherche états-unienne \textit{Classics and the Computer}\footcite{brunner_classics_1993}. En 1968, Nathan Greenberg et John. J. Bateman obtiennent un financement de la NEH de 19.800\$ \footcite{noauthor_neh_2018} complété par 40.000\$ de financeurs secondaires, dont IBM\footnote{D'après \cite{brunner_classics_1993}: le \textit{Digital Computer Laboratory} de l'université d'Illinois, the \textit{Kiewit Computation Center} du Dartmouth College, la \textit{National Science Foundation}, la fondation Ford et l'entreprise IBM donc.}. Avec ce dernier, ils organisent une école d'été titrée \textit{Summer Institute in Computer Applications to Classical Studies}\footnote{L'équivalent de 59~800\$ au 31 janvier 1968 est de 479~745\$ en août 2021 d'après le calculateur d'inflation du \textit{Bureau of Labor Statistics}, \cite{noauthor_cpi_nodate})}. Cet événement donne naissance à un corpus d'une vingtaine d'oeuvres grecques et latines plus ou moins complètes: on y trouve à côté des classiques homériques des morceaux d'oeuvres, parfois inattendus, dont la découpe est particulière, tel le poème 64 de Catulle qui est édité seul, trois oeuvres de l'\textit{Appendix Vergiliana}, les livres I, IV, IX et XII de l'\textit{Énéide}. En dehors d'un rapport, ce corpus ne semble pas avoir eu une vie particulièrement riche, ni de nom d'ailleurs: il est pris en charge par l'\textit{American Philological Association}, est dupliqué à la demande par des institutions, mais très vite se voit couper de tous fonds supplémentaires, là où, comme le note 20 ans plus tard Brunner, le fond des monographies n'est pas touché\footcite{brunner_classics_1993}.

Au début des années 70, nous trouvons la trace d'un seul autre corpus, celui du \textit{Thesaurus Linguae Grecae} (TLG) dont les prémices remontent à 1971\footcite{brunner_classics_1993} et dont la naissance est actée en 1973\footnote{Parmi les articles cités par T. F. Brunner lui-même sur la fondation du TLG, au moins un est indisponible en France: \cite{hugues_homer_1987}}. Si son nom est dérivé d'un projet humaniste du 16\textsuperscript{e} siècle et est en écho à celui du dictionnaire \textit{Thesaurus Linguae Latinae} (TLL), il ne s'agit que d'une simple inscription dans une tradition des grands travaux humanistes: à contrario des deux derniers, ce projet se veut dès les premières conférences un corpus de texte et non un thésaurus, un dictionnaire fortement enrichi. Les premières versions du corpus voient rapidement le jour pour atteindre 61 millions de mots en 1988\footcite{brunner_overcoming_1988}, via une externalisation de la copie \enquote{manuelle} des volumes en Corée du Sud (1972-1980) puis aux Philippines\footcite[p. 111]{helgerson_cd-rom_1988}. Ce corpus pose une difficulté de taille, à savoir son alphabet: en 1972, seuls les caractères ASCII\footnote{\textit{American Standard Code for Information Interchange}} existent informatiquement, ils sont au nombre de 128 et couvrent les nombres, les caractères latins hors diacritiques et les signes de ponctuation. Il faut trouver une solution pour les caractères grecs, et c'est un certain David W. Packard qui trouve une méthode pour encoder ces derniers, le BetaCode. Cette méthode de transcription, dont il reste des traces encore aujourd'hui dans des fichiers de Perseus, propose l'encodage des diacritiques via les signes de ponctuation: ainsi, la parenthèse \texttt{)} remplace l'esprit doux, tandis que le pipe \texttt{|} représente le iota souscrit, par exemple, \textgreek{αναλαβόντες δὲ καθ᾽ ἕκαστον} donne \texttt{analabo/ntes de\ kaq`e(/kaston.}


Présent à la réunion de fondation du projet TLG et résolveur du problème d'encodage, David W. Packard n'est pas seulement important pour ce dernier: il fonde le \acrfull{phi} (\acrshort{phi}) en 1987\footcite{helgerson_cd-rom_1988}, institut ayant pour visée de produire des corpus, dont un équivalent latin du TLG\footnote{Le corpus latin n'est qu'un des multiples corpus du PHI, même si l'on utilise souvent PHI uniquement pour se référer au corpus latin.}. Docteur en langues anciennes depuis 1967 et spécialiste des tablettes en linéaire A, il est nommé en octobre 1968 comme membre du \textit{Special Committee for Computer Problems} aux côtés de N. A. Greenberg, Stephen Waite, William H. Willis et Robert Dyer, qui en est le président. La première version CD-ROM apparait en 1991 (PHI\#5), les versions précédentes n'ont pas laissé beaucoup de traces et il existe un certain flou autour de la chronologie: un article de 1991 de J. Raben mentionne qu'il est \enquote{en cours de direction par David W. Packard}\footcite{raben_humanities_1991}, un article de S. Hockey parle d'environ 8 millions de mots en 1994\footcite{hockey_electronic_1994}. Il semble qu'un premier CD de textes latins, en particulier de la \textit{Bible}, soit publié rapidement, dès 1987 dans le contexte d'un projet annexe du \textit{Center for Computer Analysis of Texts} (CCAT)\footcite{groves_tovs_1990, cornell_greek_1989}.

Le fossé temporel qui sépare les deux \enquote{premiers} projets américains encore présents s'explique d'une part par le coût que représentent ces projets, d'autre part par le manque d'équipement informatique au début des années 1970. Il faut attendre l'avènement du \textit{micro-computer} (micro-ordinateur en français, terme tombé en désuétude pour ordinateur tout simplement ou bien même PC) et de l'Apple II par exemple pour voir une montée de l'équipement informatique. Plus encore, le marché de l'informatique se popularise avec l'arrivée des interfaces utilisateurs graphiques (\acrshort{gui}), notamment à travers l'Apple Lisa (1983)\footcite{noauthor_history_2021} ou l'Apple Macintosh (1984) ou encore leur équivalent DOS et Windows déployés par IBM. Et au-delà même du micro-ordinateur, c'est le standard CD-ROM qui apparait et permet de partager des données beaucoup plus importantes en 1984\footnote{Le standard est créé plus tôt, mais ne s'applique d'abord pas aux données.}. Certains historiens parlent d'une montée en puissance, dans le secondaire comme dans le milieu académique, de l'usage des ordinateurs en classe\footcite{simkin_introduction_1989, latousek_fifty_2001}. Mais le changement s'opère sur toute la population américaine: d'après un rapport de 1999\footcite{kominski1999access}, on voit doubler  entre 1984 et 1989 le nombre de foyers américains ayant un ordinateur, une augmentation de 64\% de l'usage de l'ordinateur à l'école pour les 3-17 ans, de 41.5\% pour les 18+ à l'école\footnote{Par déduction, il devrait s'agir principalement du milieu universitaire.} et de 33\% au travail (\textit{cf.} Table \ref{tab:computer-ownership}).

\begin{table}[ht]
\centering
\begin{tabular}{l|rrr}
                                               & 1984 & 1989 & 1993 \\ \hline  \hline
Foyer avec un ordinateur                       & 7.9  & 14.4 & 22.8 \\ \hline
3-17 ans ayant accès à un ordinateur à l'école & 28.0 & 46.0 & 60.6 \\
18+ ans ayant accès à un ordinateur à l'école  & 30.8 & 43.6 & 53.8 \\
18+ ans ayant accès à un ordinateur au travail & 24.6 & 36.8 & 45.8 \\ \hline
\end{tabular}
\caption{Niveau d'accès et d'usage en \% des ordinateurs aux États-Unis sur la décennie 1984-1993, d'après l'\textit{U.S. Census Bureau, Current Population Survey, October 1984, 1989, 1993} repris par \cite{kominski1999access}}
\label{tab:computer-ownership}
\end{table}

Il faut comprendre à quel point l'histoire des projets américains est intimement liée aux grandes entreprises du domaine de l'informatique. Si elles sont souvent présentes en financement suite à des demandes, comme IBM sur le co-financement NEH de 1968-69, ou Apple comme nous le verrons pour Perseus, elles sont aussi présentes à travers les réseaux sociaux de la côte ouest. En effet, qu'il s'agisse de PHI ou de TLG, des enfants de grands patrons sont à la source du financement des projets: ainsi, David W. Packard (\acrshort{ucla}) est le fils du co-fondateur de Hewlett-Packard et utilise cette ressource pour financer le PHI; de son côté, Marianne McDonald finance le TLG alors qu'elle n'est qu'étudiante en licence grâce à son père, patron de la \textit{Zenith corpustion}, entreprise méconnue en France, mais importante pour les États-Unis puisqu'elle y commercialise alors télévisions et bouquets de chaînes. Le financement de ces entreprises (1 million de dollars offerts par M. McDonald, en 1972, soit environ 6,656 millions de dollars d'août 2021) est constant et semble \enquote{inévitable} pour ces projets jusqu'à la fin des années 80.

\subsubsection{La lente apparition du projet Perseus}

Dans les années 80, en parallèle du développement de PHI et du TLG, un autre futur mastodonte du corpus en lettres classiques commence à se formaliser: Perseus. Mais l'histoire de Perseus ne commence pas comme l'histoire d'un concurrent à PHI et au TLG, mais bien comme un ajout à ces derniers. 

% Contexte de la création de Perseus: le project HCCP
En effet, en 1982, Gregory R. Crane, alors doctorant à Harvard, ainsi que Neel Smith, Kenneth Morrell et Elli Mylonas cherchent à améliorer l'écosystème pour l'étude des langues anciennes sur plateforme informatique. À cette période-ci, il faut comprendre que le TLG n'est disponible que sur sa propre plateforme matérielle et logicielle, à savoir l'Ibycus, développé spécifiquement par D. W. Packard et financé par HP. Or, il s'agit aussi de la période de \enquote{standardisation} de la programmation, notamment à travers le développement d'Unix et de ses clones (dont GNU). Dans un article rétrospectif sur l'histoire du champ, G. R. Crane\footcite{schreibman_classics_2004} parle du développement du moteur de recherche pour le TLG permettant de faire usage des données du TLG. En effet, dès 1994, alors qu'E. Mylonas présente le projet Perseus\footcite{mylonas_perseus_1993}, elle intègre l'histoire de Perseus dans son rapport au TLG: l'équipe historique de Perseus s'intéresse d'abord à produire des ajouts pour le TLG, dont un \enquote{puissant moteur de recherche plein-texte}\footnote{\textit{\enquote{... spawned at Harvard a software project which developed a powerful full-text retrieval system.}}}. Dans son ouvrage massif \textit{Bits, Bytes and Biblical Studies} de 1986\footcite[p. 598]{hughes_bits_1987}, J. J. Hughes parle du \acrfull{hccp} (\acrshort{hccp}) qui cherche alors à développer pour UNIX et en particulier pour Mac un nouveau système complet autour de l'édition, de l'entrée de données et de la recherche plein texte. À cette époque, Perseus ou l'HCCP sont financés tour à tour par IBM, Apple (y compris à travers une stratégie globale d'adoption de la firme à la pomme par Harvard) et Xerox du côté des entreprises.

% HCCP et Morpheus
La fin des années 1980 montre encore l'intérêt d'abord de l'équipe Perseus pour l'amélioration de l'environnement de travail - en grec ancien uniquement pour le moment. Le TLG et le PHI-CCAT proposent depuis quelques années alors un outil pour la lemmatisation et l'annotation morphologique du grec ancien, appelé MORPH et développé encore une fois par David. W. Packard en assembleur puis dans son propre langage de programmation, l'IBYX\footcite[p.554-555]{hughes_bits_1987}. L'équipe de Crane propose donc d'abord d'améliorer MORPH et développe Morpheus, qui gère désormais les accents et les dialectes\footcite{mylonas_perseus_1993} et propose une formalisation par règle de la langue grecque. L'ensemble se repose sur un dictionnaire central, l'\textit{Intermediate Liddell-Scott Lexicon}, ce qui permet donc aux utilisateurs d'avoir un référentiel de lemmes consultable et navigable.

% De l'HCCP à Perseus: compléter le TLG
Et c'est à travers l'ensemble de ce travail autour de l'infrastructure logicielle que l'HCCP finit par devenir le \textit{Perseus project}. Commencé en 1990, le projet ne vise pas à concurrencer PHI et TLG. G. R. Crane et son équipe affirment dès le départ cette absence de concurrence: \enquote{\textit{The Perseus Project, with its broad range of materials, was designed to complement the textual focus of the TLG}}\footcite[p. 134]{mylonas_perseus_1993}. Il va donc chercher à compléter ce dernier en apportant de nouvelles informations, comme - pour la première fois - des traductions des textes classiques et des ressources graphiques. Le premier Perseus vise ainsi à accompagner d'images les corpus textuels disponibles jusqu'ici - on parle alors de 10~000 images à obtenir entre 1990 et 1993 - compilées avec les textes sur \enquote{\textit{compact disks and video disks}}.

% Textes et traductions
Si l'information textuelle en langue originale n'est pas avancée comme étant au centre du projet Perseus, l'équipe promet tout de même d'amasser 100 MB de données d'ici la fin du projet. Le corpus original se veut centré autour du Ve siècle avant notre ère avec des incursions vers d'autres classiques et accompagné de traductions, anciennes, modernisées et modernes fournies par des partenaires\footnote{Les premiers auteurs mentionnés sont \enquote{Eschyle, Sophocle, Hérodote, Pindare, {[...]} Pausanias, Pseudo-Appolodore, les vies grecques de Plutarque, {[...]} Homère, Aristophane, les orateurs attiques, Thucydide, la poésie élégiaque et lyrique, Platon et un peu d'Aristote {[sic]}. Des morceaux intéressants de Diodore de Sicile et Strabon} seront ajoutés plus tard. \cite{mylonas_perseus_1993}}. Cette sélection, plus restreinte que celle du TLG, vise alors les étudiants et non les chercheurs: il s'agit d'accompagner les hellénistes en formation et les non-spécialistes - comme les historiens - dans la lecture des textes en proposant des versions numériques alignées avec leur traduction \footnote{\enquote{\textit{The choice to include translations is to allow students and other scholars who are not fluent readers of Greek to work {[...]} and to broaden the circumstasnces in which Perseus will be consulted.}}, \cite[p. 136]{mylonas_perseus_1993}}. La création des données textuelles est alors faite par copie au clavier, les technologies d'OCR étant trop génératrices d'erreurs à l'époque\footnote{L'équipe a testé l'OCR au début des années 90 et estime alors que le temps de correction n'est pas plus intéressant qu'une copie manuelle.}.

Les années 80 sont des années particulièrement riches technologiquement, nous l'avons vu, et en particulier en termes de standardisation de l'écosystème informatique: partager information et code entre entreprises et consultants, entre chercheurs ou entre projets devient une problématique importante. Et ces années-là voient apparaitre un nouveau langage, le SGML\footnote{\textit{Standard Generalized Markup Language}}, un langage à balise destiné à structurer l'information textuelle plus facilement et adopté par l'\acrfull{iso} (\acrshort{iso}) en 1986. Un an plus tard, 32 chercheurs en sciences humaines et sociales se rencontrent au Vassar College de Poughkeepsie, dans l'état de New York , et posent des principes d'interopérabilités, qu'ils nomment alors les \textit{Poughkeepsie Principles}\footcite{vanhoutte_introduction_2004}. Ces principes\footcite{noauthor_design_1988}, au nombre de 9, définissent les lignes directrices pour la fondation des \textit{Text Encoding Guidelines} et commencent ainsi par celui d'obtenir un \textit{standard} pour l'échange de données dans le contexte des recherches en sciences humaines. Cet objectif est au centre de ce qui devient, en 1990, la \textit{Text Encoding Initiative} et sa première version des \textit{guidelines} qui visent à encadrer la manière d'encoder l'information textuelle et ses métadonnées. La \textit{Text Encoding Initiative} vise alors à \enquote{fournir des \textit{guidelines} explicites qui définissent un format textuel approprié au partage de données et à leur analyse; le format doit être indépendant du point de vue matériel\footnote{Comme nous l'avons vu, la richesse matérielle à l'époque fait qu'il existe encore de grandes possibilités de conflits entre différentes manière de gérer des données à cause de l'implémentation physique du principe informatique.} et de celui du logiciel, rigoureux dans sa définition des objets textuels, facile à utiliser, et compatible avec les standards existants. On attend du SGML de fournir une base adéquate pour ces \textit{guidelines}}\footnote{\enquote{\textit{The primary goal of the Text Encoding Initiative is to provide explicit guidelines which define a text format suitable for data interchange and data analysis; the format should be hardware and software independent, rigorous in its definition of textual objects, easy to use, and compatible with existing standards. The \acrlong{sgml} (\acrshort{sgml}) is expected to provide an adequate basis for the guidelines. }}, \cite{noauthor_design_1988}}.

Il est alors compréhensible, devant cette révolution de l'encodage du texte, de voir le projet Perseus adopter SGML dès sa conceptualisation\footcite[p. 138]{mylonas_perseus_1993}, bien qu'aucun de ses membres fondateurs n'ait participé à la réunion de Poughkeepsie\footnote{Il est intéressant de voir que l'article publié en 93 ne parle pas de TEI directement, mais bien de SGML, tout au plus est renvoyée en notes et bibliographie une mention du travail de Lou Burnard sur la TEI, au même titre que de l'utilisation de la technologie SGML par le département de la défense, \textit{cf.} \cite[notes 8 et 9, p.~155]{mylonas_perseus_1993}}. Ils adoptent en effet ce standard dès le départ comme format d'archivage en estimant que seul un format d'archivage standardisé permettra de survivre aux évolutions technologiques et en particulier de survivre au logiciel utilisé à l'époque, à savoir \textit{Hypercard} sur Mac: plus de vingt ans plus tard, les corpus originaux de Perseus sont toujours disponibles, on ne peut que confirmer cette intuition. Mais cette opportunité prise, il reste aussi à l'équipe de traduire en SGML les pratiques de mise en page et d'édition du domaine de l'antiquité, à savoir ses modes de citation en structures logiques ou éditoriales (chapitre, section, vers, pages de \textit{Stephanus} pour Platon), afin de ne pas rompre avec cette tradition philologique: le passage de l'imprimé au numérique permet ainsi de traduire les informations fournies par la mise en page en métadonnées sur le texte. Ainsi, en dehors de ces informations éditoriales, une annotation supplémentaire dans le SGML de la métrique, des \enquote{types de discours dans la prose historique et rhétorique}, les noms des intervenants dans les pièces est considérée dès la conception du projet\footcite[p. 137]{mylonas_perseus_1993}.

Bien que les données textuelles soient ultimement celles qui nous intéressent pour notre travail, ignorer la partie non textuelle du projet Perseus à sa fondation ne permettrait pas de comprendre en quoi ce projet ne se pose - au départ - pas comme un concurrent au TLG. Pour les ressources sur l'archéologie, Perseus souhaite en effet se constituer comme une \textit{bibliothèque}, avec une couverture dont la sélection est le résultat d'un \enquote{\textit{opportunisme guidé}}\footcite[p. 145]{mylonas_perseus_1993}. L'objectif est de rassembler, pour la première fois sous une forme numérique, un outillage pédagogique et de recherche permettant d'aborder une grande variété d'objets et de thèmes pour la Grèce ancienne: cela comprend photographies, dessins, plans, mais aussi descriptions ou textes d'introduction thématique traitant de la sculpture ancienne par exemple. Selon les fondateurs du projet\footcite[p. 143]{mylonas_perseus_1993}, par manque d'expertise entre autres et de concurrents numériques prédatant ce projet, l'équipe de Perseus va chercher à rassembler les principes de trois modèles:
\begin{itemize}
    \item ceux d'une archive photographique, avec des descriptions sommaires qui se concentrent sur la description de l'image elle-même;
    \item d'une base de données ou d'un catalogue muséal ou de fouilles, centré sur l'objet et concentré sur la description de propriétés, mais sans projet éditorial;
    \item d'une publication plus enrichie, du type \enquote{catalogue archéologique multi-volumes}  proposant à la fois des volumes de textes et des planches, mais nécessitant une plus grande sélection, et donc omission, d'objets.
\end{itemize}
Le résultat de cette sélection doit offrir une modélisation suffisante pour découvrir, se former, et enseigner. Son implémentation suit encore les principes de SGML pour les contenus textuels et une modélisation complexe des métadonnées permettant formellement un enrichissement par des contributeurs extérieurs à l'avenir\footnote{\enquote{\textit{Perseus cannot possibly foot the costs of assembling the quantities {[of information ...]}; there, we must design a system that will not merely permit but encourage collaboration.}}, \cite[p. 148]{mylonas_perseus_1993}. Nous verrons plus tard que cet objectif deviendra un \textit{leitmotiv} de G. Crane à travers les évolutions de Perseus.}

\subsubsection{Période manquante ? L'apparition de la patristique numérique}

% Introduction du CETEDOC et du CLCLT2
Jusqu'au projet Perseus, l'ensemble des efforts se font sur les périodes classiques, canoniques, celles du \enquote{bon grec} ou du \enquote{bon latin}, des orateurs ou dramaturges, des poètes épiques, celles des oeuvres que l'on étudie pour l'agrégation en France. Les pères de l'Église sont rarement inclus dans les projets, et s'ils le sont, ils sont sous-représentés et n'y apparaissent alors que partiellement. Cette scission, entre période chrétienne et période classique, se retrouve aussi dans le travail des corpus: si on trouve le LASLA à Louvain pour s'occuper de la période classique (jusqu'à la fin du Ier siècle environ), un autre laboratoire se fonde en 1968 sous la direction de Paul Tombeur pour traiter des données \enquote{médiévales}: le Centre de Traitement Électronique des Documents, ou CETEDOC\footcite[p. 70]{gueret_analyse_1977}. Ce centre se concentre pendant vingt ans à la production de données similaires à celles du LASLA, des concordances, des données lemmatisées. En 1984\footcite{iogna-prat_centre_1984}, le centre se compose \enquote{d'un ingénieur informaticien, un analyste informaticien, une secrétaire, une (demi-!) {[sic]} assistance, deux universitaires dont P. Tombeur {[...]} et des vacataires}. Les services que propose le centre incluent alors la reprographie de thèses, la mise à disposition des données collectées et l'accueil de chercheur pour faire traiter des textes \enquote{à la mode} du CETEDOC.

% Du CETEDOC au CLCLT
%  Rappel que Tombeur était à Poughkeepsie
Les années 80 représentent cependant un tournant pour le centre: la question de la mise à disposition de corpus \enquote{médiévaux}~-~il faut entendre ici pères de l'Église et textes médiévaux en général~-~se fait de plus en plus pressante par son absence des corpus principaux, PHI et TLG. En 1981, à Liège, au congrès mené par le LASLA sur \enquote{l'informatique et les sciences humaines}, Paul Tombeur parle alors de publier un \textit{Thesaurus Patrum Latinorum}, englobant les textes chrétiens latins et les textes médiévaux publiés dans les collections \textit{Corpus Christianorum, Series latina} et \textit{Continuatio Mediaevalis}\footcite{tombeur_constitution_1981}. Le directeur du centre est présent à Poughkeepsie en 1987 et signe l'appel, répétant ainsi ces nouvelles ambitions\footcite{burnard_report_1988}. Et de fait, en 1991 sort chez Brepols la \textit{CETEDOC Library of Christian Latin Texts on CD-ROM}, ou CLCLT, une base de données comprenant 21 millions de mots et l'équivalent de 300 volumes imprimés\footcite[p. 90]{bucknall_review_1994}. Si T. Bucknall la compare dès lors avec les bases PHI ou TLG, la situation est légèrement différente: le CLCLT est avant tout une base à interroger plus qu'un corpus à lire, et c'est ainsi qu'il est implémenté.

% Possibilités et limites du CLCLT2: impression de 30 résultats par exemple
La base CLCLT consiste alors en une interface donnant accès à un système de recherche (par forme, par groupe de formes, par forme partielle, par proximité entre formes), mais repose sur un séquençage du texte en \textit{sententiae}, des phrases que les éditeurs ont produites dans leur édition. Choix regrettable si l'on en croit les comptes-rendus de l'époque, tant elle produit des disparités: en effet, \enquote{les uns {[éditeurs]} paraissent préférer des phrases très longues {[tandis que]} les autres s'appliquent à hacher menu le discours}\footcite{gryson_nouvelle_1992}. Et de ce séquençage dépend alors bon nombre de recherches qui ne peuvent inclure les éléments de \textit{sententiae} voisines. La base est cependant munie d'un très grand nombre de métadonnées, de notes critiques sur le texte, sur son authenticité et son attribution par exemple. On peut y lire les textes, bien que l'on ait vu plus confortable: les oeuvres ne comprennent pas d'index, et si l'on veut lire le chapitre 14 d'un long ouvrage, il faudra passer de page en page manuellement. Le logiciel est uniquement disponible sur PC, en particulier sous DOS à l'époque. Les résultats sont imprimables, mais les comptes-rendus divergent: si R. Gryson semble indiquer l'absence de limite lors de l'impression\footcite[p. 421]{gryson_nouvelle_1992}, les autres sources, dont T. Bucknall\footcite[p. 94]{bucknall_review_1994}, semblent s'accorder sur une limite pour le téléchargement ou l'impression à 30 lignes consécutives de texte.

% L'apparition de la PLD
Le vide laissé par PHI et le TLG ont cependant intéressé d'autres éditeurs que Paul Tombeur, puisqu'un concurrent au CLCLT apparait au même moment: la \textit{Patrologia Latina Database}, ou PLD, éditée par l'entreprise Chadwyck-Healey. Basée sur une numérisation de la patrologie de Migne, une somme des textes chrétiens du IIe au bas moyen-âge éditée au XIXe siècle économique\footnote{\enquote{Migne présente sa Patrologie comme une \textit{bibliotheca oeconomica} et {[...]} comme étant du bon, bon marché}, \cite[p. 228]{tombeur_pld_1993}}, elle propose sous une interface remaniée \textit{DynaText} et à partir de fichiers en SGML TEI\footcite{smith_dynatext_1993} de lire ou de chercher à l'intérieur d'un immense corpus sur 5 CD-Roms. C'est à notre connaissance le premier projet commercial en SGML TEI, et le premier très large projet qui utilise cette technologie. Contrairement au CLCLT, la PLD fonctionne sous les OS principaux de l'époque (Mac, \enquote{UNIX avec X-Windows} et Windows\footcite{smith_dynatext_1993}). 221 volumes de la PL sont repris et acceptent une recherche plein texte plus ou moins équivalente à celle du CLCLT. Les métadonnées des premières versions sont par contre particulièrement pauvres: les périodes sont divisées sommairement en deux périodes, \textit{medieval}, avant 1500, et \textit{modern}, après cette date. Elle permet par contre la lecture ciblée de documents, et ne nécessite pas, comme le CLCLT, de faire défiler manuellement les contenus. Enfin, contrairement au CLCLT, elle permet l'export du SGML et contient l'apparat critique des textes qu'elle comporte.

Cette collision littéraire et temporelle conduit les deux bases de données à être comparées et à faire naître des controverses. D'abord, car les deux objets ne font pas le même prix: la PLD est annoncée originellement pour 50~000\$ tandis que le CETEDOC l'est pour 3~800, avec des mises à jour bi annuelles\footcite{bucknall_review_1994}. Le prix de la PLD semble varier beaucoup, y compris suite à la réaction du public: on parle de 70~000\$ quand elle fut annoncée sur bandes magnétiques au début des années 90 et de 45~000\$ en précommande dans l'article de Ron W. Crown\footcite{crown_comparing_2000}, de 27~000£ en 1995 chez R. Gryson\footnote{Avec un taux de change en 1995 d'environ 1.55\$ pour 1£, 41~850\$, d'après \cite{noauthor_british_2021}}, de prix négociés chez certaines petites bibliothèques aussi bas que 5~000\$\footcite[Note 10, p.~108]{crown_comparing_2000} qui la rendent alors hautement compétitive avec le CLCLT. En 1993, une discographie\footcite{pellen_les_1993} nous permet de comparer ces prix: Perseus se vend pour 230\$, le TLG pour 5~860 francs français hors taxe\footnote{Avec un taux de change à 5.66, 1~035\$ d'après \cite{noauthor_france_nodate}}. Avec des prix relativement stables pour les outils cités, un article de Beth Juhl indique un prix de 50\$ par CD en 1995 pour le PHI\footcite{juhl_ex_1995}. À cette époque, l'offre de la PLD est donc plus de dix fois plus chère que toute autre base de données majeure en lettres classiques.

Ensuite, le fond de la controverse dépasse cependant de loin les questions des possibilités des différents outils\footnote{Bien que certaines fonctionnalités de la PLD soient \enquote{discutables}: d'après R. Gryson, les \enquote{titres et sommaires, références scripturaires, appels de notes} font partie du texte dans les résultats, et les recherches en contexte incluent, si le terme est en début d'œuvre ou en fin, le contenu de l'œuvre suivante ou précédente. \cite[p. 148]{gryson_patrologia_1997}} et celles du prix. Le principal reproche fait à la PLD concerne sa source, la patrologie de Migne. Qu'il s'agisse de T. Bucknall ou de R. Gryson, les comptes-rendus sont sévères: la patrologie de Migne n'est pas \enquote{conforme aux exigences de la science moderne}\footcite[p. 147]{gryson_patrologia_1997}: éditions datées du 16e siècle, reprise telles quelles par leur collateur au milieu du 19e siècle, erreurs d'attribution \enquote{inacceptables}, et pire, erreurs d'impressions qui se retrouvent ensuite dans le texte proposé par la PLD, car directement copié, sans vérification, par les équipes de Chadwyck-Healey, occasionnant, en plus de possibles erreurs de copies, une augmentation du nombre de coquilles dans la base. La controverse est clairement lancée par P. Tombeur en 1992 lors de son article introductif au Bulletin de Philosophie Médiévale\footcite{tombeur_informatique_1992} (BPM) qui cherche à donner des perspectives au domaine médiéval dans ses projets numériques, en appelant notamment à ne pas dupliquer les efforts. Après avoir présenté son projet au CETEDOC comme \enquote{ne voulant pas être une simple mise en mémoire des oeuvres {[...]} rassemblées par Migne}\footcite[p. 44]{tombeur_informatique_1992}, il présente en contraste la PLD comme une \enquote{photocopie électro-magnétique de l'oeuvre de l'abbé Migne}\footcite[p. 45]{tombeur_informatique_1992} dont le contenu lui-même est douteux. Un droit de réponse peu avisé de Sir Chadwyck-Healey précise qu'il ne s'agit pas de simples fac-similés (avait-il vraiment compris que P. Tombeur parlait de photographie ?), mais bien de textes recopiés\footcite{chadwick-healey_droit_1993}, ce que re-précise P. Tombeur dans une réponse au droit de réponse\footcite{tombeur_reponse_1993} qu'il fera suivre ensuite d'un article plus large de critique - dans le même volume - de la PLD\footcite{tombeur_pld_1993}. L'affaire semble se clore, dans le BPM en tout cas, en 1994 avec l'ultime réponse d'un membre de la PLD\footcite{jordan_facts_1994}. Si des problèmes techniques sont évoqués, que des annonces publicitaires sont interprétées et réinterprétées\footnote{Par exemple, il ne serait pas sûr que les formes courantes du type \textit{ipse} soient cherchables car constituant des \textit{stop-words}}, le problème revient toujours sur la qualité des données originales. Dans sa comparaison des deux outils\footcite{crown_comparing_2000}, R. W. Crown semble absoudre rapidement les auteurs de la PLD pour recommander l'usage de cette dernière -~à condition que le budget suive~-~car elle ne nécessiterait pas l'usage de sources papiers et formerait un \enquote{véritable "e-Book"}. Si l'on en croit les autres comptes-rendus, et notamment les faiblesses en termes d'attribution des textes et l'historique derrière les éditions, cela semble loin d'être vrai en 1995.

Quoi qu'il en soit, entre 1970 et 1995, on voit apparaître de nombreux projets, dont nous n'avons retenu que les principaux et les survivants, qui cherchent à numériser les corpus, entre autres pour rendre plus rapide le travail des chercheurs. Ces \enquote{incunables}, comme les appelle R. W. Crown\footcite[p.~107]{crown_comparing_2000}, forment alors une évolution considérable, sans véritablement transformer les approches du texte: il s'agit de trouver plus facilement un terme, et en cela, c'est une réussite. Deux exemples d'époques sont souvent cités: John J. Hughes, dans un article de 1986\footcite{hughes_ibycus_1986} cité par L. W. Helgerson\footcite{helgerson_cd-rom_1988}, qui indique qu'une recherche de \textit{\textgreek{διαθεκε}} avait pris 25 minutes sur le TLG pour 1~079 résultats alors même que cette recherche, pour des résultats moins importants, lui avait pris \enquote{la plus grande partie d'une semaine dans les bibliothèques de l'université de Cambridge}; Peter Zahn, en 1992, qui explique comment le CLCLT lui a permis rapidement d'identifier un nouveau fragment d'Augustin en cinq minutes, là où la recherche manuelle lui avait pris cinq jours\footcite[p. 427]{zahn_kirchenvater-texte_1992}.

\subsection{Web, standards et corpus: changement d'échelles, changement de pratiques}

Dès le milieu des années 1990, la connexion à internet commence à rentrer dans les foyers avant de vivre une explosion au début du troisième millénaire, en France comme dans le reste du nord économique. D'après T. Karsenti et G. Clermont, on parle d'un passage de seize à sept cents millions d'internautes dans le monde entre 1995 et 2006\footcite{karsenti_les_2006}. Aux États-Unis, en 1995, les chiffres sont de dix millions de foyers avec un accès internet (et dix-huit millions équipés d'un modem, mais sans connexion\footcite{nw_americans_1995}). Dans les bibliothèques américaines, 25\% d'entre elles fournissent un accès à internet en 1996, mais ce chiffre cache la spécificité du déploiement technique: 96\% des villes de 250~000 à 499~999 habitants et 84\% des villes de plus d'un million d'habitants mettent à disposition des connexions internet dans leurs espaces anciennement réservés au papier\footcite{zumalt_internet_1998}. En France, la pénétration de la technologie est un peu plus lente au démarrage, mais les chiffres de l'\acrfull{arcep} (\acrshort{arcep}) montrent une forte croissance: 1,28 million d'abonnements en 1998, 5,33 millions en 2000 (dont 68~000 xDSL), 12,648 millions en 2005; en 2020, ce chiffre atteint 30,627 millions (\textit{cf.} table \ref{tab:chap1:croissance-abonnements-internets}). Avec cette révolution de l'accès aux contenus \enquote{dématérialisé} vient donc l'ère du corpus sans CD-ROM.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|r|rrrrr}
\hline
                                   & 1998 & 2000     & 2005     & 2010     & 2015     & 2020     \\ \hline
Bas débit (en millions)                         & 1.28 & 5.26     & 3.75     & 0.48     & 0.09     &          \\
Haut débit                         &      & 0.07     & 8.90     & 20.23    & 22.66    & 15.96    \\
Très haut débit                    &      &          &          & 1.13     & 4.21     & 14.67    \\ \hline
Total (en millions)                              & 1.28 & 5.33     & 12.65    & 21.84    & 26.96    & 30.63    \\ \hline
Croissance en 5 ans (sauf 98-2000) &      & 416.45 \% & 237.27 \% & 172.69 \% & 123.42 \% & 113.62 \% \\
Croissance en 10 ans               &      &         &          & 409.74 \% &          & 140.23 \% \\ \hline
\end{tabular}%
}
\caption[Caption for LOF]{Évolution du nombre d'abonnements internet en France d'après l'ARCEP (hors abonnements mobiles)\footnotemark. La décennie 2000-2010 représente une croissance extrêmement importante, démontrant bien la pénétration de cette technologie dans les habitudes des Français.}
\label{tab:chap1:croissance-abonnements-internets}
\end{table}
\footnotetext{\cite{lautorite_de_regulation_des_communications_electroniques_indicateurs_nodate}}

\subsubsection{Du CD-ROM à internet}

En parallèle de la massification vue plus tôt de l'accès à l'ordinateur (\textit{c.f.} table \ref{tab:computer-ownership}), on assiste à la naissance du \textit{Personal Computer} et de monopoles dans le marché des systèmes d'exploitation\footcite{schlender_whos_1990}. En 1983, IBM et Apple représentent environ 40\% des ventes (aux États-Unis), les 60\% restants contenant toute une myriade d'autres OS. En 1990, les PC IBM (dont DOS) représentent 85\% du marché, Apple est second à 5\%. En 1998, 90,5\% du marché appartient à Windows pour les \acrfull{os} (\acrshort{os}), Mac tient bon à 5\% et Linux grimpe légèrement sur ce marché avec 2,1\%\footcite{miles_windows_1999, reimer_total_2005}. Cette croissance et solidification du marché autour de deux interfaces, proches, permet une simplification de l'apprentissage de l'informatique. Et avec la massification -- au moins dans les bibliothèques -- de l'accès à internet, le passage des médias CD-ROM à des sites internet a -- semble-t-il -- été une conversion évidente. \textit{Perseus} -- avant même sa version 2.0 -- s'y convertit dès 1995, la \textit{Patrologia} en 1996, la \textit{Duke Databank of Documentary Papyri}, sorti en 1982 sur bandes, en 1996 aussi. Le CLCLT du CETEDOC  et PHI restent uniquement sur CD-ROM en 1997 parmi les corpus originaux. Pour le premier, ce passage se fait vers le web avec la version 6 en 2005, soit 9 ans après son concurrent direct. P.~Tombeur prend bien en compte internet dans son rapport au \acrfull{bpm} (\acrshort{bpm}) de 1997, mais ne fait aucun lien avec son oeuvre\footcite{tombeur_informatique_1997}, il faudra attendre une mention en 2004 pour voir se préciser une version en ligne\footcite{tombeur_augustin_2004}, qui sera ensuite renommée \acrfull{llt} (\acrshort{llt}) en 2009.

Dans son article de comparaison entre la PLD et le CLCLT\footcite{crown_comparing_2000}, Ron W. Crown mentionne quant à lui la nouvelle interface de la PLD en ligne, mais aussi les nouveaux choix économiques qui l'accompagnent. Contrairement à Perseus, la PLD fait le choix du site web à abonnement, coûtant entre 400\$ par an pour les universités possédant la PLD en CD-ROM et 3~995\$ par an pour les universités aux plus hauts budgets. L'interface ne diffère pas foncièrement de la version locale et permet la lecture des documents. Des autres incunables, le TLG fait le choix du modèle payant ou semi-payant tandis que le PHI, bien que sortant extrêmement tard, entre 2011 et 2015\footnote{Archive.org donne une archive en 2011 du site des oeuvres latines -- que nous avons tendance à conserver -- tandis que la \textit{review} du RIDE donne une sortie en 2015. \cite{daniel_kozak_classical_2018}}, est mis à disposition gratuitement en ligne, mais avec d'importantes restrictions concernant sa réutilisation et son partage. Ainsi, il faut seize ans, de 1995 à 2011, pour retrouver en ligne l'ensemble des incunables. 

% De Perseus à la Perseus Digital Library
\enquote{La seconde génération de ressources électroniques pour lettres classiques}, d'après le titre du premier index proposé par Maria Pantelia\footcite{pantelia_electronic_1994}, commence entre autres par le passage du CD-ROM au web, et réalisé en premier par Perseus, qui semble avoir été le plus rapide à faire cette transition. Dans un article de 1996\footcite{crane_building_1996}, G. R. Crane parle de cette transition -- presque \enquote{indolore} -- d'une application reposant sur des \textit{Hypercards} à un site web\footnote{On ne parle pas d'application web avant quelques années. Une recherche sur Google N-Gram, avec ses défauts, montre une apparition du terme autour des années 2000-2005, avec la naissance du web 2.0}. Selon lui, si le passage a été autant facile, c'est grâce au choix technologique coûteux du SGML (\enquote{Big costs, huge potential, growing benefits}\footcite[p. 7]{crane_building_1996}) pour encoder les textes dès l'origine du projet. Si les utilisateurs, les visiteurs ou \enquote{clients} (au sens web comme au sens mercantile) ne voient peu ou pas l'avantage de ce choix\footcite[p. 8]{crane_building_1996}, il a de fait permis de transformer facilement l'intégralité des données en HTML en un temps record.

Les années 1995 et suivantes ne sont que l'apparition du web comme média, et tout comme celle du CD-ROM, ne permettent d'abord que de faire émerger des outils assez simples. En 1995, l'interface de Perseus est identique à celle du CD-ROM, et donne accès aux données sans modification de l'interaction humain-machine. Il n'est pas facile de savoir quelle quantité de données visuelles a pu être portée dans ce passage sur le web, d'autant que, pour certaines, il est attesté que des problèmes de droits, cédés uniquement pour les versions CD, ont émergé très rapidement\footcite[p.~3]{crane_building_1996}. Cette interface est mise à jour en 1996 en même temps que la version 2.0 sur CD-ROM pour Mac (il faudra attendre 2000 pour une version CD compatible toutes plateformes)\footcite[p.~109]{rockwell_interface_2020}. D'après Rockwell et ses co-auteurs, le passage au web n'aurait pas influencé les ventes des CD-ROM. Nous pouvons attribuer -- hypothétiquement -- cette absence de fluctuation à quatre facteurs:
\begin{itemize}
    \item la relative habitude des bibliothèques d'acheter les CD-ROM et ouvrages mis à jour;
    \item le coût relativement faible du CD-ROM, comparativement aux autres \enquote{incunables};
    \item l'intérêt pour les ressources audiovisuelles qu'il contient et qui ne se trouvent pas sur le web;
    \item le possible élargissement de la base client via le produit d'appel -- involontaire -- que pouvait représenter le site web.
\end{itemize}
Perseus 2.0 est la dernière version CD-ROM de Perseus, contractuellement obligé envers l'université de Yale de produire cette version\footcite[p.~3]{crane_building_1996}. 

Contrairement au CD-ROM, le site web de Perseus permet une mise à jour en continu, incluant de nouveaux textes et de nouveaux domaines. Cet avantage se voit dès la version 2.0 Web, qui se démarque de la version CD-ROM en incluant des données hors champ des études grecques. En effet, dès 1996, Perseus rentre dans une nouvelle phase, celle de la fondation d'une \textit{digital library}: l'équipe de G. Crane remporte un financement de la \acrshort{neh} pour un projet nommé \enquote{\textit{Digital Library on Ancient Roman Culture}\footnote{\textit{Grant ED-20456-96} avec un financement de 190.000\$ sur 3 ans, soit 334~729\$ octobre 2021.}}. Avec ce financement, qui annonce d'ailleurs l'inclusion de ses résultats sur CD-ROM sans que cela n'arrive jamais à notre connaissance\footnote{La description porte l'information suivante: \enquote{\textit{To support the development of a digital library on ancient Roman culture which will serve students of Latin and ancient Rome, and which will be published both on CD-ROM and via the World Wide Web.}}. \cite{noauthor_neh_nodate}}, Perseus étend son travail vers le latin. En 1996 aussi, l'équipe de Tufts obtient de son \textit{Tufts Provost Office and Arts\&Sciences Research funds} un financement pour inclure des oeuvres de la renaissance anglaise (Marlowe, Shakespeare)\footcite{crane_perseus_1998}. Cette situation financière et cet élargissement placent Perseus comme pièce importante de l'espace internet consacré aux sources anciennes, avec des pics de 75~000 visites en vingt-quatre heures\footcite{crane_digital_1998}. En 1998, via un appel de la NEH et de la \acrfull{nsf} (\acrshort{nsf}), le deuxième financement le plus important de l'histoire de Perseus après celui qui le lança en 1987 lui fournit le budget nécessaire pour naviguer vers une version 3.0: l'équipe de G. R. Crane en collaboration avec Nancy Allen du \textit{Boston Museum of Fine Arts} et Ross Scaife de l'université du Kentucky obtient 2,8 millions de dollars\footnote{4~793~456\$ en dollars octobre 2021} pour un projet qui efface même l'héritage gréco-latin du projet dans son titre: \enquote{\textit{A Digital Library for the Humanities}}\footcite{crane_digital_1998}. Ce financement permet l'émergence d'une troisième version en 2000, uniquement web, et correspondant à la fin de la seconde vague de financements de Perseus (\textit{cf.} figure \ref{fig:chap1:perseus_fundings}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/chap1/part1/PerseusFinancements.png}
    \caption{Financements américains connus de Perseus, en millions de dollars non constants, d'après les pages \textit{Grants} de Perseus, le CV de G. Crane et les archives de la Mellon Foundation et de la NEH. On distingue clairement trois vagues (1: fondation, 2: web, 3: expansion) et l'exportation de Perseus vers l'Allemagne pendant quelques années pour un retour après la fin des financements à partir de 2017.}
    \label{fig:chap1:perseus_fundings}
\end{figure}

Au milieu de la décennie 2000, la fin du financement \textit{A Digital Library for the Humanities} et un ensemble d'autres financements permettent la sortie d'une nouvelle version majeure\footcite{noauthor_gregory_nodate}, la 4.0. La 3.0 ayant subi de nombreuses modifications via les besoins émergents pour l'ensemble des financements de la seconde phase, son code n'est plus maintenable. La 4.0 est une remise à plat complète du projet Perseus avec un passage vers le langage Java pour le fonctionnement du site, le passage au XML \acrshort{tei} P4 pour ses ressources textuelles et la mise à disposition pour la première fois de ces dernières en téléchargement libre. Ce changement structurel du \textit{backend} s'opère en 2005, suivi d'une mise sous licence \textit{Creative Commons} de ses sources en 2006 et de son code en \textit{open-source} en 2007\footcite{rockwell_face_2013}. Cette troisième phase marquée par la 4.0 voit l'expansion de Perseus dans le domaine textuel se confirmer: à partir de 2000, aucun financement ne concerne la partie archéologique ou visuelle de Perseus, tandis que se développent une bibliothèque sur la guerre civile (2003), une extension pour les textes arabes (2006), le traitement des entités nommées (2007) ou de la grammaire grecque par \textit{treebank} (2008), etc. Les corpus ont grossi, toujours dans un objectif de mise à disposition des traductions\footnote{Avec comme seule source notre expérience pour le projet Perseus, des statistiques de 500~000 visites sur le site par semaine nous sont parvenues.}. En 2013, alors que G. R. Crane obtient la ``chaire Humboldt pour les Humanités Numériques'' à Leipzig, la troisième phase s'éteint, et peu de financements sont obtenus du côté américain, malgré une attache conservée à Tufts.
% Phase 2 de Perseus: Le latin et le reste, version 3.0 puis 4.0

Un phénomène nouveau accompagne cette apparition du web dans les foyers et bibliothèques: la naissance de corpus produits par des non-spécialistes, transformation numérique de ce que l'érudition locale et les sociétés savantes produisaient et produise en papier, avec la mise à disposition par l'effort de particuliers hors du monde académique de documents, données ou analyses scientifiques. Trois corpus existants encore en 2021 naissent sur la période 1995-1998: \textit{Curculio}\footcite{hendry_curculio_1995}, \textit{LacusCurtius}\footcite{lomarcan_lacuscurtius_1999} et \textit{The Latin Library}\footcite{carey_latin_1998} (1998). Les trois se concentrent en particulier sur la problématique des textes latins, et pour cause, ni Perseus ni \acrshort{phi} ne fournissent alors ces corpus. Ils sont rejoints par des projets francophones dans la décennie 2000, qui correspond à l'explosion de l'accès au web en France et en Europe: \textit{Remacle.org} arrive en 2003\footcite{philippe_remacle_site_2008}, \textit{Latin, Grec, Juxta} de Gérard Gréco en 2006\footcite{gerard_greco_latin_2006}. La particularité des sites francophones et de leurs fondateurs tient en leur carrière professionnelle: tous deux sont professeurs de lettres classiques de formation. Quelle que soit la situation professionnelle de ces créateurs de contenu, ils partagent tous la particularité de réaliser ces projets sans financement propre, en dehors des cadres universitaires, avec parfois une exhaustivité particulièrement importante, comme pour \textit{The Latin Library}, et avec une véritable focalisation sur le latin.

\subsubsection{Les projets nés sur le web}

La fin des années 90 et la première décade des années 2000 voient aussi l'émergence de projets nouveaux, cherchant à installer durablement dans le paysage numérique les lettres classiques. Si nous en parlons peu, car nous ne les utiliserons pas dans notre recherche, les premiers à se développer sur le web sont de loin les projets épigraphiques, dont plus d'une vingtaine est déjà répertoriée par Tom Elliott pour le compte de la société américaine d'épigraphie latine et grecque en juillet 1998\footcite{elliott_links_1998}. Mais les projets littéraires s intéressent aussi à la toile et y naissent.

Le premier mouvement de ces projets nés sur le web est celui de projets qui resteront au niveau du HTML: des sites pour lire des textes, donner accès à ces derniers avant tout. En 1996, par exemple, naît la \textit{Bibliotheca Augustana} d'Ulrich Harsch\footcite{harsch_bibliotheca_nodate}, dont il nous est malheureusement impossible de retrouver le contenu original. En 1998, c'est au tour d'\textit{Itinera Electronica} d'apparaître construite autour de deux axes: un ensemble de cours (sur quatre niveaux: acquisition, maîtrise, transmission et approfondissement) et de ressources textuelles dont la mise en ligne ne semble remonter qu'à 2002, d'après les journaux du site\footcite{meurant_itinera_nodate}. La \textit{Roman Law Library} sort en 2001: il est produit par des historiens du droit, hors du domaine des lettres classiques, fruit d'une collaboration internationale, et cherche à couvrir ``depuis les premiers textes de l'époque royale jusqu'aux compilations de la période byzantine''\footcite{lassard_roman_2001}. Les corpus naissent peu à peu aux États-Unis et en Europe, en latin comme pour les autres langues, tant que le catalogue est complexe à construire\footnote{D'autant que le passage du temps fait disparaître ces corpus et le peu de références faites dans les ouvrages ou articles scientifiques n'aident pas à en conserver la trace.}. Les outils de développement web sans apprentissage du code font leur apparition, les compétences intègrent les institutions peu à peu. En 1995 sort \textit{Vermeer Frontpage}, renommé \textit{Microsoft FrontPage} en 1996, qui permet le développement de site web sans compétences avancées en programmation, via une interface graphique. Dès 1997, on voit apparaître l'émergence de guides\footcite{la1997guide} à destination des non-spécialistes des communautés éducatives. Un projet de bibliothèque numérique de ressources slaves fait clairement mention de l'usage de \textit{FrontPage} dans son élaboration\footcite{deyrup1998character} et celle de son corpus, tandis que d'autres, tel le projet \textit{Journeys in Time 1809-1822} à l'université de Macquarie (Australie), rejettent son usage pour la production d'un code ``plus propre''\footcite[p.~41]{10.3316/informit.752609435027594}. Pour la plupart des premiers projets cités, ils survivent -- c'est ainsi qu'on les connait aujourd'hui -- et se sont enrichis, mais ne sont jamais sortis du contexte des sites web statiques, précompilés en HTML.

Or, la fin 90 et surtout le début 2000 voient des innovations majeures dans le monde du développement web et de la gestion de corpus électroniques. D'abord, les bases de données SQL, et notamment les serveurs MySQL, et le langage PHP voient le jour et dominent rapidement le monde du développement amateur tout en se faisant sa place dans le monde du développement professionnel\footnote{Malgré nos recherches, nous n'avons pas trouvé d'autres sources sur ce sujet que celles que nous citons. Et pourtant, le début des années 2000 voit l'émergence de sites à tutoriel comme celui du \textit{Site du zéro}, la réduction des prix pour l'hébergement de sites, la naissance (et la mort) des salons de discussions pour l'entraide qui favorisent clairement la formation en autodidacte d'une nouvelle génération de développeurs. C'est en tout cas notre expérience de ces années-ci.}. Associé aux serveurs Apache\footcite{smith_lamp_nodate}, facile à mettre en place pour les hébergeurs comme Free en France et d'autres, il devient facilement possible de déployer des sites dynamiques à bases de données relationnelles avec une formation rapide à la programmation. Des outils de publication (\acrshort{cms}, \acrlong{cms}) faciles à installer voient le jour et accompagnent ce mouvement technologique\footcite{purer_php_nodate}. Parmi ces applications plus complexes, on notera l'apparition entre autres de \textit{Musisque deoque}\footcite{gelderblom_musisque_2008}. Dans son compte-rendu, Gelderblom indique qu'il s'agirait du premier corpus -- il parle d'archive -- à intégrer les variantes et l'apparat critique\footnote{``\textit{The important innovation of MQDQ is that it is the first large-scale archive to include [apparatus] for a growing number of texts, and that it also provides effective search tools for them}'', \cite[p.233]{gelderblom_musisque_2008}}. D'autres projets similaires se développent, avec au centre de ceux-ci le développement de bases complexes, avec des technologies avancées comparativement à du pur HTML, comme le CGL\footcite{garcea_corpus_2010}. Ce dernier montre par ailleurs l'inconvénient de cette nouvelle couche de complexité: si \textit{MQDQ} est encore en ligne aujourd'hui, le \textit{Corpus Grammaticorum Latinorum} a complètement disparu -- bien qu'une nouvelle version soit prévue; l'hébergement de simples fichiers HTML et celui d'applications complètes ne posent pas les mêmes défis.

Ensuite, les années 2000 sont aussi celles de l'adoption par les \textit{guidelines} TEI du XML, d'abord avec la TEI P4 en 2001 puis avec la TEI P5 en 2007. La technologie prend de plus en plus de place dans plusieurs champ académiques, la liste des participants au meeting de 2003 montre par exemple cette belle diversité de domaines\footnote{\url{https://tei-c.org/Vault/MembersMeetings/2003-info/mm22.html}}. En 2007, une étape supplémentaire est passée: la portée de la réunion annuelle de la TEI change de forme, passant du nom \textit{annual members meeting} à celui d'\textit{annual conference}, et on ne publie plus la liste des participants à cette réunion\footcite{noauthor_members_nodate}. S'il n'a pas fallu ces changements organisationnels pour que la grammaire TEI soit une promesse attirante, ils en sont autant d'indice que les structures qui adoptent ou tenter d'adopter cette technologie. Dès 2001, T. Nellhaus dresse le portait d'un outil pouvant révolutionner les libraires dans leur mise à disposition de corpus grâce à la standardisation qu'elle implique: pour l'auteur, sa souplesse et sa capacité de représenter des faits précis représentent une opportunité, bien qu'il ne soit pas sans défauts\footcite{nellhaus_xml_2001}. Dès 2002, l'\acrfull{ehess} (\acrshort{ehess}) via le laboratoire en médiévistique  \acrfull{ciham} (\acrshort{ciham}) à Lyon adopte la TEI pour l'édition de sermons et d'autres projets à travers les figures tutélaires de Marjorie Burghart et Nicole Dufournaud\footcite{burghart_edition_2011}. Dès 2002 aussi, l'\acrfull{enc} (\acrshort{enc}) adopte via sa cellule numérique le standard\footcite{poupeau_les_2006}. Chacune de ces institutions évoque la même raison: la TEI, à travers son encodage fin de phénomènes divers (linguistiques, historiques, littéraires, etc.), est un langage pivot permettant de nombreuses sorties et interprétations dont le HTML de lecture -- reproduisant presque les limites de l'imprimé augmenté des liens hypertextes -- n'est qu'une vue. C'est la même raison qui permet à G. Crane de crier victoire quelques années plus tôt au sujet du passage de Perseus au web.

Dans le monde des lettres latines antiques, peu de projets adoptent dans un premier temps cette technologie, en dehors de Perseus qui avait parié dessus dès la fin des années 80.  D'une part, il existe le projet \textit{Hyperdonat} qui est à notre connaissance la première et seule édition scientifique d'une oeuvre latine littéraire classique ou tardive raisonnablement longue\footnote{Il existe quelques extraits ici et là, ou quelques oeuvres courtes comme le texte de Calpurnius dont nous parlons plus bas.} à utiliser le média web et des sources TEI\footcite{bureau2008hyperdonat}. L'usage de cette technologie est d'ailleurs justifié par Bruno Bureau comme le seul moyen d'éditer la base de données que représentent les commentaires de Donat, loin de l'édition d'un texte linéaire que serait celui d'un Victor Hugo par exemple\footcite[La comparaison est la nôtre.]{chaire_de_recherche_sur_les_ecritures_numeriques_exemple_2018}. D'autres tentatives existent cependant: mais même des initiatives aussi prometteuses que la \acrfull{ldlt} (\acrshort{ldlt}), cherchant à simplifier et promouvoir l'édition critique de textes en TEI, n'arrive à proposer qu'une seule édition de texte (Calpurnius) après des années de mise en place. Comme le note d'ailleurs Samuel J. Huskey à propos de son projet, en 2019, ``les vraies éditions critiques sur internet sont encore rares''\footnote{``\textit{truly critical editions on the internet are still rare}'', \cite{huskey_digital_2019}}. De l'autre côté du spectre des projets en TEI, hors des objectifs d'éditions critiques, se trouve aussi le projet italien de la  \textit{Latin Digital Library of Late Antiquity} (DigilibLT)\footnote{\textit{Biblioteca digitale di testi latini tardoantichi}, d'où le LT.} dont l'objectif est de produire un nouveau corpus de textes tardifs, absents de Perseus, sans pour autant en proposer de nouveaux établissements de texte. À partir d'éditions imprimées globalement plus récentes que celles de Perseus, plutôt issues de la période 1950-2000, elle propose une collection cataloguée de textes sur la période du deuxième au huitième siècle, incluant des textes impossibles à trouver par ailleurs sous format numérique, au premier titre desquels on trouve les traités de médecine et de gynécologie. Le \textit{DigilibLT}\footcite{lana_metodologie_2012} prend par ailleurs le même chemin que celui du projet \textit{Perseus} en affirmant l'importance du caractère \textit{open access} et libre de son projet, dont le corpus est téléchargeable dès sa fondation\footnote{Dans son article, Maurizio Lana titre une de ses parties ``\textit{Accesso aperto, licenze Creative Commons, software libero}'' (fr. Accès ouvert, licence Creative Commons, logiciel libre). \cite{lana_metodologie_2012}}. Mais le monde de la TEI latine classique et tardive s'arrête là, du moins pour les oeuvres ``littéraires'' (on inclut les traités de médecine): l'épigraphie a -- elle -- bien adopté l'usage de la TEI et de sa variante Epidoc\footcite{elliott2007epidoc} pour la publication de ces corpus de textes\footcite{bodard2007inscriptions,cayless2010epigraphy}.

\subsubsection{OCR et corpus de masse}

En 2011 et 2012, David Bamman, avec G. Crane\footcite{Bamman:2011:MHW:1998076.1998078} puis avec David Smith,\footcite{bamman_extracting_2012} s'intéresse pour la première fois à un corpus en friche: celui des campagnes de numérisation, majoritairement privées dans son cas précis, et du résultat de la reconnaissance optique de caractères (\acrshort{ocr}) issue de ces documents. Il dénombre, en 2012, 27~014 textes catalogués comme latins dans l'index du projet \textit{Internet Archive}, comprenant tout autant les classiques latins que les thèses et ``commentaires de la philosophie d'Hegel'' produits en Allemagne, mais écrits en latin au dix-neuvième siècle\footcite{bamman_extracting_2012}. Puis, en triant les données, y compris manuellement\footcite{bamman_dbammanlatintexts_2018}, il obtient une liste de plus de onze mille ouvrages comprenant environ 1,38 milliard de mots\footnote{Ce chiffre est à prendre avec une certaine précaution: la méthode de calcul des mots n'est pas précisée et la définition de ce qu'est un mots ici non plus.}. À partir de cette récupération de données, il met à disposition les premiers \textit{embeddings} massifs de l'histoire du latin approché par méthode computationnelle. Cependant, ses données sont problématiques: d'une part, la méthode de vérification manuelle du catalogue n'est pas expliquée; d'autre part, le résultat final contient des documents dont l'\acrshort{ocr} est absolument inexploitable (``\texttt{tkei: SiiimiemfiBgMiffem mvemsfimUttrUffHk rejcijps}'' étant un des exemples de mauvaise texte qu'il cite lui-même).

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/ocrSycophant.png}
    \caption{Récapitulatif de la chaîne de traitement appliquée sur Archive.org}
    \label{fig:chap1:workflow-sycophant}
\end{figure}

La littérature classique commence à être bien couverte: il reste quelques textes impossibles à trouver édités numériquement, par exemple les \textit{Declamatio Minores} de Quintilien ou les auteurs fragmentaires, en dehors des dépôts protégés comme ceux du PHI, mais cela reste marginal. Du côté de la littérature tardive, entre DigilibLT, les \textit{Patrologia Latina} et le CSEL, la couverture tend à être meilleure, même si de nombreux textes ne sont pas édités ou ne sont tout simplement pas encore tombés clairement dans le domaine public\footnote{La question du droit d'auteur sur les éditions de textes anciens est complexe, spécifique à chaque territoire, y compris en Europe, et pose de vrais problèmes d'interprétations, dont certains producteurs de corpus se plaignent, comme Philipp Roelli dans \cite{roelli2014corpus}. Sur le plan national, de nombreuses publications existent, comme \cite{combalbert_lediteur_2015, demonet_confiscation_2018}. Sur le plan international, et celui des textes latins \cite{fischer2017digital, dillen_digital_2016}. Sur des textes plus récents, \cite{dusollier_international_2019}}. Au contraire, la littérature latine médiévale et le néo-latin sont particulièrement absents des corpus en ligne, et leur mise à disposition tarde. Dans ce contexte, le recours à ces dépôts massifs, pour élaborer de nouveaux outils ou offrir de nouvelles approches.

L'OCR ayant progressé depuis 2013, et la méthode n'étant pas totalement claire dans l'approche de D. Bamman, nous nous sommes donc intéressés à l'évolution du dépôt \textit{Internet Archive}. Pour faire cela, nous avons traité les données de cette bibliothèque numérique en cinq étapes (reproduites en figure \ref{fig:chap1:workflow-sycophant}). Après avoir obtenu une liste d'oeuvres classées comme étant en latin par l'\textit{Internet Archive}, nous avons téléchargé l'ensemble des oeuvres cataloguées. Une fois téléchargées, on prend au hasard un quart des lignes de chacune des oeuvres et on applique un classificateur de langue\footcite{salcianu2018compact}. Ce classificateur indique des statistiques pour plusieurs langues s'il a des doutes sur cette classification: on retient alors qu'une ligne est classée comme latine si elle a un score supérieur à 60\% en latin\footnote{Ce seuil a été fixé pour prendre en compte l'existence d'oeuvres bilingues.}. Après ce traitement, on applique un outil \textit{ad hoc}, OCR~Sycophant\footcite{Clerice_OCR_Sycophant_2021}, qui classe les lignes en termes de qualité: lisible ou non lisible. OCR~Sycophant a été développé autour de modèles de classification classiques basés sur des n-grams, et a été entraîné à partir d'un dataset de phrases du corpus Archive.org sélectionnées aléatoirement et annotées à la main: ont été classées comme ``sales'' les données qui n'étaient pas en latin (\textit{eg.} ``\texttt{" Mr Bryce's test, on account of the difficulty of pro-}''), qui étaient illisibles (eg. ``\texttt{"7 „ 7- f Ak. —2 vi rt*- ('wbrf-}'') ou difficilement lisibles (\textit{eg.} ``\texttt{(Hciucuto qucfo quob fwutuinlto}''), ou encore qui correspondaient à des lignes considérées comme trop courtes comme du bruit (\textit{eg.} ``\texttt{5}''). Ce classement disponible, on obtient un pourcentage de lignes estimées propres et on compte les mots de chaque texte (un mot étant considéré simplement comme un élément séparé par un espace).

Ce résultat donne des chiffres absolument prometteurs, tout en demandant une forme de patience. On compte dans les documents disponibles presque 5~000 oeuvres avec une qualité d'OCR à plus de 90\%(\textit{cf.} table \ref{tab:chap1:latin-OCR}) représentant 635 millions de mots. Si l'on ajoute à ce corpus les textes avec plus de 80\% de qualité OCR, on atteint les 1,451 milliard de mots et un peu plus de 17~000 ouvrages. Bien sûr, il existe probablement des doublons dans ces oeuvres. Mais un vrai foisonnement de données existe dans ces dépôts: avec ces deux catégories de textes propres, on atteint près de dix fois plus de mots que les oeuvres contenues dans \textit{Corpus Corporum}, et en ne prenant en compte que la bibliothèque \textit{Internet Archive}.

\begin{table}[ht]
\centering
\begin{tabular}{l|rrr}
\toprule
                               & Nombre de volumes & \% du total & Nombre de mots \\ \midrule
Qualité \textgreater 90\% OCR  & 4 946              & 23.73       & 635 201 534    \\
Qualité \textgreater 80\% OCR  & 12 169             & 58.38       & 816 236 079      \\
Qualité \textgreater 60 \% OCR & 3 709              & 17.79       & 182 697 928      \\
Reste                          & 19                 & 0.09        & N/A           \\ \bottomrule
\end{tabular}
\caption{Statistiques sur les ouvrages latins disponibles sur Archive.org au début juillet 2021}
\label{tab:chap1:latin-OCR}
\end{table}

La question de l'OCR et de sa qualité (surtout pour les oeuvres imprimées avant la fin du dix-neuvième siècle), deviennent donc prépondérantes dans le contexte de l'acquisition de textes fac-similaires (et donc non édités). Le problème de la reconnaissance de texte pour les oeuvres de l'époque moderne, avec ses S longs et ses restes d'abréviations, des incunables est peu à peu réglé par la mise à disposition massive de données d'entraînements et de modèles adaptés. Dans ce cadre, le travail de Simon Gabay autour des imprimés du 17e siècle est absolument majeur et fondateur\footcite{simon_gabay_2020_3826894}: son usage a permis de générer de très bons modèles OCR\footcite{gabay:hal-02577236} et, à partir de quelques données latines\footcite{Clerice_CREMMA_16_18_Prints_2021}, permet de produire des données propres de textes latins imprimés avant le 19e siècle, de l'\textit{Utopie} de Thomas More à des oeuvres en zoologie du 18e siècle en passant par l'\textit{Historia de duobus amantibus Euralio et Lucretia}, avec des taux de reconnaissance à plus de 96\%. 

Mais un autre enjeu pour la mise à disposition de texte arrive aussi à travers la Reconnaissance d'Écriture Manuscrite (REM, ou plus communément l'anglais HTR pour \textit{Handwritten Text Recognition}). La transcription automatique des documents de la pratique et des manuscrits littéraires, quel que soit leur genre, apportera une autre masse de données pour l'étude du latin sur le très long terme. Le développement de cette technologie, sa popularisation par le biais de Transkribus\footcite{kahle2017transkribus}, sa mise à disposition \textit{open source} par Ben Kiessling via Kraken\footcite{kiessling2019kraken} puis eScriptorium\footcite{kiessling_escripto}, ont permis l'émergence de modèles partagés extrêmement performants. Les travaux d'Ariane Pinche sur les manuscrits en ancien français\footcite{Pinche_CREMMA_Medieval_an_2021} avec une reconnaissance des caractères supérieur à 95\% ou les travaux de Dominique Stutzmann\footcite{hazem2020books} ont montré que cette approche était prometteuse et pouvait potentiellement passer à l'échelle. Il est probable que l'étude quantitative du latin soit largement redéfinie par la mise à disposition de ces corpus nouveaux sur le moyen terme, en s'attaquant de front aux dépôts institutionnels nationaux ou régionaux, comme la Bibliothèque nationale de France et son dépôt Gallica. Dans ce cadre, des projets comme le Gallicorpora, dont font d'ailleurs partie S. Gabay et A. Pinche, montreront rapidement ce à quoi l'on peut s'attendre sur le court terme, avec le développement d'une chaîne de traitement pour la production de documents fac-similaires encodés finement en TEI.

Si l'approche évoquée précédemment est celle d'une approche massive, bruitée et sans réelle intervention humaine, une approche qualitative des corpus en friche est aussi possible. La révolution de la qualité des données obtenues via OCR a permis aussi de développer de nouveaux projets, comme le projet VELUM dirigé entre autres par Bruno Bon pour la mise en place d'un corpus médiéval latin de texte OCRisés\footcite{bon2019challenges}. Du côté des périodes classiques et tardives, il faut alors se tourner vers l'initiative \acrfull{ogl} (\acrshort{ogl}) menée par G. Crane depuis l'université de Leipzig puis de Tufts.

Idée née entre 2008 et 2009, le projet \acrshort{ogl} est le fruit d'un besoin ressenti par des enseignants et chercheurs en lettres classiques associés au \acrfull{chs} (\acrshort{chs}) d'Harvard\footnote{Les locaux de ce dernier sont par ailleurs complètement distincts de ceux d'Harvard, au point d'être dans deux États et deux villes différentes: Cambridge, Massachusetts et Washington DC}, Neels Smith et Christopher Blackwell\footcite{muellner2019free}. Ce projet a pour objectif dès le départ de produire un corpus pour le grec qui soit intégralement \textit{open source}, gratuit et accessible, et qu'il soit standardisé afin de pouvoir s'assurer de la collaboration et de la réutilisation par des partenaires divers. Il passe d'abord par une tentative de partenariat avec le TLG d'ouvrir leur collection, qui restera lettre morte. Ce refus se traduit en 2010-2011 par une première tentative de compilation d'un nouveau jeu de données, mais il faut attendre 2015-2016 pour que le projet prenne forme à part entière.

Neel Smith est en effet un proche ami de Gregory Crane, ils ont partagé les bancs de Harvard, ont travaillé sur les premières moutures de Perseus en équipe et ont partagé le même directeur de thèse, Gregory Nagy, directeur depuis 2000 du CHS. Or, en 2013, Gregory Crane obtient donc la \textit{Digital Humanities Chair} à l'université de Leipzig, où il a pour objectif de relancer le projet Perseus. Dans ce cadre, ses premières productions sont simples: il faut agrandir le corpus gréco-latin, en particulier sur le premier millénaire de notre ère, et ajouter un grand nombre de traductions. Les pères de l'Église et l'ensemble de la littérature tardive restent inaccessibles à ce moment précis en accès libre. Pour le latin, l'OCR a fait les progrès qui permettent à l'équipe de Perseus de mettre en place de nouveaux corpus, dont les deux premiers sont le \acrfull{csel} (\acrshort{csel})\footcite{noauthor_csel_nodate} et la \acrfull{pl} (\acrshort{pl}) de Migne. Si la \acrshort{pl} a été introduite précédemment et existait dans des versions concurrentes, le CSEL est quant à lui un corpus d'éditions critiques des pères latins de l'Église. Née en 1864, cette initiative autrichienne de 1864 toujours en activité à travers l'université de Salzburg\footcite{noauthor_history_nodate} a publié depuis sa fondation plus d'une centaine de volumes comprenant potentiellement plusieurs oeuvres, comme le volume 10 constitué des oeuvres complètes de Sidulius (IXe siècle), et dont une partie est tombée, avec son apparat critique, dans le domaine public.

Au niveau technique, ces corpus sont transcrits automatiquement, puis corrigés et structurés en XML TEI par des entreprises spécialisées, dont l'entreprise française Jouve et sa succursale malgache. Leur XML est ensuite adapté aux attentes de Perseus par des équipes internes puis mis à disposition sur Github. C'est ce même fonctionnement qui est repris lors de la mise en place de l'alliance entre le CHS, l'université de Mount Alison et les équipes de G. Crane. Dans l'article de L. Muellner, on apprend que l'OCR est réalisée par les équipes de Mount Alison, sous l'égide de Bruce Robertson qui entretient des modèles pour l'OCR grecque. Puis, les données sont envoyées à l'entreprise plurinationale \textit{Digital Divide Data} (DDD; Cambodge, Kenya, Indonésie) pour leur mise en Epidoc, le format choisi par Perseus. On y apprend que le coût budgété de numérisation et d'encodage par DDD est de 50~000\$ pour quatre millions de mots, soit bien moins que les premiers projets des années 70 et 80. Enfin, la mise en conformité et la vérification des données sont assurées par des stagiaires, étudiants de la licence au doctorat, hébergés d'abord uniquement par le CHS puis par l'université de Virginie\footcite{robertson2019optical}.

La production de l'ensemble de ces corpus a permis ensuite à Perseus de se tourner vers une nouvelle version, en partie financée par le CHS, Perseus 5\footnote{\url{https://scaife.perseus.org/}}, dont la mise à jour est automatiquement liée à l'évolution des corpus, contrairement à la version 4, et qui a été l'objet d'une refonte totale de l'infrastructure de Perseus. Cette version a abandonné -- pour le moment -- les données graphiques (archéologiques, histoire de l'art, etc.) pour ne s'intéresser qu'aux données textuelles.

Enfin, avec l'apparition de tous ces projets se pose la question de l'éclatement des corpus sur internet. Entre les données de Perseus, de DigilibLT, d'autres projets comme le \textit{Corpus Grammaticorum Latinorum} de Jussieu (CGL), l'accès à un corpus latin unifié devient problématique. C'est un problème d'autant plus important que ces sites ne partagent pas une architecture commune qui pourrait alors permettre de centraliser les recherches sur de multiples corpus. En 2011, Philipp Roelli, un éditeur de texte aussi intéressé par la linguistique de corpus, met en route le projet \textit{Corpus Corporum} à l'université de Zurich\footcite{roelli2014corpus}. Ce projet est très peu financé en dehors d'une aide de la chaire de latin et d'une partie des fonds de la COST-Action IS1005 dont l'objectif principal était la formalisation d'un réseau de recherche autour du latin médiéval\footnote{\textit{Corpus Corporum} est donc plus une externalité positive de cette dernière que l'un de ses objectifs.}. P. Roelli le définit comme une ``meta-collection'': \textit{Corpus Corporum} ne produit pas de numérisation ou d'édition numérique, il centralise, en harmonisant, les données d'une dizaine (en 2014) puis d'une trentaine d'autres projets (en 2021), accumulant ainsi en un seul endroit presque 164 millions de tokens à la fin 2021, s'étalant du latin classique au néo-latin. Bien que techniquement ``rustique'' du côté client\footnote{L'usage des \textit{iframes} a presque complètement disparu du web, hormis sur \textit{Corpus Corporum}.}, le projet a l'avantage d'être rapide, facile d'usage -- à l'image de \textit{The Latin Library} -- et de permettre l'accès à des corpus perdus comme les CGL, indisponibles sur leur site d'origine depuis quelques années.

Après l'avènement du micro-ordinateur et du web, les corpus latins ont ainsi évolué pour atteindre aujourd'hui une ouverture sans commune mesure. Pour les plus grands classiques, il est possible d'en trouver des éditions voire des traductions -- en anglais majoritairement -- assez facilement en plein texte. Et quand cela n'est pas possible, il peut toujours être fait recours aux dépôts institutionnels ou privés tels qu'Archive.org ou HathiTrust aux États-Unis afin d'obtenir la numérisation d'un de ces ouvrages. Cette révolution, sur presque cinquante ans, est celle de l'accès aux textes latins classiques et tardifs dans leur intégralité -- ou presque -- et de manière gratuite, et permet de produire de nouveaux questionnements, de nouvelles approches.

% Méta-corpus
%% Corpus Corporum

% Côté éditorial
% \subsubsection{Le renouveau \textit{Open Greek And Latin} et l’apport de l’OCR de masse}
% Approche Github 
% Approche API ?

% Côté non-éditorial non universitaire et l'hors-classique un peu aussi
% \subsubsection{Les corpus en jachères}
%Archive.org et institutions patrimoniales qui OCRisent
% Réfléxion autour de l'OCR de Archive.org, statistiques obtenues quand on a fait des fouilles

\section{Constituer un corpus de recherche}


\begin{quote}[\enquote{The \textit{Corpus Corporum}, a new open open Latin text repository and tool}]{Philipp Roelli}
    \textit{The idea was born from my linguistic research to create an open and non-commercial Latin text meta-collection.} \\
    \enquote{L'idée est née de mes recherches linguistiques: créer une méta-collection ouverte et non-commerciale de textes latins.}
\end{quote}

Avec l'histoire des corpus, il est clair que le nombre de corpus disponibles, ouverts, fermés ou à réutilisation limitée est assez important pour produire une recherche plein texte ou la compilation d'extrait. Nous commenterons ici le travail de constitution du corpus final, qui atteint près de vingt millions de mots, dans ces choix éthiques et techniques. Nous discuterons de l'importance de l'existence de corpus annotés finement et du risque que représentent les corpus plein textes. Enfin, nous discuterons de la compilation du corpus à partir de l'oeuvre scientifique de J. N. Adams, des limites de cette dernière mais aussi des méthodes employées pour compiler cette collection.

\subsection{Constitution d’un corpus général de sources littéraires latines} 

\begin{quote}{\cite{camps_ou_2018}}
La distinction entre humanités « numériques » et « computationnelles » est dans l’air. Au‑delà d’un pur choix terminologique distinctif ou d’un retour aux \textit{humanities computing} du XXe siècle, la revendication d’une dimension computationnelle rend compte d’un basculement, à mon sens éminemment souhaitable, d’une perspective tournée vers la diffusion et la publication électronique, à un accent mis sur les données et leur exploitation pour la création de nouveaux savoirs scientifiques.
\end{quote}

Le constat de Jean-Baptiste Camps est juste: depuis les années 2000 et 2010 en particuliers, la technicisation de l'analyse des documents et des sources, à travers la stylométrie par exemple, et le besoin d'une reconnaissance à part de cette technicisation a donné lieu à de nouvelles sous-communautés des humanités numériques, avec leurs réseaux parallèles de conférences. Là où nous différons cependant, c'est sur l'apparente exclusion mutuelle qu'opèrerait ce nouveau paradigme de la recherche numérique: la constitution de corpus, la collection de textes, leur contrôle qualité, voire leur édition sont autant de missions qu'il ne faut pas négliger dans une approche plus ``computationnelle'' des sciences-humaines, sans quoi les humanités computationnelles ne sont plus des humanités mais de l'informatique appliquée.

\subsubsection{Le choix d’un corpus open-source: Traçabilité des textes, textes et reproductibilité}

Qu'est-ce qu'un corpus ? \textit{Le Robert} définit le corpus comme un ``Ensemble fini de textes choisi comme base d'une étude.'' tandis que le \textit{Larousse} prend -- par un heureux hasard -- l'exemple des corpus grecs pour appuyer sa première définition: ``Recueil de documents relatifs à une discipline, réunis en vue de leur conservation : Corpus des inscriptions grecques.'' et rejoint légèrement le \textit{Robert} pour sa seconde (``Ensemble fini d'énoncés écrits ou enregistrés, constitué en vue de leur analyse linguistique.''). Si les deux dictionnaires se rejoignent sur un point, le corpus est une collection de documents, potentiellement de texte, ils évoquent deux finalités différentes. La première, la conservation, sous-entendue la maintenabilité et l'accessibilité en un même endroit, physique ou numérique, d'une ensemble documentaire, n'est mentionnée comme définissante que par le \textit{Larousse}. Dans son article de 2013, Alex H. Poole fait le constat tour à tour que ``les humanités humériques pivotent autour des données''\footnote{``\textit{The digital humanities pivot around data.}''\cite{poole_now_2013}} mais aussi que ces dernières, au format numérique, étaient ``notoirement fragiles, d'une courte espérance de vie, et facile à manipuler sans laisser forcément de traces, rendant la fraude difficile à détecter [..., sachant que] la plupart des données collectées n'étaient ni organisées ni publiées''\footnote{``\textit{Our Cultural Commonwealth} report characterized digital data as “notoriously fragile, short-lived, and easy to manipulate without leaving obvious evidence of fraud”. Worse, much collected data were neither curated nor published whatsoever;'', \cite{poole_now_2013} citant \cite{unsworth2006our}}. Nous partageons ce constat, cette importance de la conservation pour que corpus existe, et établissons ce point comme premier objectif autour de notre corpus. 

La seconde finalité évoquée est celle de l'analyse (``la base d'une étude'', ``en vue d'une analyse linguistique''). Si nous estimons que cette finalité peut être déplacée (le corpus peut être compilé pour qu'une tierce personne s'en empare), elle est bien évidemment centrale dans notre projet. Et elle demande ainsi de définir l'objectif de notre corpus, car celui-ci définira la forme et les informations nécessaires à y retrouver. Nous reviendrons plus tard sur l'impact qu'a cette finalité sur le corpus, dans son annotation et sa documentation(\textit{cf.} \ref{chap1:method-annotation}).

Il faut cependant ajouter à la notion de corpus un autre point: celui de son ouverture, en droit et en accès. A. H. Poole le mentionne partiellement dans la citation avec la question de la ``fraude'' mais la citation de Borgman\footcite{borgman2012conundrum}, reprise par J.-B. Camps\footcite{camps_ou_2018} est à notre sens assez complète. Un corpus doit être ouvert pour
\begin{enumerate}
    \item reproduire ou vérifier la recherche,
    \item rendre les résultats d'une recherche publique disponible pour le public,
    \item rendre la possibilité à d'autres de poser de nouvelles questions aux données
    \item avancer l'état de la recherche et de l'innovation.\footnote{``(1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation.''\footnote{\cite{borgman2012conundrum} chez \cite{camps_ou_2018}}.}
\end{enumerate}

La question de la reproductibilité, par l'ouverture du corpus et sa documentation, est centrale pour Borgman, pour Poole et pour Jean-Baptiste Camps. Si la traçabilité des sources n'est pas une nouveauté pour les lettres et l'histoire -- la citation de ces dernières est extrêmement codifiée afin d'être compréhensible et exhaustive, la transcription des sources de la pratique souhaitée pour les publications --, la reproductibilité des expériences est définitivement nouvelle. D'abord, car la notion d'expérience en lettres comme en histoire est nouvelle, bien qu'elle ne le soit pas nécessairement partout dans les sciences humaines. Ensuite, car la notion de reproductibilité est tout autant complexe dans le monde des sciences dites dures. Comme le dit J.-B. Camps, ``au fur et à mesure que l’analyse de données prend de l’importance dans la constitution de nouveaux savoirs, le besoin se fait plus criant de vérifier l’intégrité des données, de reproduire les expériences, de vérifier ou infirmer les énoncés qui en découlent.''\footcite{camps_ou_2018}. L'arrivée de ces questionnements scientifiques et la "crise de la reproductibilité" en 2000, que mention J.-B. Camps est suivi peu à peu par une crise en intelligence artificielle\footcite{hutson2018artificial}, traitement automatique des langues\footcite{belz2021systematic} et en histoire\footcite{eijnatten_big_2013}.

Un autre avantage, en partie lié à la reproductibilité, des corpus ouverts est celui de son analyse et en particulier de ses biais. En accumulant des données dont on essaye de tirer des analyses, des conclusions et même simplement des faisceaux d'indices, la possibilité d'introduire, inconsciemment, des biais de corpus et -- à travers eux -- d'établir des conclusions invalides est un danger éminemment connecté aux corpus fermés. Les conséquences peuvent être importantes dans le domaine de l'intelligence artificielle, le \textit{machine learning} ne pouvant qu'apprendre ce qu'on lui montre. L'exemple le plus connu des dernières années est celui de la reconnaissance d'image de Google, qui, en 2019, avait tout simplement catégorisé des personnes afro-américaines comme gorilles\footcite{lohr2018facial, chokshi2019facial}. Si les conséquences pour notre corpus ne pourraient être aussi graves et socialement problématiques, il reste que la question du biais est à prendre en compte. Il ne s'agit pas de promettre l'exhaustivité: le domaine des lettres classiques a depuis longtemps admis la partialité -- dans les deux sens -- de ses sources ainsi que les pertes de nombreuses autres sources. Sur ce sujet, nous reprendrons l'exemple de l'article de I. D. Raji \textit{et al.}\footcite{raji2021ai}:

\begin{quote}{\cite{raji2021ai}}
    Dans le livre d'histoires pour enfants \textit{Sesame Street}, ``Grover and the Everything in the Whole Wide World Museum''[Stiles et Wilcox, 1974], le monstre \textit{Muppet Grover} visite un musée qui prétend présenter "tout ce qui existe dans le monde entier". Des exemples d'objets représentant certaines catégories remplissent chaque pièce. Plusieurs catégories sont arbitraires et subjectives, notamment les salles d'exposition des "choses que l'on trouve sur un mur" et de "la salle des choses qui peuvent vous chatouiller". Certaines sont étrangement spécifiques, comme "La salle des carottes", tandis que d'autres sont inutilement vagues comme "La grande salle". Alors qu'il pense avoir vu tout ce qu'il y a, Grover arrive à une porte intitulée "Tout le reste". Il ouvre la porte et se retrouve dans le monde extérieur.\footnote{\textit{``In the 1974 Sesame Street children’s storybook Grover and the Everything in the Whole Wide World Museum [Stiles and Wilcox, 1974], the Muppet monster Grover visits a museum claiming to showcase “everything in the whole wide world”. Example objects representing certain categories fill each room. Several categories are arbitrary and subjective, including showrooms for “Things You Find On a Wall” and “The Things that Can Tickle You Room”. Some are oddly specific, such as “The Carrot Room”, while others unhelpfully vague like “The Tall Hall”. When he thinks that he has seen all that is there, Grover comes to a door that is labeled “Everything Else”. He opens the door, only to find himself in the outside world.''}}
\end{quote}

Tout comme l'idée d'un musée du monde est absurde, l'absence de biais dans un corpus l'est tout autant. Mais la possibilité de les décrire et des les vérifier à travers un corpus ouvert est primordiale pour la critique des résultats.

Nous ajouterons cependant une dernière possibilité derrière l'ouverture de ces données, particulier à leur dimension numérique: l'\textit{open access} et l'\textit{open source} dans ce contexte permet aussi la croissance et la modification des données sur le long terme, ne figeant pas le corpus en un instant T (bien qu'il soit important de pouvoir revenir à ce dernier pour la reproductibilité). Le corpus de notre recherche doit non seulement survivre à sa publication mais aussi se corriger, s'arranger: il serait probablement présomptueux de le croire exhaustif, sans erreur, et d'autres seront -- nous l'espérons -- intéressés par sa correction ou son extension à d'autres textes, d'autres périodes.



% Rappel des objectifs: à la fois la constitution d'un corpus d'entraînement ET de sources
% Si corpus d'entraînement, besoin de pouvoir accéder à ce corpus, donc corpus ouvert.
% Si corpus de source, besoin de traçabilité

% Donc Choix Open-Source uniquement
% Et choix d'un corpus TEI, en entrée et en sortie

% De la question de traçabilité découle la question  Capitains
% Citabilité, manipulabilité, compatibilité: XML-TEI et Capitains ? (ou dans le .2 ?)
% Le machine actionnable / readable est pas mentionné au final.?

\subsubsection{Méthode d’annotation et de “regroupement”}
\label{chap1:method-annotation}

% Reprendre le travail de Mc Gillivray ici
% Reprendre la méthode de datation
% La question de la datation
% Poser la question de la citabilité et de la section des textes
% Métadonnées de “lecture”: modèles de citation, niveau de citation recommandé (Introduction du concept de SATU ?)

\subsubsection{Corpus: réutilisation, production, contrôle}

% Statistiques sur les corpus Perseus et autres
% La conversion de DigilibLT
% Présentation des corpus Lasciva Roma
%% Priapées
%% Additional-Texts

\subsection{Du corpus au document: qu’est-ce qu’un document pour l’ordinateur ?}

% Environ cinq à dix pages

% L’importance du choix de CapiTainS, rerédaction de l’article précédemment écrit

\subsection{Constitution d’un corpus sur l’expression de la sexualité} % Environ dix pages

\subsubsection{Le choix d’une source: Adams et histoire des tentatives de vocabulaires de la sexualité latine ?} 

% TLL et problème du TLL chez Adams

\subsubsection{Conséquence du choix de Adams}

% Les données épigraphiques: pourquoi non.
%% Difficulté de lemmatisation
%% Présence relativement faible

% Les bornes “chronologiques” du corpus
% Notes sur quelques données absentes

\subsubsection{Corpus résultat: format, métadonnées, limites}

% Format et tags: interprétation


\section{Composition et analyse des corpus employés}

\subsection{Analyse de la diversité du corpus par période, auteur et genre}

% Représentativité
% Les périodes creuses ?

\subsection{Analyse du corpus sexuel final (stats et autres)}
% Représentation et sur-représentation des auteurs
%% L’angle mort de l’étude d’Adams: la période Chrétienne sous-représentée ?
% Une analyse lexicométrique du corpus: termes les plus fréquents “hors” stopword ?
% Création d’un corpus négatif:
%% Spécificité des termes du corpus: nombre de termes commun (lemme comme formes, stop-words inclus)
%% Nombre de textes très communs (% de lemmes communs importants) via une analyse à la Tesserae ou autre ?
