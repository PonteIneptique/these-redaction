\chapter*{Conclusion}
% pour faire apparaitre l'introduction dans le sommaire
\addcontentsline{toc}{chapter}{Conclusion}
% Pour que l'entete soit correcte car chapter* ne redefinit pas l'entete.
\markboth{CONCLUSION}{}

\begin{quote}[\textit{Les Nuits d'une Demoiselle} (1963), paroles de Guy Breton]{Colette Renard (interprète)}
Que c'est bon d'être demoiselle \\,
Car le soir, dans mon petit lit \\
Quand l'étoile Vénus étincelle \\
Quand doucement tombe la nuit \\  
Je me fais sucer la friandise \\
Je me fais caresser le gardon \\
Je me fais empeser la chemise \\
Je me fais picorer le bonbon \\
Je me fais frotter la péninsule \\
Je me fais béliner le joyau \\
Je me fais remplir le vestibule \\
Je me fais ramoner l'abricot {[...]} \\
Et vous me demanderez peut-être \\
Ce que je fais le jour durant \\
Oh! cela tient en peu de lettres \\
Le jour... je baise, tout simplement.\footnote{Nous remercions l'algorithme de Spotify de nous avoir fait découvrir cette chanson pendant notre travail de thèse via la fonction écoute recommandée.}
\end{quote}

% Rappel de la problématique
Nous avions commencé cet ouvrage avec une question \enquote{simple}: est-il possible pour un ordinateur d'apprendre à reconnaître une isotopie, en particulier celle de la sexualité, en latin ? L'isotopie étant simplement définie par une répétition de sèmes, elle peut prendre un nombre de formes quasi illimitées: vocabulaire explicite ou formules implicites via des métaphores, métonymies, etc. Les cacemphatons de Cicéron (\enquote{\textit{cum nobis}}, \enquote{\textit{illam dicam}}) démontrent parfaitement que, du sens premier aux divers emplois figuratifs, les moyens d'expression à disposition des auteurs sont particulièrement étendus. En ajoutant à cela la présomption d'isotopie, même une phrase comme \enquote{Sous son regard fatigué, il branchait sa clef usb.} peut être perçue comme sexuelle. Quand il s'agit de parler de sexe, tant de sèmes sont activables (par exemple, ceux liés au mouvement, au contact, à certaines formes, etc.), que même \enquote{caresser le gardon} devient obscène. La question n'est pas seulement, \enquote{peut-on faire faire de la lexicographie à la machine ?} mais elle est aussi \enquote{peut-on lui faire faire vraiment de la stylistique ?} Il s'agit alors de dépasser la simple détection des dérivations métaphoriques ou métonymiques lexicalisées\footnote{Méthode utilisée par nombre de chercheurs, qui s'appuie tout particulièrement sur des dictionnaires pour relever, par exemple, des emplois métaphoriques.}, pour s'attaquer enfin aux figures de style et aux emplois connotatifs complexes, fortement dépendants du contexte.

Cette question est d'autant plus intéressante que le domaine linguistique et culturel auquel elle s'applique est celui du latin. Du point de vue culturel, contrairement à la détection d'isotopies sexuelles en langue française du XX\textsuperscript{e} ou du XXI\textsuperscript{e} siècle, il ne nous est pas possible de poser la question à l'auteur, d'utiliser des dictionnaires contemporains à ce dernier, ou encore de recourir à des témoins vivants pour évaluer la pertinence d'une interprétation. Par ailleurs, la langue latine reste encore sous-dotée et sous-étudiée par le traitement automatique des langues: son étude est généralement réservée à une poignée de spécialistes en linguistique latine, également compétents en informatique. C'est notamment le cas pour les auteurs des plus récents travaux de lemmatisation produits ces dernières années, comme par exemple Enrique Manjavacas, créateur de Pie et détenteur d'une licence de Philologie classique, l'équipe du LASLA avec Dominique Longrée qui mène son travail en collaboration avec quelques TAListes réceptifs à ces problématiques, ou encore les membres de l'ERC LiLa, qui sont tous ou presque latinistes de formation. La langue latine, avec ses deux mille ans d'histoire, et donc d'évolutions, offre pourtant un \enquote{terrain de jeu} remarquable pour évaluer de nouveaux outils: elle possède une morphologie extrêmement riche, des périodes de variations graphiques, des phénomènes dialectaux et scripturaux depuis les papyri et graffitis jusqu'aux chartes médiévales. Comme le dit F.~Rastier, \enquote{[la philologie fait partie des ]disciplines injustement oubliées, du moins dans le domaine des Traitements automatiques du langage\footcite{rastier2005enjeux}}. 

%Inutile,je trouve : "Quelle montagne de travail à gravir pour qui voudrait se laisser tenter par la combinaison de ces deux mondes..." Tu le dis bien assez avant

En dehors de la pratique du TAL en latin, cette question est intéressante pour le croisement du thème qu'elle traite (la sexualité) et des méthodes qu'elle se propose d'utiliser (numériques). Ces dernières années, le problème des biais dans les \enquote{algorithmes d'intelligence artificielle} a produit un grand nombre de débats\footnote{Pour une typologie des biais en jeu, et les débats qu'ils peuvent générer, \textit{cf.} \textcite{mehrabi2021survey}.} et a poussé les chercheurs à les quantifier\footcite{bolukbasi2016quantifying} ou à en évaluer l'impact social\footcite{doi:10.1126/science.aal4230}. On sait que les modèles ne sont pas neutres, notamment à cause des données à partir desquelles ils apprennent. S'il faut être conscient de l'influence de ces biais, cette capacité de la machine à capturer des \enquote{clichés} est aussi une force pour l'étude de civilisations passées. Bien sûr, il ne faut pas utiliser la machine seule, car les textes ne sont pas les seules sources à notre disposition. En croisant les textes avec d'autres sources, nous sommes ainsi capables de dépasser le \enquote{problème du mouton noir\footnote{Nous avons entendu ce terme de nombreuses fois, mais jamais nous n'avons réussi à trouver de manière certaine sa source. Nous proposons tout de même celle-ci, à propos des \textit{embeddings} de Google. \textcite{daume_blacksheep_2016}.}}. Le \enquote{\textit{black sheep problem}} correspond au phénomène qui pousse tout algorithme dont la représentation du monde est issue uniquement de textes à estimer que le mouton et noir sont deux termes plus proches que mouton et blanc. Ce résultat est dû aux connaissances extra-linguistiques du réel des locuteurs, qui ne se sentent pas obligés de préciser \enquote{le mouton blanc}. Or, à propos de la sexualité romaine, les clichés nous intéressent autant que les exceptions. L'approche algorithmique offre dans ce cadre une nouvelle méthode heuristique pour l'étude des civilisations passées, qui permet d'appuyer, de relativiser ou de faire émerger de manière computationnelle des hypothèses qui pré-existaient ou non à l'avènement du numérique dans les sciences humaines.

\paragraph{Corpus, exempliers, lettres classiques}

Il est intéressant de voir combien, chez F.~Rastier, B.~Pincemin et ou D.~Mayaffre, la question du corpus est \enquote{centrale\footcite[p.~59]{mayaffre2008occurrence}} et valorisée. Nous avons vu pourtant que l'histoire des corpus latins -- et grecs quand la référence était nécessaire -- avait été très peu documentée et étudiée: les corpus sont relégués au stade d'outil non-cités, non valorisés. Nous avons rassemblé une nouvelle documentation -- qui pourra être complétée -- inédite sur le sujet, de la fondation des premiers corpus grecs aux corpus latins des années 1990 pour les premières initiatives, jusqu'à 2021 pour les projets plus récents avec l'inclusion de l'\textit{Internet Archive}. Les informations commencent bien évidemment à se perdre, et, peu à peu, les responsables des premiers projets approchent d'une fin de carrière bien méritée. \textit{Quid} de leur expérience, en dehors des articles bien évidemment élogieux pour obtenir un nouveau financement ? Nous avons découvert à cette occasion une bibliographie insoupçonnée et difficile d'accès, qui nous permet d'accéder à cette histoire. Par exemple, \enquote{Bits, Bytes \& Biblical Studies} de John. J.~Hughes\footcite{hughes_bits_1987} n'est présent qu'en un seul exemplaire en France, d'après le Sudoc ou le CCfr, et seulement en trente-sept exemplaires en Europe d'après le WorldCat ; et pourtant, il est la seule source connue à donner une image aussi précise des projets informatiques dans le domaine gréco-latin pour les années 1980.

Nous avons aussi découvert les révolutions numériques qui ont suivi le milieu des années 1990 et les promesses des deux dernières décades, à travers, entre autres, la reconnaissance de caractère. Là où les chercheurs de \textit{Perseus} ou du TLG disaient vouloir éviter l'OCR, car le nombre d'erreurs était trop important sur de l'imprimé du XX\textsuperscript{e} ou de la fin du XIX\textsuperscript{e}, nous sommes en train de voir émerger des modèles de reconnaissance de textes permettant le traitement d'éditions des XVI\textsuperscript{e} et XVII\textsuperscript{e} mais aussi de manuscrits médiévaux. Cette révolution permettra peut-être d'en accompagner une autre, qui serait celle d'un basculement vers les éditions nativement numériques et la mise à disposition de textes par les éditeurs scientifiques. La très grande majorité des corpus étudiés sont aujourd'hui constitués de textes sans apparat critique ; cela constitue un obstacle pour les philologues numériques qui souhaiteraient réaliser une étude méthodique et globale des phénomènes de variations. Nous avons vu que c'est l'ambition de projets comme la \textit{Digital Latin Library}, mais nous sommes en peine de trouver d'autres exemples. Cette histoire nous a donc permis de comprendre l'état des corpus en latin à notre disposition. Nous sommes conscient d'avoir adopté - par force - une optique très anglo-saxonne. On trouvera cependant des projets italiens évoqués dans l'histoire des corpus, mais nous n'abordons pas dans le détail l'histoire des corpus en Italie\footnote{Et pour cause: malgré une maîtrise relativement correcte de l'anglais et une certaine familiarité avec le français, il nous fut extrêmement difficile de trouver des sources sur les corpus principaux (Perseus, PHI, TLG, PLD et CLCLT). Notre niveau italien ne nous aurait pas permis de conduire avec la même diligence cette historicisation des corpus.}.

À partir de cette analyse de la production des corpus sur les cinquante dernières années, nous avons pu entamer une réflexion sur les critères techniques qui doivent définir un corpus, à la fois réalisation d'ingénierie et collection de documents. L'ambition de cette thèse n'était pas de créer un corpus nouveau -- nous n'aurions pas eu le temps -- mais bien de rassembler, là où c'était possible, un nouvel ensemble de textes à même de constituer une base de recherche pour la philologie et l'histoire latine, computationnelles ou non. Les critères retenus sont simples: le texte doit être encodé en XML TEI, afin de garantir son interopérabilitié ; en accès libre, \textit{open source} et donc perfectible ; et enfin actionnable. L'interopérabilité, doublée par l'utilisation d'un standard, permet de pérenniser l'information produite et d'en assurer une forme de documentation minimale: un document TEI, d'autant plus un document simple à l'image de ceux à notre disposition, est assuré d'être compréhensible et maniable grâce aux connaissances et aux compétences partagées d'une communauté, ici celle des humanités numériques. L'accès libre comme l'\textit{open source} assurent non seulement la possibilité de corriger des sources (erreur d'OCR, fausse attribution, texte manquant), mais aussi la reproductibilité des expériences. Cette dernière amène un niveau de confiance plus grand dans les résultats, et permet également de comparer des approches en assurant la possible réutilisation de ces données. Enfin, nous avons vu que la partie \textit{machine actionable} avait un impact important pour nos corpus. Contrairement à une grande partie de ceux créés pour le TAL, nos corpus ne sont pas pensés comme des corpus pour linguistes, mais pour une consommation en \textit{close reading}: c'était l'objectif pédagogique de Perseus en 1990, et c'était également celui de \textit{DigilibLT} qui met à disposition des textes difficiles d'accès. L'injection et la reconnaissance de l'importance des systèmes de structures logiques permet d'extraire des documents indépendants ou semi-indépendants des fichiers. Nous avons montré qu'ignorer le rôle des (S)ATU mène à des résultats différents: s'il s'agit de s'appuyer sur un traitement numérique du texte pour une question de philologie ou d'histoire, il faut pouvoir s'adapter aux exigences de ces dernières pour garantir la pertinence de nos conclusions. Encore une fois, qui estimerait que le dernier mot d'une épigramme est co-occurrent du premier mot de la suivante dans un commentaire littéraire ou une analyse linguistique ?

Les critères techniques nous ont permis de faire émerger un corpus principal, notre meta-corpus, constitué en majeure partie de corpus pré-existants voire encore en activité. À travers \textit{Perseus}\footcite{perseus_latinLit}, le CSEL d'\textit{Open Greek and Latin}\footcite{csel_latinlit} et la \textit{DigilibLT}\footcite{digiliblt}, une couverture de l'espace chronologique de -250 à +800 est possible. Elle n'est pas parfaite, et nous avons dû ajouter des textes de la période classique comme de la période tardo-antique, notamment les \textit{Priapées} ou l'\textit{Anthologie Latine}. Le corpus final est constitué de près de vingt millions de mots et il demande encore du travail de conversion vers une structuration \textit{machine actionable} pour des textes de \textit{Perseus} et de la \textit{DigilibLT}: le corpus potentiellement disponible en utilisant uniquement les oeuvres à disposition reste donc immense.

Une fois le méta-corpus compilé, nous nous sommes tourné vers la thématique qui constitue notre terrain d'expérience, à savoir la sexualité dans la littérature latine classique et tardive. L'histoire de la sexualité est un domaine très récent, ou en tout cas complètement renouvelé depuis l'ouvrage fondamental de Michel Foucault. Nous avons montré qu'il existe plusieurs vagues de recherche sur le sujet dans le domaine historique, mais qu'au contraire, l'étude de son lexique est un peu plus ancienne, avec quelques glossaires massifs au XIX\textsuperscript{e} siècle. Cette lexicographie s'est retrouvée intégralement reprise en une dizaine d'années, à travers deux thèses et une monographie: Enrico Montero Cartelle a fini son doctorat sur le sujet en 1973\footcite{montero_cartelle_aspectos_1973}, Amy Richlin en 1978\footcite{richlin_sexual_1978}, et James N.~Adams a conclu ce mouvement par le \textit{Latin Sexual Vocabulary}\footcite{adams}, qui fait source pour notre recherche. Nous avons montré qu'Adams s'intègre dans une école mancunienne de linguistique, et son oeuvre sur le sujet dépasse la \enquote{simple} monographie citée. Il s'avère d'ailleurs que le vocabulaire compilé par Adams n'est pas exhaustif, en dépit de sa taille conséquente, et fait preuve de manques systématiques: les adjectifs liés à la moralité sexuelle (\textit{lascivus}, \textit{mollis}, \textit{impudicus}, etc.), le vocabulaire de l'abstinence et la virginité, le domaine médical et la période tardive chrétienne sont soit sous-traités, soit totalement ignorés.

À travers notre lecture d'Adams, première pierre fondatrice et fondamentale d'un projet qui peut encore grandir, nous avons établi, à partir des mêmes principes établis pour notre corpus, une collection d'échantillons. Nous proposons de nommer ce type de collections \enquote{exemplier numérique} en cela qu'il a une vocation doublement pédagogique. Il a en effet deux publics différents: d'une part, les (apprentis) chercheurs, enseignants et amateurs du latin, qui peuvent puiser dans une lecture et recherche de cette base de données des exemples pour mieux comprendre un texte ou un phénomène social; d'autre part, la machine, utilisatrice nouvelle des documents numériques, destinée à apprendre à partir de données consignées par des spécialistes pour produire de nouveaux savoirs. Nous avons présenté les limites de notre approche d'Adams, où une lecture parfois trop linéaire a posé un problème dans notre classement sous-thématique (par exemple, \textit{violent} n'inclut pas \textit{arme}) et pose les bases de la nécessité d'un \textit{thesaurus} avant d'aller plus loin. Cependant, l'exemplier produit contient plus de deux mille cinq cents échantillons, qui permettent désormais d'entraîner une machine à reconnaître des isotopies autant qu'elle peut faire base de données. Nous présentons d'ailleurs les prémices d'une interface d'exploration de ces extraits collectés, avec les fonctionnalités qui semblent à notre sens nécessaires pour assurer, en plus d'une exploitation statistique pour les plus hardis numériquement, une simple visualisation et contextualisation des données.

\paragraph{Lemmatisation du latin}
%% Comment je suis arrivé grâce à la technique à faire apprendre à la machine de la stylistique.

Le latin présente une morphologie riche et un ordre des mots plus libre que certaines langues comme le français ou l'allemand, en particulier pour les formes poétiques: cela en fait une langue complexe à traiter automatiquement. Par exemple, l'utilisation de la racinisation (\textit{stemming}) pose problème. Si on compare le latin à l'anglais, on comprend facilement les limites de cette méthode. La langue anglaise possède une dizaine de formes verbales non composées au plus (quatre formes pour un verbe régulier, hors composition: \textit{love}, \textit{loves}, \textit{loved}, \textit{loving}), tandis que le latin comporte lui près de six formes par combinaison temps--mode--voix, avec bien évidemment des classes verbales. Elle connaît également une déclinaison adjectivale pour ses participes\footnote{Pour \textit{amo}, une déclinaison automatique donne cent cinquante formes différentes et \enquote{possibles} morphologiquement, en-dehors des formes composées à l'aide d'un auxiliaire.}.

La lemmatisation du latin permet de faire face à la diversité morphologique, qui est complétée suivant les périodes et les types de textes par de la variation graphique, en ramenant chaque forme en contexte à une forme unique, \enquote{canon}. En latin classique, la première personne du singulier présent de l'indicatif actif est utilisée comme lemme pour les verbes tandis que le nominatif singulier (masculin le cas échéant) l'est pour les noms et adjectifs. Ce travail s'inscrit dans une longue lignée de travaux concernant la lemmatisation du latin, que nous avons fait débuter avec les premiers concordanciers du XIII\textsuperscript{e} siècle. Ces ouvrages présentent une liste d'entrées : le lemme y est suivi de phrases dans lesquelles il est présent, accompagnées des références permettant une rapide inspection des usages d'un mot par un lecteur et un retour au contexte si nécessaire. L'établissement de ces concordanciers a fait partie des premiers lieux d'application du numérique, d'abord par Roberto Busa dans les années 1950 puis par le LASLA dans les années 1960. Mais l'annotation du texte et le référencement sont des tâches manuelles coûteuses (sans compter le prix du matériel qui s'y ajoute): les membres du LASLA se mettent alors à rêver d'une \enquote{machine à lemmatiser} qui ne verra pas le jour avant la charnière des années 1980-1990. On voit à cette période émerger des lemmatiseurs, que nous avons classés en trois catégories (à règles, statistiques, à \enquote{traduction}). 

Nous expliquons en particulier l'importance du travail de Mike Kestemont\footcite{kestemont_lemmatization_2017} et Enrique Manjavacas\footcite{manjavacas_improving_2019}, son doctorant, autour de la lemmatisation des langues médiévales, applicables au latin classique et tardif. Les systèmes extrêmement performants proposés par ces deux chercheurs ont pour particularité de reposer, comme les modèles statistiques, sur un apprentissage à partir de corpus d'entraînement, c'est-à-dire de données pré-annotées. La première année de notre thèse nous a donné quelques inquiétudes sur ce point: les corpus ouverts latins, de \textit{Perseus}, de \textit{Perseids} ou de \textit{Proiel}, ne sont pas suffisants pour obtenir des modèles d'annotation performants. Nous avions alors à notre disposition soit des corpus variés avec très peu de mots (moins de cent vingt mille) soit des corpus très peu diversifiés, mais avec un plus grand nombre de mots (deux cent vingt-cinq mille mots, \textit{Proeil}). C'est l'accès au corpus du LASLA fin 2018 qui a probablement rendu possible notre thèse: le corpus disponible total était presque multiplié par six avec un million six cent mille formes. Une fois ces nouvelles données réceptionnées, notre étude s'est concentrée sur la compréhension du corpus, de son format d'expression aux choix d'annotations\footcite{BodsonCodification1966}. Nous avons identifié quelques choix d'annotation importants à signaler en amont de toute utilisation: la non-reconnaissance du genre (le genre des adjectifs est grammatical et non décidé en contexte, celui des noms n'est pas reporté) ; l'utilisation de lemmes issus du Forcellini et d'une désambiguïsation syntaxique au niveau lemme ; l'usage de temps composés, portés par les participes - là où il est habituel en annotation automatique de retrouver simplement l'information du participe (et c'est alors la combinaison auxiliaire + participe qui donne l'information temps), etc.

À partir de ces \enquote{nouvelles} données, nous avons proposé une méthode d'entraînement permettant d'obtenir un modèle efficace (97,41\% d'\textit{accuracy} en lemme), \enquote{rapide} à entraîner (quelques heures) et dont les erreurs sont acceptables (choix de formes \textit{negligens} à place de \textit{neglegens} par exemple). En plus du modèle de lemmatisation, nous avons préparé un second ensemble de modèles pour la morphosyntaxe, avec des résultats allant, pour la morphologie, de 87,8\% (pour les cas) à 98,49\% (pour les personnes). Cette variation des résultats d'une catégorie à l'autre s'explique par des niveaux de complexité propres à chaque catégorie, certaines présentant des homographes difficiles à désambiguïser (comme les cas, par ex. \textit{domino} qui peut être datif ou ablatif singulier) alors que d'autres peuvent s'appuyer sur des morphèmes différents (comme la personne, par ex. \textit{-o} pour la P1 vs \textit{-as} pour la P2 de \textit{amare}, \og~aimer~\fg{}).

Nous avons établi des typologies d'erreur qui permettent de comprendre les limites du modèle: une grande partie des confusions est issue de problèmes d'interprétation de la syntaxe et de phénomènes de lexicalisation (\textit{liber}/\textit{liberi}, \textit{multus}/\textit{multum}, etc.).

Nous proposons ensuite des méthodes d'évaluation afin de définir la qualité du modèle sur des données en dehors du corpus du LASLA dites \enquote{hors-domaine}. Le corpus du LASLA auquel nous avons eu accès s'arrête en effet à la période classique, et il traite peu de la sexualité vulgaire avec l'absence de Martial et des \textit{Priapées}. Dans ce cadre, nous proposons des tests sur ces dernières, annotées par nos soins, et sur le corpus d'Anthony Glaise: les résultats permettent d'apprécier les succès et les limites du modèle sur des corpus thématiquement éloignés (les \textit{Priapées}) et chronologiquement plus étendus (la version du corpus d'A.~Glaise utilisée dans notre évaluation s'étend du II\textsuperscript{e} au IX\textsuperscript{e} siècle). Dans la même perspective, nous avons converti semi-manuellement les données de la \textit{Vulgate} de \textit{Proiel}. Avec ces cas extrêmes par rapport aux données d'entraînement, nous pouvons nous faire une bonne idée des limites du modèle sur de ces nouveaux échantillons. Nous avons aussi cherché à déterminer l'influence que pouvaient avoir les genres et les traces d'auctorialité, permettant là aussi d'apprécier la capacité du modèle à s'étendre au-delà des textes composant le corpus du LASLA. 

Pour finir, nous avons mis en valeur l'importance de l'outillage autour des modèles. Les données pour l'entraînement sont en effet \enquote{parfaites}: la sélection des unités syntaxiques (par exemple, des phrases) est majoritairement manuelle, les \textit{tokens} de bruit (par ex.: notes marginales, numéro de page, renvoi de note) sont filtrés en amont par l'annotateur, etc. En développant \textit{Pie-Extended}, nous avons cherché à assurer une correspondance entre les données du monde réel et les données d'entraînement. Nous prenons ainsi en compte cette différence de statut entre ces deux \enquote{mondes} en préparant les données en amont de leur traitement par le modèle neuronal lui-même. Nous avons montré que ce dernier est \enquote{perturbé} par la découverte de caractères inconnus: les signes de ponctuation sont non seulement lemmatisés en pronom relatif à cause de leur inexistence dans le corpus du LASLA et donc des données d'apprentissages, mais en plus influencent le traitement d'autres mots des phrases où ils sont présents en perturbant la compréhension du contexte. En extrayant ces signes et en normalisant les usages u/v et i/j (v et j étant absents du LASLA), on permet au lemmatiseur de traiter optimalement les données; en les réinjectant après le traitement, on assure une correspondance avec le texte d'origine.

Pour finir, nous avons mis en valeur l'importance de l'outillage autour des modèles. Les données pour l'entraînement sont en effet \enquote{parfaites}: comme la sélection des unités syntaxiques (par exemple, des phrases) est majoritairement manuelle, les \textit{tokens} de bruit (par ex.: notes marginales, numéro de page, renvoi de note) sont filtrés en amont par l'annotateur, etc. La confrontation avec des données \enquote{réelles}, forcément éloignées de la \enquote{perfection} des données d'entraînement, peut donc être problématique, et il convient de travailler à réduire ce hiatus. Pour ce faire, nous avons décidé d'intervenir en amont dans \textit{Pie-Extended}, en préparant les données \enquote{réelles} pour les \enquote{rapprocher} formellement des données d'entraînement. C'est notamment le cas pour les signes de ponctuation ou les lettres j et v, absents du corpus du LASLA, qu'il convient de retirer avant de les montrer au modèle neuronal qui ne les \enquote{connait} logiquement pas. Les laisser entraînerait un dégradation sérieuse des résultats, se propageant potentiellement aux tokens périphériques -- les signes de ponctuation sont non seulement lemmatisés en pronom relatif, mais affectent les autres mots de la phrases en perturbant la compréhension du contexte. Le texte est donc envoyé au modèle après ce \enquote{nettoyage}, et les tokens retirés sont réintroduits une fois la tâche d'annotation effectuée, pour assurer une correspondance parfaite avec le texte d'origine.

\paragraph{La détection d'isotopie}

Avec notre dernier chapitre, nous abordons la détection d'isotopie à partir des relevés d'Adams. Nous y traitons la détection isotopique comme une tâche de classification binaire: un extrait, \textit{a priori} une \enquote{phrase}, présente ou non une isotopie sexuelle. L'apprentissage profond repose sur trois piliers, les données (ou \textit{features}), les architectures et les méthodes d'entraînement. Les méthodes d'entraînement ont généralement un bénéfice sur le dernier kilomètre, comme nous l'avons constaté pour la lemmatisation latine, et n'ont pas fait l'objet d'un traitement systématique dans cette dernière partie. Pour les deux autres piliers, nous avons passé en revue les pratiques en considérant le \enquote{thème} du modèle (détection d'implicite), son domaine (antiquité), ses particularités en quantité (petit corpus d'entraînement) et sa catégorie (classification à label unique de texte). Ce compte-rendu des méthodes nous a permis d'établir un ensemble de points ou d'architectures à tester. Par exemple, l'utilisation de réseaux siamois, en vogue en traitement de données visuelles, connus pour leur capacité à apprendre à partir de peu de données, est issue de ce panorama. Nous avons aussi vu que notre travail, contrairement à ce que le titre aurait pu laisser penser, ne rentre pas dans la même catégorie que le travail de \enquote{détection de métaphore} qui s'intéresse plus particulièrement à la dérivation et à l'évolution sémantique. Quelques travaux ont montré l'importance des annotations linguistiques, complémentaires à la forme: cela va dans le sens du souhait de F.~Rastier qui appelait dès 2005 à un dépassement de la forme par une approche \enquote{morphosémantique}\footnote{\enquote{On doit compléter et sans doute dépasser la question distributionnelle du texte par une conception morphosémantique qui tienne compte des inégalités qualitatives entre formes. \textcite[p.~100]{rastier2005enjeux}.}}.

Il fallait assurer que les données de l'exemplier soient non seulement exploitables, mais aussi contre-balancées par un nouvel ensemble de données ne présentant pas l'isotopie sexuelle. Sur ce point, l'approche manuelle fut rapidement écartée: comment, en tant que lecteur, peut-on sélectionner de manière exhaustive ou représentative ce qui n'est pas sexuel ? Très rapidement, des biais peuvent apparaître, en choisissant des sujets qui seraient plus clairement marqués comme non sexuels, et pourtant, la phrase \enquote{Je pense donc je suis} est tout autant non sexuelle que \enquote{J'ai écouté de la musique toute l'après-midi} ou \enquote{Les concombres sont de la famille des cucurbitacées}. Nous avons donc décrit la mise en place semi-automatique d'un corpus négatif, en cherchant à représenter chaque oeuvre dans notre set de données. 

L'un des éléments prometteurs vus dans le panorama des données était l'injection dans une architecture neuronale de métadonnées descriptives du texte afin d'informer le modèle du genre, de la période ou de l'auteur derrière un texte pour l'aider à identifier des traits socio- ou idiolectaux. Cette perspective nous a donc conduits logiquement à nous assurer que ces données soient exploitables et représentées dans le corpus. De même, il était important de tester, au-delà de la forme, la capacité de la machine de s'entraîner sur une combinaison lemme + informations morphosyntaxiques, et donc de prévoir l'inclusion de ces métadonnées linguistiques à l'entraînement, mais aussi à la phase de prédiction.

Avec le panorama et les données, il fallait mettre en place un logiciel modulaire: il faut qu'il soit capable d'ingérer différents niveaux de complexité de données, de changer d'architecture, de méthode d'évaluation et de prédiction. Nous avons donc listé l'ensemble des données possibles, mais aussi des modules exploitables (définis en chapitre 2). Nous représentons notre logiciel comme une suite de trois couches du point de vue neuronal: une couche de projection, c'est-à-dire de passage du mot à une représentation numérique (\textit{embeddings} avec \textit{Word2Vec}, \textit{FastText} et BERT); une couche d'encodage, et donc de prise en compte de la séquence de mots pour en produire une représentation (réseaux LSTM, GRU, CNN; concaténation BERT, etc.); une couche décisionnelle qui se sépare entre les couches linéaires (classification \enquote{classique} et comparaison de distance pour les réseaux siamois). Pour rappeler le type de variations que nous avons prévues en entrée, un mot peut être représenté par des combinaisons incluant sa forme, son lemme, sa POS, chacune des autres catégories de morphosyntaxe (Cas, Genre, etc.), les caractères qui forment son lemme et sa forme, des sous-tokens (BERT), des métadonnées sur le texte dont il est issu.

BERT a généré un travail important. Nous avons essayé d'utiliser BERT en parallèle d'un traitement \enquote{classique} des mots ou comme seule entrée. Nous aurions souhaité pouvoir utiliser les représentations BERT comme sous-information de chaque mot, au même titre que le lemme, mais le manque de rigueur en ingénierie des travaux sur \textit{Latin BERT} a rendu cela presque impossible: renouveler l'ingénierie de \textit{transformers} pour le latin (et donc ré-entraîner un modèle) est l'une des pistes à explorer à l'avenir pour compléter ce travail.

Programmes et données obtenus, nous avons pu passer à l'évaluation des architectures. De la même manière que nous avons souhaité étudier la variation des résultats pour les modèles de lemmatisation, les modèles de détection d'isotopie ont été testés dans un cadre général puis sur des variations du \textit{set} de données original. La première nouvelle est positive: on trouve des architectures qui produisent des modèles performants. Les modèles avec injection de métadonnées ainsi que les modèles à base d'un \textit{Word2Vec} classique et de réseaux récurrents proposent des résultats plus que satisfaisants. Au contraire, les architectures en réseaux siamois ont complètement échoué à capturer des isotopies, quelles que soient les variations et optimisations appliquées. Sur le banc d'essai des modèles et architectures, l'injection de métadonnées est incroyablement performante, mais une étude approfondie démontre qu'elle finit par dépasser le contenu en importance dans les décisions. Pour simplifier la situation, les écarts entre auteurs ou périodes sont tellement grands, qu'il suffit de se fier à ces métadonnées pour produire une bonne classification: nous l'avons montré avec un Cicéron qui était estimé totalement pornographique quand on mentait à la machine en lui disant qu'il était Martial, quand, à l'inverse, Martial, \enquote{déguisé en Cicéron}, pouvait utiliser les mots les plus horriblement vulgaires de la langue latine sans que le modèle ne réagisse.

Nous avons essayé, en réutilisant le même corpus de données dans des arrangements différents, d'évaluer la flexibilité des architectures et leurs réponses à des contraintes différentes. Parmi ces dernières, nous avons quantifié l'impact de la taille du corpus d'entraînement, d'un corpus constitué uniquement d'extraits explicites, d'un corpus constitué uniquement de métaphores. Ces trois évaluations nous ont permis de rendre compte d'une évolution des résultats suivant les contraintes de tailles et de contenus, permettant de recommander des architectures particulières pour chacun des cas rencontrés. Nous finissons notre thèse par ces recommandations: il est plus pratique d'utiliser un réseau où la couche d'encodage est un module HAN, permettant d'avoir des traces de la décision effectuée par le modèle à travers l'attention, et des couches d'\textit{embeddings} en \textit{Word2Vec} pour des corpus de plus de cinq cents exemples, ou de BERT pour des corpus plus petits. L'injection de métadonnées est à utiliser avec parcimonie, suivant la spécificité auctoriale, périodique ou générique d'une thématique, afin de pouvoir construire des corpus équilibrés d'entraînement ne surreprésentant pas un auteur, une période ou un genre.

\paragraph{Contributions, limites, futur}

Notre thèse de doctorat a été un travail qui a touché à de nombreux domaines, aussi il nous semble important de traiter chacun de ces domaines séparément, en listant au besoin les limites et les recommandations pour chacun concernant de futures recherches. Nous aborderons donc les thématiques transversales dans l'ordre suivant: l'histoire du champ des études latines dans le cadre des humanités numériques, les corpus et l'exemplier, la lemmatisation et l'annotation morphosyntaxique, la classification de textes, la méthode de recherche numérique \enquote{expérimentale}, le tout pour la langue latine.

En cherchant à inscrire notre travail dans une tradition plus ancienne, nous avons pu rassembler en un endroit des informations et des documents oubliés sur l'histoire de l'étude du latin à l'ère numérique, voire parfois au-delà de la simple question de l'ordinateur\footnote{Il nous semble qu'il n'existait pas, avant cette thèse, de résumé sur l'école de Manchester.}. Cette histoire est extrêmement riche et nous permet d'apprécier au mieux l'état actuel des corpus et de la recherche du domaine. Par exemple, l'ambition initiale pédagogique du corpus de Perseus est une caractéristique importante pour la critique de ce corpus: si le but était de mettre à disposition des textes pour qu'ils soient lus, voire compris par des non-spécialistes avec une véritable focalisation sur la mise à disposition de traduction, il est plus difficilement tenable de reprocher à Perseus de ne pas mettre à disposition d'apparat critique, rarement utilisé par les étudiants d'avant les \textit{postgraduate studies}. C'est sa position de seul corpus ouvert et traçable (par opposition au corpus de la \textit{Latin Library} qui laisse parfois à désirer sur les informations disponibles sur les sources employées) qui l'a fait devenir un corpus pour le traitement automatique des langues ! Sur ce point, l'histoire des corpus annotés en latin permet aussi d'en comprendre les limites. L'ouverture récente du corpus du LASLA peut permettre une véritable révolution de l'annotation automatique du latin. Pour autant, il est clair que notre approche historique a souffert d'une incompétence linguistique en italien et laisse peu de place à d'autres aires géographiques très en pointe sur le latin médiéval, comme l'Allemagne. En dehors d'une recommandation de capturer au plus tôt la mémoire des créateurs de corpus, à travers une démarche d'histoire orale et d'archivage de ces captations, il faut absolument se tourner vers ces pays pour compléter l'histoire des corpus, littéraires ou linguistiques. De même, réintégrer l'importance de l'épigraphie et sa capacité à adopter l'informatique doit pouvoir intégrer un discours sur l'histoire des \textit{digital classics}.

Il était nécessaire de réunir un corpus assez important pour pouvoir constituer l'exemplier et nourrir les modèles. On pourrait penser la question des corpus résolus: après bientôt quarante ans de production de corpus latins classiques et tardif, ne serait-il pas temps de passer à autre chose ? C'est finalement ce qu'a fait le projet \textit{Open Greek and Latin} en se tournant vers le grec, moins bien fourni en dehors de la période classique... \textit{Corpus Corporum} affiche en effet des chiffres qui donnent le vertige: cent soixante-dix millions de mots seraient réunis sur la plate-forme... Nous l'avons montré: il ne s'agit pas pour le traitement automatique que de quantité, et les chiffres cachent parfois des réalités bien problématiques. Par exemple, la totalité de \textit{Corpus Corporum} n'est pas librement téléchargeable, limitant déjà le corpus à une partie beaucoup plus petite. Par ailleurs, les corpus sont inégalement annotés, et la capacité d'exploiter un corpus \textit{machine actionable} est indissociable d'une exploitation de qualité dans laquelle le philologue, en plus du TAListe, peut avoir confiance. Il faut absolument réinjecter de l'analyse philologique dans la constitution de corpus en reconnaissant que ces corpus sont utilisés dans des approches statistiques qui ont besoin des leviers à leur lecture, au sens philologique et informatique. À travers cette limite des corpus actuels, il faut donc reprendre un travail de production ou de transformation de corpus: nous avons souvent cité les \textit{Declamationes Minores} de Quintilien comme manquantes, il est aussi nécessaire d'assurer une meilleure couverture de la période chrétienne uniquement représentée par des éditions datées (\textit{Patrologia} ou \textit{CSEL} du XIX\textsuperscript{e}). 

En dehors des corpus de textes, nous avons réussi grâce aux différentes tâches de lemmatisation ou d'annotation d'isotopie à mettre à disposition à la fois un corpus \textit{silver} de près de vingt millions de tokens et un exemplier inédit pour la lexicographie et l'histoire de la sexualité. Si la version lemmatisée du corpus ne peut grandir qu'en même temps que ce dernier, il est clair que l'exemplier numérique devra être augmenté d'autres sources, si son objectif est de dépasser l'oeuvre d'Adams. Il faudra alors couvrir les usages de termes moraux (\textit{lascivus, impudicus}), liés à l'absence de sexualité (\textit{uirgo, uirginitas}) et d'autres métaphores ou métonymies oubliées ou ignorées par Adams. De même, il ne faudra pas hésiter à compléter chaque exemple de bibliographie secondaire, afin qu'étudiants et chercheurs puissent puiser dans cet exemplier un état de l'art de la réflexion historique ou lexicographique.

La lemmatisation du latin, mais aussi d'autres espaces linguistiques comme le grec ou l'ancien français, a connu une véritable révolution avec le travail de Mike Kestemont puis d'Enrique Manjavacas. Pour la première fois depuis TreeTagger peut-être, il a été simple de prendre en main un outil d'entraînement de modèles et de prédiction. Le succès de \textit{Pie} et le fait de l'avoir retenu, dans nos travaux ici ou dans celui effectué en ancien français avec des collègues, vient autant de ses performances que de la réactivité de ses concepteurs, de leur objectif de mettre à disposition un outil et non uniquement de produire un score pour une conférence de \textit{deep learning}. \textit{Pie} est modulaire, réutilisable et véritablement open-source dans le sens où il est documenté. Ce changement nous a permis de contribuer largement à sa base de code, et, lorsqu'Enrique Manjavacas a dû laisser \textit{Pie} de côté pour des raisons professionnelles, reprendre le travail à travers \textit{PaPie} a été d'une relative simplicité. Nous espérons que le travail d'explicitation de la méthode pour l'optimisation de cet outil permettra à d'autres soit de ré-entraîner de nouveaux modèles pour le latin, soit de l'appliquer à d'autres domaines linguistiques. La question logicielle de prédiction n'est qu'une partie du problème de la lemmatisation et de l'annotation morphosyntaxique. Il est très peu souvent fait mention de la qualité des données d'entraînement et des référentiels de lemmes. Or, cette question est centrale, car un lemmatiseur \enquote{neuronal} essaye d'apprendre des règles: mélanger des données qui utiliseraient le Gaffiot et d'autres qui ont recours au Forcellini pose un problème de qualité de prédiction, car les deux dictionnaires sont issues de traditions et pratiques lexicographiques différentes... Dans ce cadre, l'application \textit{Pyrrha} et le recours à des bases communes de référentiel ont permis d'étendre les données du LASLA vers la période tardive tout en respectant bon nombre de règles d'étiquetage de ce dernier, qu'il s'agisse du recours au Forcellini ou de l'absence d'annotation en contexte du genre\footnote{Nous avons tout de même abandonné la pratique d'annotation des participes du LASLA, qui posait trop de problèmes au lemmatiseur.}. Pour autant, même avec les \textit{Priapées} que nous avons annotées, la \textit{Vulgate} que nous avons alignée et le corpus d'Anthony Glaise, il reste un vaste travail de création de données à faire. D'une part, il faut absolument essayer de réemployer les données médiévales non normalisées\footnote{Par exemple, les 481~578 tokens de \textit{Computational Historical Semantics}: \textcite{capitularies}.} et de donner aux modèles une capacité à gérer ces dernières: un modèle diachronique sur un référentiel unique permettrait de réaliser une étude des glissements sémantiques sur le long terme. D'autre part, il est nécessaire d'élargir le traitement automatique des langues aux sources épigraphiques qui posent comme la période médiévale un problème de variation graphique, mais qui possède aussi leurs propres codes dans la retranscription du textes. Ces deux pistes amèneront peut-être d'autres besoins logiciels qui devront alors être traités.

Du point de vue de la classification automatique de texte, nous avons non seulement montré que la détection d'isotopie était possible, mais, de fait, notre \textit{pipeline} d'entraînement et de prédiction est généralisable à d'autres catégories de \enquote{détection}. Nous avons évalué les possibilités et les limites pour l'isotopie sexuelle, en proposant des architectures adaptées à des variations de taille ou de contenu. En faisant cela, nous avons démontré la possibilité pour la machine de produire des connaissances qui nous avaient échappé (par exemple, la phrase d'Ovide sur \enquote{donner la nuit} en \textit{Amours}, 3, 28). L'outil à architecture variable et les diverses méthodes d'entraînement sont applicables à d'autres isotopies. La chaîne, qui se satisfait de textes annotés en lemme et en morphosyntaxe avec des classes prédéfinies, pourrait aussi servir à d'autres formes de classification pour du latin (auteur, genre, etc.) y compris dans un objectif de développer une nouvelle herméneutique sur certaines de ces questions. Nous sommes bien évidemment déçus de la surspécialisation des modèles sur les métadonnées: il serait intéressant de voir, pour des isotopies qui connaissent des présences en période et en genre plus équilibrées, si cette injection d'informations supplémentaires pèse autant sur la décision du modèle. Par ailleurs, nous avons écarté le fait de rajouter plus de contexte à la machine, en nous limitant à une phrase des éditeurs. Pour des isotopies plus courantes, ou en s'assurant de ne pas mettre en contexte des éléments utilisés dans un \textit{set} différent (\textit{Train/Test}) , il pourrait être intéressant de tester un modèle qui prenne un contexte plus large, qu'il s'agisse du document ou des phrases voisines.

La question de la méthode a été centrale et a traversé l'ensemble du travail, qu'il s'agisse de l'établissement d'un corpus et de l'exemplier, de l'entraînement d'un lemmatiseur ou du détecteur d'isotopie, de la prédiction. À travers notre thèse de doctorat, nous avons essayé de nous raccrocher à une éthique de la recherche qui intègre les principes de reproductibilité et d'ouverture: tout ce qui a été produit est disponible ouvertement, toutes les modifications faites sur les données des projets d'origine leur ont été proposées. La reproductibilité concerne la capacité de relancer les expériences, la réutilisabilité est une vulgarisation de cette reproductibilité afin de donner la capacité à un plus grand public d'utiliser un outil. Or, tout est ouvert, mais il serait probablement exagéré de dire que tout est réutilisable. Parmi les productions logicielles présentées, les outils de classification (\textit{Seligator}), de création d'exemplier et d'exploration de ce dernier nécessitent un polissage pour leur ré-emploi par des tiers. Il s'agit ici d'écrire un tutoriel, là de mieux documenter l'installation. 

La réutilisabilité des produits de la thèse permet déjà de faire avancer d'autres projets. Nous parlions plus tôt d'ancien français: \textit{Pyrrha} et \textit{Pie-Extended} permettent à des étudiants et des chercheurs de gagner en assurance sur l'annotation de corpus dans d'autres domaines linguistiques que le latin. Nous connaissons quatre articles publiés ou en cours de publication qui parlent de l'utilisation de \textit{Pyrrha} dans leur production de données\footcite{ceresato_lanalisi_2021, meelen_old_2021,pyrrhachahan, camps:halshs-03353125}. \textit{Pie-Extended} a été pour l'instant principalement utilisé pour les autres domaines linguistiques, en tout cas lorsqu'il a été cité, on le retrouve notamment pour le français classique ou moderne, mais aussi le grec ancien\footcite{reboul:hal-03340641}. La réutilisation large de ces deux outils est principalement due à la simplicité d'utilisation: les outils ont des versions web, faciles à utiliser, ou des versions locales assez documentées pour que des personnes connaissant le python puissent exploiter leurs capacités. C'est cet horizon qu'il faut tenter d'atteindre pour l'ensemble de la production logicielle de notre thèse de doctorat: s'assurer que la recherche ne s'arrête pas après ces quelques dernières lignes de conclusion. Nous le disions en introduction, cette thèse devait à la fois être une thèse de romaniste, de \textit{digital humanist} et d'ingénieur. Nous pensons avoir tenu cette promesse.
